{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6KkToOAF1M"
      },
      "source": [
        "# Notebook to run learned kv compression scripts on colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prGnb-vKAnzs"
      },
      "source": [
        "Download Repo and Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YbnT8hMBAE49",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Download Repo\n",
        "%cd /content\n",
        "!rm -rf learned-kv-compression\n",
        "!git clone -b colab https://henro25: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@github.com/henro25/learned-kv-compression\n",
        "%cd /content/learned-kv-compression/\n",
        "%ls\n",
        "\n",
        "# Install Requirements\n",
        "%pip install -r colab_requirements.txt\n",
        "%pip uninstall gcsfs -y\n",
        "%pip install --upgrade fsspec==2025.3.2\n",
        "%pip install gcsfs==2024.12.0\n",
        "%pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7vdgntcAana"
      },
      "source": [
        "## Training the Autoencoder\n",
        "\n",
        "This trains an autoencoder that compresses each KV vector to a 16-dimensional latent representation using 1000 texts from WikiText-103."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7idaakurATWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563c4136-2eb2-411e-a292-45478d1cf366",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-08 05:49:17.395306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744091357.417357   24026 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744091357.424720   24026 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 05:49:17.447735: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 32,\n",
            " 'buffer_mult': 2,\n",
            " 'config': 'src/configs/default_config.json',\n",
            " 'device': 'cuda',\n",
            " 'eval_interval': 100,\n",
            " 'head_dim': 64,\n",
            " 'input_dim': 64,\n",
            " 'latent_dim': 16,\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 256,\n",
            " 'name': 'distilgpt2',\n",
            " 'num_epochs': 10,\n",
            " 'num_eval_texts': 100,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_key_value_heads': 12,\n",
            " 'num_train_texts': 1000,\n",
            " 'output_dir': 'models/distilgpt2_16',\n",
            " 'seed': 42}\n",
            "Using 1000 texts for training and 100 texts for evaluation\n",
            "Epoch 1/10: 100% 31/31 [00:13<00:00,  2.38it/s]\n",
            "Epoch 1/10, Average Loss: 0.2170\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_1.pth\n",
            "Epoch 2/10: 100% 31/31 [00:13<00:00,  2.29it/s]\n",
            "Epoch 2/10, Average Loss: 0.2473\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_2.pth\n",
            "Epoch 3/10: 100% 31/31 [00:13<00:00,  2.37it/s]\n",
            "Epoch 3/10, Average Loss: 0.2460\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_3.pth\n",
            "Epoch 4/10: 100% 31/31 [00:13<00:00,  2.28it/s]\n",
            "Epoch 4/10, Average Loss: 0.2429\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_4.pth\n",
            "Epoch 5/10: 100% 31/31 [00:13<00:00,  2.34it/s]\n",
            "Epoch 5/10, Average Loss: 0.2419\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_5.pth\n",
            "Epoch 6/10: 100% 31/31 [00:13<00:00,  2.25it/s]\n",
            "Epoch 6/10, Average Loss: 0.2405\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_6.pth\n",
            "Epoch 7/10: 100% 31/31 [00:13<00:00,  2.28it/s]\n",
            "Epoch 7/10, Average Loss: 0.2403\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_7.pth\n",
            "Epoch 8/10: 100% 31/31 [00:14<00:00,  2.17it/s]\n",
            "Epoch 8/10, Average Loss: 0.2389\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_8.pth\n",
            "Epoch 9/10: 100% 31/31 [00:13<00:00,  2.34it/s]\n",
            "Epoch 9/10, Average Loss: 0.2396\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_9.pth\n",
            "Epoch 10/10: 100% 31/31 [00:13<00:00,  2.25it/s]\n",
            "Epoch 10/10, Average Loss: 0.2394\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_10.pth\n",
            "Final model saved at models/distilgpt2_16/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in models/distilgpt2_16\n"
          ]
        }
      ],
      "source": [
        "!python -m src.dictionary_learning.train \\\n",
        "    --name distilgpt2 \\\n",
        "    --latent_dim 16 \\\n",
        "    --num_epochs 10 \\\n",
        "    --batch_size 32 \\\n",
        "    --output_dir models/distilgpt2_16 \\\n",
        "    --num_train_texts 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking"
      ],
      "metadata": {
        "id": "L3sF0VkRbUZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a quick test for KV Cache compression with minimal parameters"
      ],
      "metadata": {
        "id": "D6oJq5qtZTwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./quick_test.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j_99ro74ZTZw",
        "outputId": "45a5b5eb-10ad-4e21-b04b-87e75d76b085"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Quick Test: KV Cache Compression ====\n",
            "Model: distilgpt2\n",
            "Latent dimension: 16\n",
            "Number of epochs: 1\n",
            "Number of training texts: 10\n",
            "Cache size: 1 MB\n",
            "Batch size: 512\n",
            "Number of runs: 3\n",
            "========================================\n",
            "Step 1: Training autoencoder...\n",
            "2025-04-08 05:51:56.068042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744091516.089612   24733 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744091516.096595   24733 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 05:51:56.119553: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 2,\n",
            " 'buffer_mult': 2,\n",
            " 'config': 'src/configs/default_config.json',\n",
            " 'device': 'cuda',\n",
            " 'eval_interval': 100,\n",
            " 'head_dim': 64,\n",
            " 'input_dim': 64,\n",
            " 'latent_dim': 16,\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 256,\n",
            " 'name': 'distilgpt2',\n",
            " 'num_epochs': 1,\n",
            " 'num_eval_texts': 100,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_key_value_heads': 12,\n",
            " 'num_train_texts': 10,\n",
            " 'output_dir': 'test_models/distilgpt2_latent16',\n",
            " 'seed': 42}\n",
            "Using 10 texts for training and 100 texts for evaluation\n",
            "Epoch 1/1: 100% 5/5 [00:01<00:00,  3.63it/s]\n",
            "Epoch 1/1, Average Loss: 0.1833\n",
            "Checkpoint saved at test_models/distilgpt2_latent16/autoencoder_epoch_1.pth\n",
            "Final model saved at test_models/distilgpt2_latent16/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in test_models/distilgpt2_latent16\n",
            "Step 2: Running benchmark test...\n",
            "2025-04-08 05:52:22.415621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744091542.435197   24848 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744091542.440773   24848 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 05:52:22.463350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loaded autoencoder from test_models/distilgpt2_latent16/autoencoder_final.pth\n",
            "Benchmarking different cache sizes:   0% 0/1 [00:00<?, ?it/s]\n",
            "Running benchmark for 1.0MB KV cache\n",
            "\n",
            "Generating 1.0MB KV cache:   0% 0/56 [00:00<?, ?it/s]\u001b[A\n",
            "Generating 1.0MB KV cache:  18% 10/56 [00:00<00:01, 39.67it/s]\u001b[A\n",
            "Generating 1.0MB KV cache:  71% 40/56 [00:00<00:00, 89.40it/s]\n",
            "Generated KV cache of size: 0.98MB (56 tokens)\n",
            "Measuring baseline (no compression)...\n",
            "Compressing KV cache...\n",
            "Measuring with compression...\n",
            "Compression ratio: 4.00x\n",
            "Speedup factor: 0.85x\n",
            "Time to first token (baseline): 0.0063s ± 0.0002s\n",
            "Time to first token (compressed): 0.0073s ± 0.0001s\n",
            "Benchmarking different cache sizes: 100% 1/1 [00:00<00:00,  1.94it/s]\n",
            "Results saved to test_results/benchmark_results.json\n",
            "Quick test completed!\n",
            "Check test_results for results and visualizations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run experiments"
      ],
      "metadata": {
        "id": "0WF2aid4bRLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./run_experiment.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNdpwUKhbQv8",
        "outputId": "a3b6a2ac-7c10-4965-8f0c-a45774f147a1",
        "collapsed": true
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== KV Cache Compression Experiment ====\n",
            "Model: distilgpt2\n",
            "Latent dimensions: 8\n",
            "Cache sizes (MB): 1 10 100\n",
            "Number of epochs: 5\n",
            "Number of training texts: 1000\n",
            "Batch size: 1024\n",
            "Number of runs for timing: 5\n",
            "Output directory: experiment_results_distilgpt2\n",
            "========================================\n",
            "./run_experiment.sh: line 40: venv/bin/activate: No such file or directory\n",
            "Starting experiment at Tue Apr  8 06:21:55 AM UTC 2025\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with latent_dim=8\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config experiment_results_distilgpt2/distilgpt2_latent8/train_config.json\n",
            "2025-04-08 06:21:59.684277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744093319.721678   32605 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744093319.734650   32605 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 06:21:59.775211: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 2,\n",
            " 'buffer_mult': 2,\n",
            " 'config': 'experiment_results_distilgpt2/distilgpt2_latent8/train_config.json',\n",
            " 'device': 'cuda',\n",
            " 'eval_interval': 100,\n",
            " 'head_dim': 64,\n",
            " 'input_dim': 64,\n",
            " 'latent_dim': 8,\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 256,\n",
            " 'name': 'distilgpt2',\n",
            " 'num_epochs': 5,\n",
            " 'num_eval_texts': 100,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_key_value_heads': 12,\n",
            " 'num_train_texts': 1000,\n",
            " 'output_dir': 'models',\n",
            " 'seed': 42}\n",
            "Using 1000 texts for training and 100 texts for evaluation\n",
            "Epoch 1/5: 100% 500/500 [00:18<00:00, 27.71it/s]\n",
            "Epoch 1/5, Average Loss: 0.2560\n",
            "Checkpoint saved at models/autoencoder_epoch_1.pth\n",
            "Epoch 2/5: 100% 500/500 [00:16<00:00, 29.44it/s]\n",
            "Epoch 2/5, Average Loss: 0.2359\n",
            "Checkpoint saved at models/autoencoder_epoch_2.pth\n",
            "Epoch 3/5: 100% 500/500 [00:18<00:00, 27.45it/s]\n",
            "Epoch 3/5, Average Loss: 0.2247\n",
            "Checkpoint saved at models/autoencoder_epoch_3.pth\n",
            "Epoch 4/5: 100% 500/500 [00:17<00:00, 28.74it/s]\n",
            "Epoch 4/5, Average Loss: 0.2214\n",
            "Checkpoint saved at models/autoencoder_epoch_4.pth\n",
            "Epoch 5/5: 100% 500/500 [00:17<00:00, 28.01it/s]\n",
            "Epoch 5/5, Average Loss: 0.2181\n",
            "Checkpoint saved at models/autoencoder_epoch_5.pth\n",
            "Final model saved at models/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in models\n",
            "Running perplexity evaluation for latent_dim=8\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with latent_dim=8\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --model distilgpt2 --autoencoder experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_final.pth --latent_dim 8 --cache_sizes 1.0 10.0 100.0 --batch_size 1024 --num_runs 5 --output experiment_results_distilgpt2/benchmark_distilgpt2_latent8 --config experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_config.json\n",
            "2025-04-08 06:23:53.964843: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744093433.985946   33102 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744093433.992201   33102 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 06:23:54.016561: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "\n",
            "Calculating WikiText perplexity...\n",
            "Calculating perplexity:   0% 0/100 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Calculating perplexity: 100% 100/100 [00:01<00:00, 76.51it/s]\n",
            "Baseline perplexity: 86.18\n",
            "Evaluating with compressed cache: 100% 100/100 [01:15<00:00,  1.33it/s]\n",
            "Compressed cache perplexity: 2911.95\n",
            "\n",
            "Evaluating on LongBench...\n",
            "\n",
            "Evaluating on narrativeqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 124.95it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:17<00:00, 11.74it/s]\n",
            "narrativeqa - Baseline PPL: 405.45, Compressed PPL: 2609.98\n",
            "\n",
            "Evaluating on hotpotqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 126.24it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:29<00:00,  6.67it/s]\n",
            "hotpotqa - Baseline PPL: 269.38, Compressed PPL: 1869.26\n",
            "\n",
            "Evaluating on 2wikimqa...\n",
            "Calculating perplexity: 100% 200/200 [00:02<00:00, 85.64it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:26<00:00,  7.64it/s]\n",
            "2wikimqa - Baseline PPL: 420.95, Compressed PPL: 2829.50\n",
            "\n",
            "Evaluating on musique...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 125.34it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:28<00:00,  6.95it/s]\n",
            "musique - Baseline PPL: 211.17, Compressed PPL: 1308.41\n",
            "\n",
            "Evaluating on dureader...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 106.86it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:30<00:00,  6.60it/s]\n",
            "dureader - Baseline PPL: 68.06, Compressed PPL: 2916.11\n",
            "\n",
            "Benchmarking complete!\n",
            "Results saved in experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8\n",
            "- Evaluation results (JSON): experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/evaluation_results.json\n",
            "- Evaluation results (CSV): experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/evaluation_results.csv\n",
            "- Plots saved in experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/plots\n",
            "Perplexity evaluation completed for latent_dim=8\n",
            "\n",
            "================================================================================\n",
            "Experiment Summary\n",
            "================================================================================\n",
            "Model: distilgpt2\n",
            "Latent dimensions tested: [8]\n",
            "KV cache sizes tested: [1.0, 10.0, 100.0] MB\n",
            "Batch size: 1024\n",
            "Number of runs for timing: 5\n",
            "Total runtime: 0h 5m 52.41s\n",
            "Results saved to: experiment_results_distilgpt2\n",
            "- Latent dim 8: experiment_results_distilgpt2/benchmark_distilgpt2_latent8\n",
            "================================================================================\n",
            "Experiment completed at Tue Apr  8 06:27:47 AM UTC 2025\n",
            "Generating comparison report...\n",
            "Loading results from: ['experiment_results_distilgpt2/benchmark_distilgpt2_latent8']\n",
            "Generating comparison plots...\n",
            "Generating summary report...\n",
            "Report generated at experiment_results_distilgpt2/comparison/comparison_report.md\n",
            "Comparison complete. Results saved to experiment_results_distilgpt2/comparison\n",
            "Experiment and analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eval with Perplexity and Longbench"
      ],
      "metadata": {
        "id": "rZ5GZpidFIp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./src/evaluation/run_evaluation.sh"
      ],
      "metadata": {
        "id": "2koR1iA0a-Az",
        "outputId": "f3478a05-6662-4fd3-bcf6-239cdc86bbe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation...\n",
            "2025-04-08 06:00:03.677330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744092003.697675   26909 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744092003.703449   26909 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 06:00:03.728452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Successfully loaded autoencoder from models/distilgpt2_16/autoencoder_final.pth\n",
            "Loaded 100 evaluation texts\n",
            "\n",
            "Calculating baseline perplexity...\n",
            "Calculating perplexity: 100% 100/100 [00:01<00:00, 63.12it/s]\n",
            "Baseline perplexity: 86.11\n",
            "\n",
            "Calculating perplexity with compressed cache...\n",
            "Evaluating with compressed cache: 100% 100/100 [00:01<00:00, 51.55it/s]\n",
            "Compressed cache perplexity: 2.29\n",
            "\n",
            "Evaluating on LongBench...\n",
            "\n",
            "Evaluating on narrativeqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 125.47it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 63.69it/s]\n",
            "narrativeqa - Baseline PPL: 414.46, Compressed PPL: 1834.44\n",
            "\n",
            "Evaluating on hotpotqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 125.37it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 51.95it/s]\n",
            "hotpotqa - Baseline PPL: 272.10, Compressed PPL: 1788.02\n",
            "\n",
            "Evaluating on 2wikimqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 127.85it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 63.89it/s]\n",
            "2wikimqa - Baseline PPL: 421.86, Compressed PPL: 2102.78\n",
            "\n",
            "Evaluating on musique...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 126.00it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 52.75it/s]\n",
            "musique - Baseline PPL: 212.93, Compressed PPL: 1385.13\n",
            "\n",
            "Evaluating on dureader...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 126.77it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 63.97it/s]\n",
            "dureader - Baseline PPL: 68.44, Compressed PPL: 5792.14\n",
            "Saved perplexity comparison plot to models/perplexity_comparison.png\n",
            "\n",
            "Evaluation complete!\n",
            "Results saved in models\n",
            "- Evaluation results: models/evaluation_results.json\n",
            "- Perplexity comparison plot: models/perplexity_comparison.png\n",
            "Evaluation complete! Results are saved in models/distilgpt2_16\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}