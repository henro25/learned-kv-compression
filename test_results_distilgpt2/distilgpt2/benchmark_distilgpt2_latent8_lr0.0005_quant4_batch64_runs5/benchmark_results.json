{
  "baseline_perplexity": 99.62999725341797,
  "compressed_perplexity": 5097.2578125,
  "longbench_results": {
    "baseline": {
      "narrativeqa": 383.7532653808594,
      "hotpotqa": 235.79507446289062,
      "2wikimqa": 383.7532653808594,
      "musique": 182.28208923339844,
      "dureader": 83.36823272705078
    },
    "compressed": {
      "narrativeqa": 5857.2421875,
      "hotpotqa": 4365.93212890625,
      "2wikimqa": 4526.2724609375,
      "musique": 2166.38720703125,
      "dureader": 4797.94775390625
    }
  },
  "config": {
    "models": [
      "distilgpt2"
    ],
    "latent_dims": [
      8
    ],
    "learning_rates": [
      0.0005
    ],
    "num_epochs": [
      2
    ],
    "num_train_texts": [
      100
    ],
    "seed": 42,
    "skip_training": false,
    "quantization_bits": 4,
    "output_dir": "test_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant4_batch64_runs5",
    "eval_interval": 1000,
    "num_eval_texts": 5,
    "lm_batch_size": 1,
    "buffer_size": 256,
    "batch_size": 64,
    "dtype": "bf16",
    "buffer_mult": 2,
    "gradient_accumulation_steps": 2,
    "device": "cuda",
    "name": "distilgpt2",
    "num_hidden_layers": 6,
    "num_attention_heads": 12,
    "max_seq_len": 1024,
    "hidden_size": 768,
    "vocab_size": 50257,
    "cache_sizes": [
      1,
      10,
      100,
      1000
    ],
    "model_name": "distilgpt2",
    "latent_dim": 8,
    "num_runs": 5,
    "autoencoder_path": "test_results_distilgpt2/distilgpt2/distilgpt2_latent8_lr0.0005/autoencoders_final.pth",
    "learning_rate": 0.0005
  }
}