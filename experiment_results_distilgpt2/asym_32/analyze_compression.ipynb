{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latent Dim</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Raw PPL</th>\n",
       "      <th>AE PPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>76.19</td>\n",
       "      <td>5049.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>76.19</td>\n",
       "      <td>2192.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>76.19</td>\n",
       "      <td>167.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latent Dim   Dataset Raw PPL   AE PPL\n",
       "0           8  WikiText   76.19  5049.08\n",
       "1          16  WikiText   76.19  2192.71\n",
       "2          32  WikiText   76.19   167.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Table:\n",
      "\\begin{table}\n",
      "\\caption{Perplexity Comparison vs. Latent Dimension (No Compression)}\n",
      "\\label{tab:perplexity_vs_latent_dim}\n",
      "\\begin{tabular}{rlll}\n",
      "\\toprule\n",
      "Latent Dim & Dataset & Raw PPL & AE PPL \\\\\n",
      "\\midrule\n",
      "8 & WikiText & 76.19 & 5049.08 \\\\\n",
      "16 & WikiText & 76.19 & 2192.71 \\\\\n",
      "32 & WikiText & 76.19 & 167.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "Perplexity vs. Latent Dimension plots (log y-axis) and table (with LaTeX output) saved/printed.\n"
     ]
    }
   ],
   "source": [
    "# KV‑cache compression analysis (open‑source only)\n",
    "# ------------------------------------------------\n",
    "import json, glob, re, statistics, itertools, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "# Ensure the plots directory exists\n",
    "plots_dir = \"./plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# 1) locate result files in the current directory\n",
    "files = glob.glob(\"benchmark_results_distilgpt2_*.json\")\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No benchmark_results_distilgpt2_*.json files in this folder!\")\n",
    "\n",
    "records: List[Dict[str, Any]] = []\n",
    "\n",
    "# Function to extract latent dimension from filename\n",
    "def get_latent_dim(filename: str) -> Optional[int]:\n",
    "    match_ld = re.search(r\"distilgpt2_(\\d+)\\.json$\", filename)\n",
    "    if match_ld:\n",
    "        return int(match_ld.group(1))\n",
    "    return None\n",
    "\n",
    "for path in files:\n",
    "    latent_dim = get_latent_dim(path)\n",
    "    if latent_dim is not None:\n",
    "        with open(path) as fp:\n",
    "            res = json.load(fp)\n",
    "\n",
    "        raw_ppl_wiki = res.get(\"raw_baseline_ppl\")\n",
    "        ae_ppl_wiki_none = res[\"perplexities\"].get(\"none\", {}).get(\"ae_compressed_ppl\")\n",
    "\n",
    "        if raw_ppl_wiki is not None and ae_ppl_wiki_none is not None:\n",
    "            records.append(dict(\n",
    "                latent_dim=latent_dim,\n",
    "                raw_ppl=raw_ppl_wiki,\n",
    "                ae_ppl=ae_ppl_wiki_none,\n",
    "                dataset=\"WikiText\"\n",
    "            ))\n",
    "\n",
    "        lb_baseline = res.get(\"longbench\", {}).get(\"baseline\", {})\n",
    "        lb_compressed_none = res.get(\"longbench\", {}).get(\"compressed\", {}).get(\"none\", {})\n",
    "\n",
    "        for task, base_ppl in lb_baseline.items():\n",
    "            comp_ppl = lb_compressed_none.get(task)\n",
    "            if comp_ppl is not None:\n",
    "                records.append(dict(\n",
    "                    latent_dim=latent_dim,\n",
    "                    raw_ppl=base_ppl,\n",
    "                    ae_ppl=comp_ppl,\n",
    "                    dataset=f\"LongBench ({task})\"\n",
    "                ))\n",
    "\n",
    "df = pd.DataFrame(records).sort_values([\"latent_dim\", \"dataset\"]).reset_index(drop=True)\n",
    "\n",
    "latent_dims = sorted(df['latent_dim'].dropna().unique())\n",
    "longbench_tasks = sorted([d.split('(')[1][:-1] for d in df['dataset'].unique() if d.startswith(\"LongBench\")])\n",
    "\n",
    "# Plot for WikiText Perplexity (Log Scale)\n",
    "plt.figure(figsize=(8, 5))\n",
    "wiki_data = df[df['dataset'] == 'WikiText'].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl']).sort_values('latent_dim')\n",
    "if not wiki_data.empty:\n",
    "    raw_ppl_baseline_wiki = wiki_data['raw_ppl'].iloc[0]\n",
    "    plt.plot(wiki_data['latent_dim'], [raw_ppl_baseline_wiki] * len(wiki_data), linestyle='--', marker='o', label='Raw Baseline')\n",
    "    plt.plot(wiki_data['latent_dim'], wiki_data['ae_ppl'], marker='x', label='AE')\n",
    "    plt.xlabel(\"Latent Dimension\")\n",
    "    plt.ylabel(\"Perplexity (Log Scale)\")\n",
    "    plt.title(\"WikiText Perplexity vs. Latent Dimension\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, \"exp1_wikitext_perplexity_vs_latent_dim_logy.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Plot for LongBench Perplexity (averaged across tasks, Log Scale)\n",
    "if longbench_tasks:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    longbench_data = df[df['dataset'].str.startswith('LongBench')].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'])\n",
    "    if not longbench_data.empty:\n",
    "        avg_raw_ppl_lb = longbench_data.groupby('latent_dim')['raw_ppl'].mean()\n",
    "        avg_ae_ppl_lb = longbench_data.groupby('latent_dim')['ae_ppl'].mean()\n",
    "\n",
    "        plt.plot(avg_raw_ppl_lb.index, avg_raw_ppl_lb.values, linestyle='--', marker='o', label='Avg. Raw Baseline')\n",
    "        plt.plot(avg_ae_ppl_lb.index, avg_ae_ppl_lb.values, marker='x', label='Avg. AE')\n",
    "        plt.xlabel(\"Latent Dimension\")\n",
    "        plt.ylabel(\"Average Perplexity (LongBench, Log Scale)\")\n",
    "        plt.yscale('log')\n",
    "        plt.title(\"Average LongBench Perplexity vs. Latent Dimension\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, \"exp1_longbench_avg_perplexity_vs_latent_dim_logy.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Create Table Data\n",
    "table_data = []\n",
    "wiki_data_table = df[df['dataset'] == 'WikiText'].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl']).sort_values('latent_dim')\n",
    "if not wiki_data_table.empty:\n",
    "    raw_ppl_baseline_wiki_table = wiki_data_table['raw_ppl'].iloc[0]\n",
    "    for index, row in wiki_data_table.iterrows():\n",
    "        table_data.append({\n",
    "            \"Latent Dim\": int(row['latent_dim']),\n",
    "            \"Dataset\": \"WikiText\",\n",
    "            \"Raw PPL\": f\"{raw_ppl_baseline_wiki_table:.2f}\",\n",
    "            \"AE PPL\": f\"{row['ae_ppl']:.2f}\"\n",
    "        })\n",
    "\n",
    "longbench_data_table = df[df['dataset'].str.startswith('LongBench')].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'])\n",
    "if not longbench_data_table.empty:\n",
    "    avg_results_lb = longbench_data_table.groupby('latent_dim').agg(\n",
    "        Avg_Raw_PPL=('raw_ppl', 'mean'),\n",
    "        Avg_AE_PPL=('ae_ppl', 'mean')\n",
    "    ).reset_index()\n",
    "    for index, row in avg_results_lb.iterrows():\n",
    "        table_data.append({\n",
    "            \"Latent Dim\": int(row['latent_dim']),\n",
    "            \"Dataset\": \"LongBench (Avg)\",\n",
    "            \"Raw PPL\": f\"{row['Avg_Raw_PPL']:.2f}\",\n",
    "            \"AE PPL\": f\"{row['Avg_AE_PPL']:.2f}\"\n",
    "        })\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "display(table_df)\n",
    "\n",
    "# Create LaTeX Table\n",
    "latex_table = table_df.to_latex(index=False, float_format=\"%.2f\", caption=\"Perplexity Comparison vs. Latent Dimension (No Compression)\", label=\"tab:perplexity_vs_latent_dim\")\n",
    "print(\"\\nLaTeX Table:\")\n",
    "print(latex_table)\n",
    "\n",
    "print(f\"\\nPerplexity vs. Latent Dimension plots (log y-axis) and table (with LaTeX output) saved/printed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latent Dim</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>AE PPL</th>\n",
       "      <th>Raw (32-bit) PPL</th>\n",
       "      <th>Raw (2-bit) PPL</th>\n",
       "      <th>Raw (4-bit) PPL</th>\n",
       "      <th>Raw (8-bit) PPL</th>\n",
       "      <th>Raw (16-bit) PPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>5049.08</td>\n",
       "      <td>76.19</td>\n",
       "      <td>1310.36</td>\n",
       "      <td>104.02</td>\n",
       "      <td>76.30</td>\n",
       "      <td>76.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>2192.71</td>\n",
       "      <td>72.66</td>\n",
       "      <td>1150.16</td>\n",
       "      <td>97.59</td>\n",
       "      <td>72.62</td>\n",
       "      <td>72.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>167.00</td>\n",
       "      <td>72.66</td>\n",
       "      <td>1150.16</td>\n",
       "      <td>97.59</td>\n",
       "      <td>72.62</td>\n",
       "      <td>72.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latent Dim   Dataset   AE PPL Raw (32-bit) PPL Raw (2-bit) PPL  \\\n",
       "0           8  WikiText  5049.08            76.19         1310.36   \n",
       "1          16  WikiText  2192.71            72.66         1150.16   \n",
       "2          32  WikiText   167.00            72.66         1150.16   \n",
       "\n",
       "  Raw (4-bit) PPL Raw (8-bit) PPL Raw (16-bit) PPL  \n",
       "0          104.02           76.30            76.19  \n",
       "1           97.59           72.62            72.65  \n",
       "2           97.59           72.62            72.65  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Table:\n",
      "\\begin{table}\n",
      "\\caption{Perplexity Comparison: AE vs. Raw Quantization Levels}\n",
      "\\label{tab:perplexity_comparison_all_quant}\n",
      "\\begin{tabular}{rlllllll}\n",
      "\\toprule\n",
      "Latent Dim & Dataset & AE PPL & Raw (32-bit) PPL & Raw (2-bit) PPL & Raw (4-bit) PPL & Raw (8-bit) PPL & Raw (16-bit) PPL \\\\\n",
      "\\midrule\n",
      "8 & WikiText & 5049.08 & 76.19 & 1310.36 & 104.02 & 76.30 & 76.19 \\\\\n",
      "16 & WikiText & 2192.71 & 72.66 & 1150.16 & 97.59 & 72.62 & 72.65 \\\\\n",
      "32 & WikiText & 167.00 & 72.66 & 1150.16 & 97.59 & 72.62 & 72.65 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "Perplexity comparison plots (log y-axis) and table (with LaTeX output) saved/printed.\n"
     ]
    }
   ],
   "source": [
    "# KV‑cache compression analysis (open‑source only)\n",
    "# ------------------------------------------------\n",
    "import json, glob, re, statistics, itertools, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "# Ensure the plots directory exists\n",
    "plots_dir = \"./plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# 1) locate result files in the current directory\n",
    "files = glob.glob(\"benchmark_results_distilgpt2_*.json\")\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No benchmark_results_distilgpt2_*.json files in this folder!\")\n",
    "\n",
    "records: List[Dict[str, Any]] = []\n",
    "\n",
    "# Function to extract latent dimension from filename\n",
    "def get_latent_dim(filename: str) -> Optional[int]:\n",
    "    match_ld = re.search(r\"distilgpt2_(\\d+)\\.json$\", filename)\n",
    "    if match_ld:\n",
    "        return int(match_ld.group(1))\n",
    "    return None\n",
    "\n",
    "quantization_levels = [2, 4, 8, 16, 32]\n",
    "\n",
    "for path in files:\n",
    "    latent_dim = get_latent_dim(path)\n",
    "    if latent_dim is not None:\n",
    "        with open(path) as fp:\n",
    "            res = json.load(fp)\n",
    "\n",
    "        raw_ppl: float = res[\"raw_baseline_ppl\"]\n",
    "        ae_ppl_none: Optional[float] = res[\"perplexities\"].get(\"none\", {}).get(\"ae_compressed_ppl\")\n",
    "        kv_ppl_quant = {bits: res[\"perplexities\"].get(str(bits), {}).get(\"kv_cache_baseline_ppl\") for bits in quantization_levels}\n",
    "\n",
    "        if raw_ppl is not None and ae_ppl_none is not None:\n",
    "            record = {\"latent_dim\": latent_dim, \"raw_ppl\": raw_ppl, \"ae_ppl\": ae_ppl_none, \"dataset\": \"WikiText\"}\n",
    "            record.update({f\"raw_quant{b}_ppl\": kv_ppl_quant[b] for b in quantization_levels})\n",
    "            records.append(record)\n",
    "\n",
    "        lb_baseline = res.get(\"longbench\", {}).get(\"baseline\", {})\n",
    "        lb_compressed_none = res.get(\"longbench\", {}).get(\"compressed\", {}).get(\"none\", {})\n",
    "        lb_compressed_quant = {\n",
    "            bits: res.get(\"longbench\", {}).get(\"compressed\", {}).get(str(bits), {}) for bits in quantization_levels\n",
    "        }\n",
    "\n",
    "        for task, base_ppl in lb_baseline.items():\n",
    "            comp_ppl_none = lb_compressed_none.get(task)\n",
    "            comp_ppl_quant = {bits: lb_compressed_quant[bits].get(task) for bits in quantization_levels}\n",
    "            if comp_ppl_none is not None:\n",
    "                record = {\"latent_dim\": latent_dim, \"raw_ppl\": base_ppl, \"ae_ppl\": comp_ppl_none, \"dataset\": f\"LongBench ({task})\"}\n",
    "                record.update({f\"raw_quant{b}_ppl\": comp_ppl_quant[b] for b in quantization_levels})\n",
    "                records.append(record)\n",
    "\n",
    "df = pd.DataFrame(records).sort_values([\"latent_dim\", \"dataset\"]).reset_index(drop=True)\n",
    "\n",
    "latent_dims = sorted(df['latent_dim'].dropna().unique())\n",
    "longbench_tasks = sorted([d.split('(')[1][:-1] for d in df['dataset'].unique() if d.startswith(\"LongBench\")])\n",
    "\n",
    "# Plot for WikiText Perplexity Comparison (Log Scale)\n",
    "plt.figure(figsize=(10, 6))\n",
    "wiki_data = df[df['dataset'] == 'WikiText'].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'] + [f'raw_quant{b}_ppl' for b in quantization_levels]).sort_values('latent_dim')\n",
    "if not wiki_data.empty:\n",
    "    plt.plot(wiki_data['latent_dim'], wiki_data['raw_ppl'], linestyle='--', marker='o', label='Raw Baseline (32-bit)')\n",
    "    for bits in quantization_levels:\n",
    "        plt.plot(wiki_data['latent_dim'], wiki_data[f'raw_quant{bits}_ppl'], marker='v', label=f'Raw ({bits}-bit)')\n",
    "    plt.plot(wiki_data['latent_dim'], wiki_data['ae_ppl'], marker='x', label='AE (No Comp)')\n",
    "    plt.xlabel(\"Latent Dimension\")\n",
    "    plt.ylabel(\"Perplexity (Log Scale)\")\n",
    "    plt.title(\"WikiText Perplexity vs. Latent Dimension\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which=\"both\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, \"exp1_wikitext_perplexity_vs_latent_dim_all_quant_logy.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# Plot for LongBench Perplexity Comparison (averaged across tasks, Log Scale)\n",
    "if longbench_tasks:\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    longbench_data = df[df['dataset'].str.startswith('LongBench')].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'] + [f'raw_quant{b}_ppl' for b in quantization_levels])\n",
    "    if not longbench_data.empty:\n",
    "        avg_raw_ppl_lb = longbench_data.groupby('latent_dim')['raw_ppl'].mean()\n",
    "        avg_ae_ppl_lb = longbench_data.groupby('latent_dim')['ae_ppl'].mean()\n",
    "        avg_raw_quant_ppl_lb = {\n",
    "            bits: longbench_data.groupby('latent_dim')[f'raw_quant{bits}_ppl'].mean() for bits in quantization_levels\n",
    "        }\n",
    "\n",
    "        plt.plot(avg_raw_ppl_lb.index, avg_raw_ppl_lb.values, linestyle='--', marker='o', label='Avg. Raw Baseline (32-bit)')\n",
    "        for bits in quantization_levels:\n",
    "            plt.plot(avg_raw_quant_ppl_lb[bits].index, avg_raw_quant_ppl_lb[bits].values, marker='v', label=f'Avg. Raw ({bits}-bit)')\n",
    "        plt.plot(avg_ae_ppl_lb.index, avg_ae_ppl_lb.values, marker='x', label='Avg. AE (No Comp)')\n",
    "        plt.xlabel(\"Latent Dimension\")\n",
    "        plt.ylabel(\"Average Perplexity (LongBench, Log Scale)\")\n",
    "        plt.yscale('log')\n",
    "        plt.title(\"Average LongBench Perplexity vs. Latent Dimension\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, \"exp1_longbench_avg_perplexity_vs_latent_dim_all_quant_logy.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Create Table Data\n",
    "table_data = []\n",
    "wiki_data_table = df[df['dataset'] == 'WikiText'].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'] + [f'raw_quant{b}_ppl' for b in quantization_levels]).sort_values('latent_dim')\n",
    "if not wiki_data_table.empty:\n",
    "    for index, row in wiki_data_table.iterrows():\n",
    "        table_row = {\"Latent Dim\": int(row['latent_dim']), \"Dataset\": \"WikiText\", \"AE PPL\": f\"{row['ae_ppl']:.2f}\", \"Raw (32-bit) PPL\": f\"{row['raw_ppl']:.2f}\"}\n",
    "        table_row.update({f\"Raw ({b}-bit) PPL\": f\"{row[f'raw_quant{b}_ppl']:.2f}\" for b in quantization_levels})\n",
    "        table_data.append(table_row)\n",
    "\n",
    "longbench_data_table = df[df['dataset'].str.startswith('LongBench')].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'] + [f'raw_quant{b}_ppl' for b in quantization_levels])\n",
    "if not longbench_data_table.empty:\n",
    "    agg_funcs = {'Avg_AE_PPL': ('ae_ppl', 'mean'), 'Avg_Raw_PPL': ('raw_ppl', 'mean')}\n",
    "    agg_funcs.update({f'Avg_Raw_{b}bit_PPL': (f'raw_quant{b}_ppl', 'mean') for b in quantization_levels})\n",
    "    avg_results_lb = longbench_data_table.groupby('latent_dim').agg(**agg_funcs).reset_index()\n",
    "    for index, row in avg_results_lb.iterrows():\n",
    "        table_row = {\"Latent Dim\": int(row['latent_dim']), \"Dataset\": \"LongBench (Avg)\", \"Avg. AE PPL\": f\"{row['Avg_AE_PPL']:.2f}\", \"Avg. Raw (32-bit) PPL\": f\"{row['Avg_Raw_PPL']:.2f}\"}\n",
    "        table_row.update({f\"Avg. Raw ({b}-bit) PPL\": f\"{row[f'Avg_Raw_{b}bit_PPL']:.2f}\" for b in quantization_levels})\n",
    "        table_data.append(table_row)\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "display(table_df)\n",
    "\n",
    "# Create LaTeX Table\n",
    "latex_table = table_df.to_latex(index=False, float_format=\"%.2f\", caption=\"Perplexity Comparison: AE vs. Raw Quantization Levels\", label=\"tab:perplexity_comparison_all_quant\")\n",
    "print(\"\\nLaTeX Table:\")\n",
    "print(latex_table)\n",
    "\n",
    "print(f\"\\nPerplexity comparison plots (log y-axis) and table (with LaTeX output) saved/printed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latent Dim</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>AE PPL</th>\n",
       "      <th>Raw (2-bit) PPL</th>\n",
       "      <th>Raw (4-bit) PPL</th>\n",
       "      <th>Raw (8-bit) PPL</th>\n",
       "      <th>Raw (16-bit) PPL</th>\n",
       "      <th>Raw (32-bit) PPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>5049.08</td>\n",
       "      <td>1310.36</td>\n",
       "      <td>104.02</td>\n",
       "      <td>76.30</td>\n",
       "      <td>76.19</td>\n",
       "      <td>76.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>2192.71</td>\n",
       "      <td>1150.16</td>\n",
       "      <td>97.59</td>\n",
       "      <td>72.62</td>\n",
       "      <td>72.65</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>167.00</td>\n",
       "      <td>1150.16</td>\n",
       "      <td>97.59</td>\n",
       "      <td>72.62</td>\n",
       "      <td>72.65</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latent Dim   Dataset   AE PPL Raw (2-bit) PPL Raw (4-bit) PPL  \\\n",
       "0           8  WikiText  5049.08         1310.36          104.02   \n",
       "1          16  WikiText  2192.71         1150.16           97.59   \n",
       "2          32  WikiText   167.00         1150.16           97.59   \n",
       "\n",
       "  Raw (8-bit) PPL Raw (16-bit) PPL Raw (32-bit) PPL  \n",
       "0           76.30            76.19            76.19  \n",
       "1           72.62            72.65            72.66  \n",
       "2           72.62            72.65            72.66  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Table:\n",
      "\\begin{table}\n",
      "\\caption{Perplexity Comparison: AE vs. Raw Quantization Levels}\n",
      "\\label{tab:perplexity_comparison_all_quant}\n",
      "\\begin{tabular}{rlllllll}\n",
      "\\toprule\n",
      "Latent Dim & Dataset & AE PPL & Raw (2-bit) PPL & Raw (4-bit) PPL & Raw (8-bit) PPL & Raw (16-bit) PPL & Raw (32-bit) PPL \\\\\n",
      "\\midrule\n",
      "8 & WikiText & 5049.08 & 1310.36 & 104.02 & 76.30 & 76.19 & 76.19 \\\\\n",
      "16 & WikiText & 2192.71 & 1150.16 & 97.59 & 72.62 & 72.65 & 72.66 \\\\\n",
      "32 & WikiText & 167.00 & 1150.16 & 97.59 & 72.62 & 72.65 & 72.66 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "Perplexity comparison plots (log y-axis) and table (with LaTeX output) saved/printed.\n"
     ]
    }
   ],
   "source": [
    "# KV‑cache compression analysis (open‑source only)\n",
    "# ------------------------------------------------\n",
    "import json, glob, re, statistics, itertools, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "# Ensure the plots directory exists\n",
    "plots_dir = \"./plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# 1) locate result files in the current directory\n",
    "files = glob.glob(\"benchmark_results_distilgpt2_*.json\")\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No benchmark_results_distilgpt2_*.json files in this folder!\")\n",
    "\n",
    "records: List[Dict[str, Any]] = []\n",
    "\n",
    "# Function to extract latent dimension from filename\n",
    "def get_latent_dim(filename: str) -> Optional[int]:\n",
    "    match_ld = re.search(r\"distilgpt2_(\\d+)\\.json$\", filename)\n",
    "    if match_ld:\n",
    "        return int(match_ld.group(1))\n",
    "    return None\n",
    "\n",
    "quantization_levels = [2, 4, 8, 16, 32]\n",
    "\n",
    "for path in files:\n",
    "    latent_dim = get_latent_dim(path)\n",
    "    if latent_dim is not None:\n",
    "        with open(path) as fp:\n",
    "            res = json.load(fp)\n",
    "\n",
    "        raw_ppl: float = res[\"raw_baseline_ppl\"]\n",
    "        ae_ppl_none: Optional[float] = res[\"perplexities\"].get(\"none\", {}).get(\"ae_compressed_ppl\")\n",
    "        kv_ppl_quant = {bits: res[\"perplexities\"].get(str(bits), {}).get(\"kv_cache_baseline_ppl\") for bits in quantization_levels}\n",
    "\n",
    "        if raw_ppl is not None and ae_ppl_none is not None:\n",
    "            record = {\"latent_dim\": latent_dim, \"raw_ppl\": raw_ppl, \"ae_ppl\": ae_ppl_none, \"dataset\": \"WikiText\"}\n",
    "            record.update({f\"raw_quant{b}_ppl\": kv_ppl_quant[b] for b in quantization_levels})\n",
    "            records.append(record)\n",
    "\n",
    "        lb_baseline = res.get(\"longbench\", {}).get(\"baseline\", {})\n",
    "        lb_compressed_none = res.get(\"longbench\", {}).get(\"compressed\", {}).get(\"none\", {})\n",
    "        lb_compressed_quant = {\n",
    "            bits: res.get(\"longbench\", {}).get(\"compressed\", {}).get(str(bits), {}) for bits in quantization_levels\n",
    "        }\n",
    "\n",
    "        for task, base_ppl in lb_baseline.items():\n",
    "            comp_ppl_none = lb_compressed_none.get(task)\n",
    "            comp_ppl_quant = {bits: lb_compressed_quant[bits].get(task) for bits in quantization_levels}\n",
    "            if comp_ppl_none is not None:\n",
    "                record = {\"latent_dim\": latent_dim, \"raw_ppl\": base_ppl, \"ae_ppl\": comp_ppl_none, \"dataset\": f\"LongBench ({task})\"}\n",
    "                record.update({f\"raw_quant{b}_ppl\": comp_ppl_quant[b] for b in quantization_levels})\n",
    "                records.append(record)\n",
    "\n",
    "df = pd.DataFrame(records).sort_values([\"latent_dim\", \"dataset\"]).reset_index(drop=True)\n",
    "\n",
    "latent_dims = sorted(df['latent_dim'].dropna().unique())\n",
    "longbench_tasks = sorted([d.split('(')[1][:-1] for d in df['dataset'].unique() if d.startswith(\"LongBench\")])\n",
    "\n",
    "# Plot for WikiText Perplexity Comparison (Log Scale) - Separate Plots\n",
    "wiki_data = df[df['dataset'] == 'WikiText'].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'] + [f'raw_quant{b}_ppl' for b in quantization_levels]).sort_values('latent_dim')\n",
    "if not wiki_data.empty:\n",
    "    for bits in quantization_levels:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(wiki_data['latent_dim'], wiki_data[f'raw_quant{bits}_ppl'], marker='v', label=f'Raw ({bits}-bit)')\n",
    "        plt.plot(wiki_data['latent_dim'], wiki_data['ae_ppl'], marker='x', label='AE')\n",
    "        plt.xlabel(\"Latent Dimension\")\n",
    "        plt.ylabel(\"Perplexity (Log Scale)\")\n",
    "        plt.title(f\"WikiText Perplexity vs. Latent Dim - vs. Raw ({bits}-bit)\")\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.grid(True, which=\"both\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, f\"exp1_wikitext_perplexity_vs_latent_dim_quant{bits}_logy.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Plot for LongBench Perplexity Comparison (averaged across tasks, Log Scale) - Separate Plots\n",
    "if longbench_tasks:\n",
    "    longbench_data = df[df['dataset'].str.startswith('LongBench')].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'] + [f'raw_quant{b}_ppl' for b in quantization_levels])\n",
    "    if not longbench_data.empty:\n",
    "        avg_ae_ppl_lb = longbench_data.groupby('latent_dim')['ae_ppl'].mean()\n",
    "        avg_raw_quant_ppl_lb = {\n",
    "            bits: longbench_data.groupby('latent_dim')[f'raw_quant{bits}_ppl'].mean() for bits in quantization_levels\n",
    "        }\n",
    "\n",
    "        for bits in quantization_levels:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(avg_raw_quant_ppl_lb[bits].index, avg_raw_quant_ppl_lb[bits].values, marker='v', label=f'Avg. Raw ({bits}-bit)')\n",
    "            plt.plot(avg_ae_ppl_lb.index, avg_ae_ppl_lb.values, marker='x', label='Avg. AE')\n",
    "            plt.xlabel(\"Latent Dimension\")\n",
    "            plt.ylabel(\"Average Perplexity (LongBench, Log Scale)\")\n",
    "            plt.title(f\"Avg. LongBench PPL vs. Latent Dim - vs. Raw ({bits}-bit)\")\n",
    "            plt.yscale('log')\n",
    "            plt.legend()\n",
    "            plt.grid(True, which=\"both\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(plots_dir, f\"exp1_longbench_avg_perplexity_vs_latent_dim_quant{bits}_logy.png\"))\n",
    "            plt.close()\n",
    "\n",
    "# Create Table Data (as before)\n",
    "table_data = []\n",
    "wiki_data_table = df[df['dataset'] == 'WikiText'].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'] + [f'raw_quant{b}_ppl' for b in quantization_levels]).sort_values('latent_dim')\n",
    "if not wiki_data_table.empty:\n",
    "    for index, row in wiki_data_table.iterrows():\n",
    "        table_row = {\"Latent Dim\": int(row['latent_dim']), \"Dataset\": \"WikiText\", \"AE PPL\": f\"{row['ae_ppl']:.2f}\"}\n",
    "        table_row.update({f\"Raw ({b}-bit) PPL\": f\"{row[f'raw_quant{b}_ppl']:.2f}\" for b in quantization_levels})\n",
    "        table_data.append(table_row)\n",
    "\n",
    "longbench_data_table = df[df['dataset'].str.startswith('LongBench')].dropna(subset=['latent_dim', 'raw_ppl', 'ae_ppl'] + [f'raw_quant{b}_ppl' for b in quantization_levels])\n",
    "if not longbench_data_table.empty:\n",
    "    agg_funcs = {'Avg_AE_PPL': ('ae_ppl', 'mean')}\n",
    "    agg_funcs.update({f'Avg_Raw_{b}bit_PPL': (f'raw_quant{b}_ppl', 'mean') for b in quantization_levels})\n",
    "    avg_results_lb = longbench_data_table.groupby('latent_dim').agg(**agg_funcs).reset_index()\n",
    "    for index, row in avg_results_lb.iterrows():\n",
    "        table_row = {\"Latent Dim\": int(row['latent_dim']), \"Dataset\": \"LongBench (Avg)\", \"Avg. AE PPL\": f\"{row['Avg_AE_PPL']:.2f}\"}\n",
    "        table_row.update({f\"Avg. Raw ({b}-bit) PPL\": f\"{row[f'Avg_Raw_{b}bit_PPL']:.2f}\" for b in quantization_levels})\n",
    "        table_data.append(table_row)\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "display(table_df)\n",
    "\n",
    "# Create LaTeX Table (as before)\n",
    "latex_table = table_df.to_latex(index=False, float_format=\"%.2f\", caption=\"Perplexity Comparison: AE vs. Raw Quantization Levels\", label=\"tab:perplexity_comparison_all_quant\")\n",
    "print(\"\\nLaTeX Table:\")\n",
    "print(latex_table)\n",
    "\n",
    "print(f\"\\nPerplexity comparison plots (log y-axis) and table (with LaTeX output) saved/printed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Encoder Layers</th>\n",
       "      <th>Decoder Layers</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Asym. AE PPL</th>\n",
       "      <th>Raw (2-bit) PPL</th>\n",
       "      <th>Raw (4-bit) PPL</th>\n",
       "      <th>Raw (8-bit) PPL</th>\n",
       "      <th>Raw (16-bit) PPL</th>\n",
       "      <th>Raw (32-bit) PPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>167.00</td>\n",
       "      <td>1150.16</td>\n",
       "      <td>97.59</td>\n",
       "      <td>72.62</td>\n",
       "      <td>72.65</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>166.95</td>\n",
       "      <td>1150.16</td>\n",
       "      <td>97.59</td>\n",
       "      <td>72.66</td>\n",
       "      <td>72.65</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>275.12</td>\n",
       "      <td>1150.16</td>\n",
       "      <td>97.59</td>\n",
       "      <td>72.62</td>\n",
       "      <td>72.65</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>WikiText</td>\n",
       "      <td>163.30</td>\n",
       "      <td>1150.16</td>\n",
       "      <td>97.59</td>\n",
       "      <td>72.62</td>\n",
       "      <td>72.65</td>\n",
       "      <td>72.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Encoder Layers  Decoder Layers   Dataset Asym. AE PPL Raw (2-bit) PPL  \\\n",
       "0               1               1  WikiText       167.00         1150.16   \n",
       "1               2               1  WikiText       166.95         1150.16   \n",
       "2               2               2  WikiText       275.12         1150.16   \n",
       "3               3               1  WikiText       163.30         1150.16   \n",
       "\n",
       "  Raw (4-bit) PPL Raw (8-bit) PPL Raw (16-bit) PPL Raw (32-bit) PPL  \n",
       "0           97.59           72.62            72.65            72.66  \n",
       "1           97.59           72.66            72.65            72.66  \n",
       "2           97.59           72.62            72.65            72.66  \n",
       "3           97.59           72.62            72.65            72.66  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Table (Asymmetric AE - Layer Ratio):\n",
      "\n",
      "\\begin{table}\n",
      "\\caption{Perplexity Comparison: Asymmetric AE (Layer Ratio) vs. Raw Quantization Levels}\n",
      "\\label{tab:asym_perplexity_comparison_layer_ratio}\n",
      "\\begin{tabular}{rrlllllll}\n",
      "\\toprule\n",
      "Encoder Layers & Decoder Layers & Dataset & Asym. AE PPL & Raw (2-bit) PPL & Raw (4-bit) PPL & Raw (8-bit) PPL & Raw (16-bit) PPL & Raw (32-bit) PPL \\\\\n",
      "\\midrule\n",
      "1 & 1 & WikiText & 167.00 & 1150.16 & 97.59 & 72.62 & 72.65 & 72.66 \\\\\n",
      "2 & 1 & WikiText & 166.95 & 1150.16 & 97.59 & 72.66 & 72.65 & 72.66 \\\\\n",
      "2 & 2 & WikiText & 275.12 & 1150.16 & 97.59 & 72.62 & 72.65 & 72.66 \\\\\n",
      "3 & 1 & WikiText & 163.30 & 1150.16 & 97.59 & 72.62 & 72.65 & 72.66 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KV‑cache compression analysis (open‑source only) - Asymmetric AE (Layer Ratio) - Table Only\n",
    "# ------------------------------------------------\n",
    "import json, glob, re, statistics, itertools, os\n",
    "import pandas as pd\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "# 1) locate result files for asymmetric AE (layer ratio)\n",
    "files = glob.glob(\"benchmark_results_distilgpt2_ae_*.json\")\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No benchmark_results_distilgpt2_ae_*.json files found!\")\n",
    "\n",
    "records: List[Dict[str, Any]] = []\n",
    "\n",
    "# Function to extract encoder and decoder layer counts from filename\n",
    "def get_layer_counts_asym(filename: str) -> Optional[tuple[int, int]]:\n",
    "    match_ratio = re.search(r\"distilgpt2_ae_(\\d+)_(\\d+)\\.json$\", filename)\n",
    "    if match_ratio:\n",
    "        encoder_layers = int(match_ratio.group(1))\n",
    "        decoder_layers = int(match_ratio.group(2))\n",
    "        return (encoder_layers, decoder_layers)\n",
    "    return None\n",
    "\n",
    "quantization_levels = [2, 4, 8, 16, 32]\n",
    "\n",
    "for path in files:\n",
    "    layer_counts = get_layer_counts_asym(path)\n",
    "    if layer_counts:\n",
    "        encoder_layers, decoder_layers = layer_counts\n",
    "        config_str = f\"{encoder_layers}:{decoder_layers}\"\n",
    "        with open(path) as fp:\n",
    "            res = json.load(fp)\n",
    "\n",
    "        raw_ppl: float = res[\"raw_baseline_ppl\"]\n",
    "        ae_ppl_none: Optional[float] = res[\"perplexities\"].get(\"none\", {}).get(\"ae_compressed_ppl\")\n",
    "        kv_ppl_quant = {str(bits): res[\"perplexities\"].get(str(bits), {}).get(\"kv_cache_baseline_ppl\") for bits in quantization_levels}\n",
    "\n",
    "        if raw_ppl is not None and ae_ppl_none is not None:\n",
    "            records.append(dict(\n",
    "                encoder_layers=encoder_layers,\n",
    "                decoder_layers=decoder_layers,\n",
    "                config=config_str,\n",
    "                raw_ppl=raw_ppl,\n",
    "                ae_ppl=ae_ppl_none,\n",
    "                dataset=\"WikiText\"\n",
    "            ))\n",
    "            for bits_str, ppl in kv_ppl_quant.items():\n",
    "                if ppl is not None:\n",
    "                    records.append(dict(\n",
    "                        encoder_layers=encoder_layers,\n",
    "                        decoder_layers=decoder_layers,\n",
    "                        config=config_str,\n",
    "                        raw_ppl=raw_ppl,\n",
    "                        ae_ppl=ppl,\n",
    "                        dataset=f\"WikiText (Raw {bits_str}-bit)\"\n",
    "                    ))\n",
    "\n",
    "        lb_baseline = res.get(\"longbench\", {}).get(\"baseline\", {})\n",
    "        lb_compressed_none = res.get(\"longbench\", {}).get(\"compressed\", {}).get(\"none\", {})\n",
    "        lb_compressed_quant = {\n",
    "            str(bits): res.get(\"longbench\", {}).get(\"compressed\", {}).get(str(bits), {}) for bits in quantization_levels\n",
    "        }\n",
    "\n",
    "        for task, base_ppl in lb_baseline.items():\n",
    "            comp_ppl_none = lb_compressed_none.get(task)\n",
    "            if comp_ppl_none is not None:\n",
    "                records.append(dict(\n",
    "                    encoder_layers=encoder_layers,\n",
    "                    decoder_layers=decoder_layers,\n",
    "                    config=config_str,\n",
    "                    raw_ppl=base_ppl,\n",
    "                    ae_ppl=comp_ppl_none,\n",
    "                    dataset=f\"LongBench ({task})\"\n",
    "                ))\n",
    "                for bits_str, quant_data in lb_compressed_quant.items():\n",
    "                    raw_quant_ppl_lb = quant_data.get(task)\n",
    "                    if raw_quant_ppl_lb is not None:\n",
    "                        records.append(dict(\n",
    "                            encoder_layers=encoder_layers,\n",
    "                            decoder_layers=decoder_layers,\n",
    "                            config=config_str,\n",
    "                            raw_ppl=base_ppl,\n",
    "                            ae_ppl=raw_quant_ppl_lb,\n",
    "                            dataset=f\"LongBench ({task}) (Raw {bits_str}-bit)\"\n",
    "                        ))\n",
    "\n",
    "df = pd.DataFrame(records).sort_values([\"encoder_layers\", \"decoder_layers\", \"dataset\"]).reset_index(drop=True)\n",
    "\n",
    "# Create Table Data for Asymmetric AE (Layer Ratio)\n",
    "table_data_asym = []\n",
    "wiki_data_table_asym = df[df['dataset'] == 'WikiText'].dropna(subset=['encoder_layers', 'decoder_layers', 'ae_ppl']).sort_values(['encoder_layers', 'decoder_layers'])\n",
    "if not wiki_data_table_asym.empty:\n",
    "    for index, row in wiki_data_table_asym.iterrows():\n",
    "        table_row = {\"Encoder Layers\": int(row['encoder_layers']), \"Decoder Layers\": int(row['decoder_layers']), \"Dataset\": \"WikiText\", \"Asym. AE PPL\": f\"{row['ae_ppl']:.2f}\"}\n",
    "        for bits in quantization_levels:\n",
    "            raw_quant_row = df[(df['dataset'] == f'WikiText (Raw {bits}-bit)') & (df['encoder_layers'] == row['encoder_layers']) & (df['decoder_layers'] == row['decoder_layers'])]\n",
    "            if not raw_quant_row.empty:\n",
    "                table_row[f\"Raw ({bits}-bit) PPL\"] = f\"{raw_quant_row['ae_ppl'].iloc[0]:.2f}\"\n",
    "            else:\n",
    "                table_row[f\"Raw ({bits}-bit) PPL\"] = \"N/A\"\n",
    "        table_data_asym.append(table_row)\n",
    "\n",
    "longbench_ae_table_data = df[df['dataset'].str.startswith('LongBench') & (df['dataset'].str.find('(Raw') == -1)].dropna(subset=['encoder_layers', 'decoder_layers', 'ae_ppl'])\n",
    "if not longbench_ae_table_data.empty:\n",
    "    grouped_lb = longbench_ae_table_data.groupby(['encoder_layers', 'decoder_layers'])['ae_ppl'].mean().reset_index()\n",
    "    for index, row in grouped_lb.iterrows():\n",
    "        table_row = {\"Encoder Layers\": int(row['encoder_layers']), \"Decoder Layers\": int(row['decoder_layers']), \"Dataset\": \"LongBench (Avg)\", \"Avg. Asym. AE PPL\": f\"{row['ae_ppl']:.2f}\"}\n",
    "        for bits in quantization_levels:\n",
    "            avg_raw_quant_ppls = []\n",
    "            for task in longbench_tasks:\n",
    "                raw_data = df[(df['dataset'] == f'LongBench ({task}) (Raw {bits}-bit)') & (df['encoder_layers'] == row['encoder_layers']) & (df['decoder_layers'] == row['decoder_layers'])]\n",
    "                if not raw_data.empty:\n",
    "                    avg_raw_quant_ppls.append(raw_data['ae_ppl'].mean())\n",
    "            if avg_raw_quant_ppls:\n",
    "                table_row[f\"Avg. Raw ({bits}-bit) PPL\"] = f\"{statistics.mean(avg_raw_quant_ppls):.2f}\"\n",
    "            else:\n",
    "                table_row[f\"Avg. Raw ({bits}-bit) PPL\"] = \"N/A\"\n",
    "        table_data_asym.append(table_row)\n",
    "\n",
    "table_df_asym = pd.DataFrame(table_data_asym)\n",
    "display(table_df_asym)\n",
    "\n",
    "# Create LaTeX Table for Asymmetric AE (Layer Ratio)\n",
    "latex_table_asym = table_df_asym.to_latex(index=False, float_format=\"%.2f\", caption=\"Perplexity Comparison: Asymmetric AE (Layer Ratio) vs. Raw Quantization Levels\", label=\"tab:asym_perplexity_comparison_layer_ratio\")\n",
    "print(\"\\nLaTeX Table (Asymmetric AE - Layer Ratio):\\n\")\n",
    "print(latex_table_asym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Latent Dimension</th>\n",
       "      <th>Average AE Decompression Speed (s/token)</th>\n",
       "      <th>Average AE Compressed Overhead (s/token)</th>\n",
       "      <th>Average Raw Baseline Overhead (s/token)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0.042156</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>0.006337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>0.041232</td>\n",
       "      <td>0.006514</td>\n",
       "      <td>0.006337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>0.042075</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.006337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Latent Dimension Average AE Decompression Speed (s/token)  \\\n",
       "0                 8                                 0.042156   \n",
       "1                16                                 0.041232   \n",
       "2                32                                 0.042075   \n",
       "\n",
       "  Average AE Compressed Overhead (s/token)  \\\n",
       "0                                 0.006435   \n",
       "1                                 0.006514   \n",
       "2                                 0.006660   \n",
       "\n",
       "  Average Raw Baseline Overhead (s/token)  \n",
       "0                                0.006337  \n",
       "1                                0.006337  \n",
       "2                                0.006337  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Table (Experiment 2 - Averaged):\n",
      "\n",
      "\\begin{table}\n",
      "\\caption{Average Decompression Speed and Overhead vs. Autoencoder Latent Dimension}\n",
      "\\label{tab:exp2_avg_speed_overhead_vs_latent_dim}\n",
      "\\begin{tabular}{rlll}\n",
      "\\toprule\n",
      "Latent Dimension & Average AE Decompression Speed (s/token) & Average AE Compressed Overhead (s/token) & Average Raw Baseline Overhead (s/token) \\\\\n",
      "\\midrule\n",
      "8 & 0.042156 & 0.006435 & 0.006337 \\\\\n",
      "16 & 0.041232 & 0.006514 & 0.006337 \\\\\n",
      "32 & 0.042075 & 0.006660 & 0.006337 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n",
      "\n",
      "Averaged plots and LaTeX table for Experiment 2 (Decompression Speed vs. Latent Dimension) created.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Ensure the plots directory exists\n",
    "plots_dir = \"./plots_experiment_2_averaged\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob(\"benchmark_results_distilgpt2_*.json\")\n",
    "relevant_files = [f for f in files if f.endswith((\"8.json\", \"16.json\", \"32.json\"))]\n",
    "\n",
    "if not relevant_files:\n",
    "    raise FileNotFoundError(\"No benchmark_results_distilgpt2_{8,16,32}.json files found!\")\n",
    "\n",
    "data_points = []\n",
    "\n",
    "for path in relevant_files:\n",
    "    latent_dim = int(path.split('_')[-1].split('.')[0])\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        data_points.append({\n",
    "            \"latent_dim\": latent_dim,\n",
    "            \"ttft_ae_none\": data[\"perplexities\"][\"none\"].get(\"avg_decompression_speed_per_token_s\"),\n",
    "            \"overhead_ae_none\": data[\"perplexities\"][\"none\"].get(\"ae_compressed_overhead_per_token_s\"),\n",
    "            \"ttft_raw_none\": data[\"perplexities\"][\"none\"].get(\"kv_cache_baseline_overhead_per_token_s\"),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data_points)\n",
    "\n",
    "# Average the values for each latent dimension\n",
    "averaged_df = df.groupby(\"latent_dim\").agg(\n",
    "    avg_ttft_ae_none=(\"ttft_ae_none\", \"mean\"),\n",
    "    avg_overhead_ae_none=(\"overhead_ae_none\", \"mean\"),\n",
    "    first_ttft_raw_none=(\"ttft_raw_none\", \"first\") # Baseline should be consistent, so take the first value\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the average raw baseline overhead\n",
    "avg_ttft_raw_none = df[\"ttft_raw_none\"].mean()\n",
    "\n",
    "# Plotting Averaged Time to First Token (TTFT) - AE Compressed (None)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(averaged_df[\"latent_dim\"], averaged_df[\"avg_ttft_ae_none\"], marker='o', linestyle='-', label=\"Average AE Decompression Speed\")\n",
    "plt.xlabel(\"Autoencoder Latent Dimension\")\n",
    "plt.ylabel(\"Average Decompression Speed (s/token)\")\n",
    "plt.title(\"Average Decompression Speed vs. Autoencoder Latent Dimension\")\n",
    "plt.xticks(averaged_df[\"latent_dim\"])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"exp2_avg_decompression_speed_vs_latent_dim.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Plotting Averaged Overheads\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(averaged_df[\"latent_dim\"], averaged_df[\"avg_overhead_ae_none\"], marker='o', linestyle='-', label=\"Average AE Compressed Overhead\")\n",
    "plt.axhline(y=avg_ttft_raw_none, linestyle='--', color='r', label=f\"Average Raw Baseline Overhead ({avg_ttft_raw_none:.6f} s/token)\")\n",
    "plt.xlabel(\"Autoencoder Latent Dimension\")\n",
    "plt.ylabel(\"Average Overhead per Token (s)\")\n",
    "plt.title(\"Average Overhead vs. Autoencoder Latent Dimension\")\n",
    "plt.xticks(averaged_df[\"latent_dim\"])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"exp2_avg_overhead_vs_latent_dim.png\"))\n",
    "plt.close()\n",
    "\n",
    "# Create Table with Averaged Values\n",
    "table_data = []\n",
    "for index, row in averaged_df.iterrows():\n",
    "    table_data.append({\n",
    "        \"Latent Dimension\": int(row[\"latent_dim\"]),\n",
    "        \"Average AE Decompression Speed (s/token)\": f\"{row['avg_ttft_ae_none']:.6f}\",\n",
    "        \"Average AE Compressed Overhead (s/token)\": f\"{row['avg_overhead_ae_none']:.6f}\",\n",
    "        \"Average Raw Baseline Overhead (s/token)\": f\"{avg_ttft_raw_none:.6f}\",\n",
    "    })\n",
    "\n",
    "table_df = pd.DataFrame(table_data)\n",
    "display(table_df)\n",
    "\n",
    "# Generate LaTeX Table with Averaged Values\n",
    "latex_table = table_df.to_latex(index=False, float_format=\"%.6f\", caption=\"Average Decompression Speed and Overhead vs. Autoencoder Latent Dimension\", label=\"tab:exp2_avg_speed_overhead_vs_latent_dim\")\n",
    "\n",
    "print(\"\\nLaTeX Table (Experiment 2 - Averaged):\\n\")\n",
    "print(latex_table)\n",
    "\n",
    "print(f\"\\nAveraged plots and LaTeX table for Experiment 2 (Decompression Speed vs. Latent Dimension) created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated pairwise and combined plots for Experiment 4 in the './plots_experiment_4_comparisons' directory.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Ensure the plots directory exists\n",
    "plots_dir = \"./plots_experiment_4_comparisons\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob(\"benchmark_results_distilgpt2_ae_*.json\")\n",
    "\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No benchmark_results_distilgpt2_ae_*.json files found!\")\n",
    "\n",
    "quantization_levels = [2, 4, 8, 16, 32]\n",
    "asym_ae_results = {}\n",
    "raw_baseline_ppl = {}\n",
    "\n",
    "def get_layer_counts_asym(filename: str) -> Optional[tuple[int, int]]:\n",
    "    match_ratio = re.search(r\"distilgpt2_ae_(\\d+)_(\\d+)\\.json$\", filename)\n",
    "    if match_ratio:\n",
    "        encoder_layers = int(match_ratio.group(1))\n",
    "        decoder_layers = int(match_ratio.group(2))\n",
    "        return (encoder_layers, decoder_layers)\n",
    "    return None\n",
    "\n",
    "for path in files:\n",
    "    layer_counts = get_layer_counts_asym(path)\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "        for bits in quantization_levels:\n",
    "            raw_ppl = data[\"perplexities\"].get(str(bits), {}).get(\"kv_cache_baseline_ppl\")\n",
    "            if raw_ppl is not None:\n",
    "                raw_baseline_ppl.setdefault(bits, []).append(raw_ppl)\n",
    "\n",
    "        if layer_counts and layer_counts[0] != layer_counts[1]:  # Identify asymmetric AEs\n",
    "            encoder_layers, decoder_layers = layer_counts\n",
    "            config_str = f\"{encoder_layers}:{decoder_layers}\"\n",
    "            asym_ae_results.setdefault(config_str, [])\n",
    "            for bits in quantization_levels:\n",
    "                ae_ppl = data[\"perplexities\"].get(str(bits), {}).get(\"ae_compressed_ppl\")\n",
    "                if ae_ppl is not None:\n",
    "                    asym_ae_results[config_str].append({\"bits\": bits, \"ppl\": ae_ppl})\n",
    "\n",
    "# Average raw baseline perplexity for each bit level\n",
    "avg_raw_baseline_ppl = {bits: sum(ppls) / len(ppls) if ppls else None for bits, ppls in raw_baseline_ppl.items()}\n",
    "\n",
    "# Generate pairwise comparison plots\n",
    "for config, data in asym_ae_results.items():\n",
    "    df_ae = pd.DataFrame(data).sort_values(\"bits\")\n",
    "    if not df_ae.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(df_ae[\"bits\"], df_ae[\"ppl\"], marker='o', linestyle='-', label=f\"Asym. AE ({config}) + Quant.\")\n",
    "        if avg_raw_baseline_ppl:\n",
    "            raw_ppl_values = [avg_raw_baseline_ppl.get(bit) for bit in df_ae[\"bits\"]]\n",
    "            plt.plot(df_ae[\"bits\"], raw_ppl_values, marker='x', linestyle='--', label=\"Average Raw Quantization\")\n",
    "        plt.xlabel(\"Quantization Bits\")\n",
    "        plt.ylabel(\"Perplexity (Log Scale)\")\n",
    "        plt.title(f\"Perplexity vs. Quantization Bits (Asym. AE: {config} vs. Avg. Raw)\")\n",
    "        plt.yscale(\"log\")\n",
    "        plt.xticks(quantization_levels)\n",
    "        plt.grid(True, which=\"both\", linestyle=\"-\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, f\"exp4_pairwise_perplexity_vs_quant_bits_asym_ae_{config.replace(':', '_')}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Generate combined plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "if avg_raw_baseline_ppl:\n",
    "    plt.plot(quantization_levels, [avg_raw_baseline_ppl.get(bit) for bit in quantization_levels], marker='s', linestyle='-', label=\"Average Raw Quantization\", color='black')\n",
    "\n",
    "for config, data in asym_ae_results.items():\n",
    "    df_ae = pd.DataFrame(data).sort_values(\"bits\")\n",
    "    if not df_ae.empty:\n",
    "        plt.plot(df_ae[\"bits\"], df_ae[\"ppl\"], marker='o', linestyle='-', label=f\"Asym. AE ({config}) + Quant.\")\n",
    "\n",
    "plt.xlabel(\"Quantization Bits\")\n",
    "plt.ylabel(\"Perplexity (Log Scale)\")\n",
    "plt.title(\"Perplexity vs. Quantization Bits (All Asym. AEs vs. Avg. Raw)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(quantization_levels)\n",
    "plt.grid(True, which=\"both\", linestyle=\"-\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"exp4_combined_perplexity_vs_quant_bits_asym_aes.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nGenerated pairwise and combined plots for Experiment 4 in the '{plots_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated comparison plot for Asymmetric AE 2:2 vs 2:1 in the './plots_experiment_4_comparison_2_2_vs_2_1' directory.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Ensure the plots directory exists\n",
    "plots_dir = \"./plots_experiment_4_comparison_2_2_vs_2_1\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "files = glob.glob(\"benchmark_results_distilgpt2_ae_*.json\")\n",
    "\n",
    "if not files:\n",
    "    raise FileNotFoundError(\"No benchmark_results_distilgpt2_ae_*.json files found!\")\n",
    "\n",
    "quantization_levels = [2, 4, 8, 16, 32]\n",
    "comparison_results = {}\n",
    "\n",
    "def get_layer_counts_asym(filename: str) -> Optional[tuple[int, int]]:\n",
    "    match_ratio = re.search(r\"distilgpt2_ae_(\\d+)_(\\d+)\\.json$\", filename)\n",
    "    if match_ratio:\n",
    "        encoder_layers = int(match_ratio.group(1))\n",
    "        decoder_layers = int(match_ratio.group(2))\n",
    "        return (encoder_layers, decoder_layers)\n",
    "    return None\n",
    "\n",
    "for path in files:\n",
    "    layer_counts = get_layer_counts_asym(path)\n",
    "    if layer_counts and layer_counts in [(2, 2), (2, 1)]:\n",
    "        encoder_layers, decoder_layers = layer_counts\n",
    "        config_str = f\"{encoder_layers}:{decoder_layers}\"\n",
    "        comparison_results.setdefault(config_str, [])\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "            for bits in quantization_levels:\n",
    "                ae_ppl = data[\"perplexities\"].get(str(bits), {}).get(\"ae_compressed_ppl\")\n",
    "                if ae_ppl is not None:\n",
    "                    comparison_results[config_str].append({\"bits\": bits, \"ppl\": ae_ppl})\n",
    "\n",
    "# Generate comparison plot for 2:2 vs 2:1\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for config, data in comparison_results.items():\n",
    "    df_ae = pd.DataFrame(data).sort_values(\"bits\")\n",
    "    if not df_ae.empty:\n",
    "        plt.plot(df_ae[\"bits\"], df_ae[\"ppl\"], marker='o', linestyle='-', label=f\"Asym. AE ({config}) + Quant.\")\n",
    "\n",
    "plt.xlabel(\"Quantization Bits\")\n",
    "plt.ylabel(\"Perplexity (Log Scale)\")\n",
    "plt.title(\"Perplexity vs. Quantization Bits (Asym. AE 2:2 vs 2:1)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(quantization_levels)\n",
    "plt.grid(True, which=\"both\", linestyle=\"-\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"exp4_comparison_perplexity_vs_quant_bits_ae_2_2_vs_2_1.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nGenerated comparison plot for Asymmetric AE 2:2 vs 2:1 in the '{plots_dir}' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! See ./plots/wiki_sym_vs_asym.png and longbench_sym_vs_asym.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# compare_sym_vs_asym.py\n",
    "\n",
    "import os, glob, json, statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) setup\n",
    "plots_dir = \"./plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# find only the AE result files\n",
    "ae_files = glob.glob(\"benchmark_results_distilgpt2*_ae_*.json\")\n",
    "if not ae_files:\n",
    "    raise RuntimeError(\"No *ae_*.json files found\")\n",
    "\n",
    "quant_bits = [2, 4, 8, 16]\n",
    "\n",
    "# 2) gather per‐run stats, grouping symmetrics into one \"Sym\" bucket\n",
    "wiki_map = {}         # label -> list of wiki-ppls\n",
    "longbench_map = {}    # label -> list of {bit->avg longbench-ppl}\n",
    "\n",
    "for fp in ae_files:\n",
    "    name = os.path.basename(fp) \\\n",
    "             .replace(\"benchmark_results_distilgpt2_\",\"\") \\\n",
    "             .replace(\".json\",\"\")  # e.g. \"ae_2_1\"\n",
    "    # rename the two symmetric runs\n",
    "    if name in (\"ae_1_1\",\"ae_2_2\"):\n",
    "        label = \"Sym ae_1_1\"\n",
    "    else:\n",
    "        label = f\"Asym {name}\"\n",
    "\n",
    "    data = json.load(open(fp))\n",
    "\n",
    "    # WikiText PPL\n",
    "    w_ppl = data[\"perplexities\"][\"none\"][\"ae_compressed_ppl\"]\n",
    "    wiki_map.setdefault(label, []).append(w_ppl)\n",
    "\n",
    "    # LongBench average PPL over tasks\n",
    "    tasks = data[\"longbench\"][\"baseline\"].keys()\n",
    "    lb_avg = {\n",
    "        b: statistics.mean(\n",
    "             data[\"longbench\"][\"compressed\"][str(b)][task]\n",
    "             for task in tasks\n",
    "        )\n",
    "        for b in quant_bits\n",
    "    }\n",
    "    longbench_map.setdefault(label, []).append(lb_avg)\n",
    "\n",
    "# 3) average across runs per label\n",
    "wiki_avg = {lbl: statistics.mean(vals) for lbl, vals in wiki_map.items()}\n",
    "longbench_avg = {\n",
    "    lbl: {\n",
    "       b: statistics.mean(d[b] for d in dicts)\n",
    "       for b in quant_bits\n",
    "    }\n",
    "    for lbl, dicts in longbench_map.items()\n",
    "}\n",
    "\n",
    "# 4) plot WikiText bar chart\n",
    "plt.figure(figsize=(8,4))\n",
    "labels = list(wiki_avg.keys())\n",
    "ppls   = [wiki_avg[lbl] for lbl in labels]\n",
    "\n",
    "plt.bar(labels, ppls)\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Perplexity on WikiText\")\n",
    "plt.title(\"WikiText PPL: Sym vs. Asym AEs\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"wiki_sym_vs_asym.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 5) plot LongBench line chart\n",
    "plt.figure(figsize=(8,4))\n",
    "for lbl, series in longbench_avg.items():\n",
    "    plt.plot(quant_bits,\n",
    "             [series[b] for b in quant_bits],\n",
    "             marker=\"o\",\n",
    "             label=lbl)\n",
    "\n",
    "plt.xlim(min(quant_bits), max(quant_bits))\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Quantization bits\")\n",
    "plt.ylabel(\"Avg. Perplexity on LongBench\")\n",
    "plt.title(\"LongBench PPL vs Bits: Sym vs. Asym AEs\")\n",
    "plt.legend()\n",
    "plt.grid(which=\"both\", linestyle=\":\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"longbench_sym_vs_asym.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Done! See ./plots/wiki_sym_vs_asym.png and longbench_sym_vs_asym.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Group1 plots written to ./plots/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# compare_asymmetric_ae_group1_fixed.py\n",
    "\n",
    "import os, glob, json, statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plots_dir = \"./plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# pick your three runs\n",
    "group = [\"3_1\", \"2_1\", \"1_1\"]\n",
    "labels = [f\"AE_{x}\" for x in group]\n",
    "\n",
    "all_json = glob.glob(\"benchmark_results_distilgpt2*_ae_*.json\")\n",
    "group_files = {}\n",
    "for lbl, suf in zip(labels, group):\n",
    "    fn = next((f for f in all_json if f\"_ae_{suf}.json\" in f), None)\n",
    "    if not fn:\n",
    "        raise FileNotFoundError(f\"Couldn't find JSON for AE_{suf}\")\n",
    "    group_files[lbl] = fn\n",
    "\n",
    "# 1) WikiText\n",
    "wiki_ppls = {}\n",
    "for lbl, fn in group_files.items():\n",
    "    r = json.load(open(fn))\n",
    "    wiki_ppls[lbl] = r[\"perplexities\"][\"none\"][\"ae_compressed_ppl\"]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(wiki_ppls.keys(), wiki_ppls.values())\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Perplexity on WikiText\")\n",
    "plt.title(\"WikiText PPL: AE 3_1 vs 2_1 vs 1_1\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"wiki_asym_3_1_2_1_1_1.png\"))\n",
    "plt.close()\n",
    "\n",
    "# 2) LongBench\n",
    "quant_bits = [2,4,8,16]\n",
    "longbench_avg = {}\n",
    "\n",
    "for lbl, fn in group_files.items():\n",
    "    r = json.load(open(fn))\n",
    "    # get the list of tasks from the baseline\n",
    "    tasks = r[\"longbench\"][\"baseline\"].keys()\n",
    "    # average only the numeric PPLs per bit\n",
    "    avg_ppl = {\n",
    "      b: statistics.mean(\n",
    "           r[\"longbench\"][\"compressed\"][str(b)][task]\n",
    "           for task in tasks\n",
    "      )\n",
    "      for b in quant_bits\n",
    "    }\n",
    "    longbench_avg[lbl] = avg_ppl\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "for lbl, series in longbench_avg.items():\n",
    "    plt.plot(quant_bits,\n",
    "             [series[b] for b in quant_bits],\n",
    "             marker=\"o\", label=lbl)\n",
    "\n",
    "plt.xlim(min(quant_bits), max(quant_bits))\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Quantization bits\")\n",
    "plt.ylabel(\"Avg. Perplexity on LongBench\")\n",
    "plt.title(\"LongBench PPL vs Bits: AE 3_1 vs 2_1 vs 1_1\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"longbench_asym_3_1_2_1_1_1.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Group1 plots written to ./plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Group1 plots written to ./plots/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# compare_asymmetric_ae_group1.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Configuration ---\n",
    "plots_dir = \"./plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Which asymmetric-AE runs to compare\n",
    "group = [\"3_1\", \"2_1\", \"1_1\"]\n",
    "labels = [f\"AE_{s}\" for s in group]\n",
    "\n",
    "# Discover all AE JSON files\n",
    "all_json = glob.glob(\"benchmark_results_distilgpt2_ae_*.json\")\n",
    "if not all_json:\n",
    "    raise RuntimeError(\"No benchmark_results_distilgpt2_ae_*.json files found\")\n",
    "\n",
    "# Map labels to their file paths\n",
    "group_files = {}\n",
    "for lbl, suffix in zip(labels, group):\n",
    "    fn = next((f for f in all_json if f\"_ae_{suffix}.json\" in f), None)\n",
    "    if fn is None:\n",
    "        raise FileNotFoundError(f\"Couldn't find JSON for AE_{suffix}\")\n",
    "    group_files[lbl] = fn\n",
    "\n",
    "# --- 1) WikiText bar chart ---\n",
    "wiki_ppls = {}\n",
    "for lbl, fn in group_files.items():\n",
    "    data = json.load(open(fn))\n",
    "    wiki_ppls[lbl] = data[\"perplexities\"][\"none\"][\"ae_compressed_ppl\"]\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(wiki_ppls.keys(), wiki_ppls.values())\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Perplexity on WikiText\")\n",
    "plt.title(\"WikiText PPL: AE 3_1 vs 2_1 vs 1_1\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"wiki_asym_3_1_2_1_1_1.png\"))\n",
    "plt.close()\n",
    "\n",
    "# --- 2) LongBench line chart ---\n",
    "quant_bits = [2, 4, 8, 16]\n",
    "longbench_avg = {}\n",
    "\n",
    "for lbl, fn in group_files.items():\n",
    "    data = json.load(open(fn))\n",
    "    # Use baseline task list to pull only the numeric PPLs\n",
    "    tasks = data[\"longbench\"][\"baseline\"].keys()\n",
    "    avg_ppl = {\n",
    "        b: statistics.mean(\n",
    "            data[\"longbench\"][\"compressed\"][str(b)][task]\n",
    "            for task in tasks\n",
    "        )\n",
    "        for b in quant_bits\n",
    "    }\n",
    "    longbench_avg[lbl] = avg_ppl\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for lbl, series in longbench_avg.items():\n",
    "    plt.plot(\n",
    "        quant_bits,\n",
    "        [series[b] for b in quant_bits],\n",
    "        marker=\"o\",\n",
    "        label=lbl\n",
    "    )\n",
    "\n",
    "plt.xlim(min(quant_bits), max(quant_bits))\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"Quantization bits\")\n",
    "plt.ylabel(\"Avg. Perplexity on LongBench\")\n",
    "plt.title(\"LongBench PPL vs Bits: AE 3_1 vs 2_1 vs 1_1\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"longbench_asym_3_1_2_1_1_1.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Group1 plots written to ./plots/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Overhead comparison plot saved to ./plots/overhead_baseline_vs_ae.png\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# compare_overhead.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Setup\n",
    "plots_dir = \"./plots\"\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# 2) Locate the symmetric-latent=32 JSON (baseline reference)\n",
    "all_files = glob.glob(\"benchmark_results_distilgpt2_*.json\")\n",
    "ref_file = next(\n",
    "    f for f in all_files\n",
    "    if f.endswith(\"_32.json\") and \"_ae_\" not in f\n",
    ")\n",
    "with open(ref_file) as fp:\n",
    "    data = json.load(fp)\n",
    "\n",
    "# 3) Define the quantization levels (including the \"none\" case)\n",
    "levels = [\"none\", 2, 4, 8, 16, 32]\n",
    "\n",
    "# 4) Extract per‐token overheads for baseline vs. AE\n",
    "baseline_overhead = []\n",
    "ae_overhead       = []\n",
    "for lvl in levels:\n",
    "    key = str(lvl)\n",
    "    p = data[\"perplexities\"][key]\n",
    "    baseline_overhead.append(p[\"kv_cache_baseline_overhead_per_token_s\"])\n",
    "    ae_overhead.append(p[\"ae_compressed_overhead_per_token_s\"])\n",
    "\n",
    "# 5) Plot: grouped bar chart\n",
    "x = np.arange(len(levels))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(x - width/2, baseline_overhead, width, label=\"KV Baseline\")\n",
    "plt.bar(x + width/2, ae_overhead,       width, label=\"AE Compressed\")\n",
    "plt.xticks(x, [str(l) for l in levels])\n",
    "plt.xlabel(\"Quantization bits\")\n",
    "plt.ylabel(\"Overhead per token (s)\")\n",
    "plt.title(\"Per‐token Overhead: KV Baseline vs AE Compressed\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\":\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(plots_dir, \"overhead_baseline_vs_ae.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"✅ Overhead comparison plot saved to ./plots/overhead_baseline_vs_ae.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
