{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6KkToOAF1M"
      },
      "source": [
        "# Notebook to run learned kv compression scripts on colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prGnb-vKAnzs"
      },
      "source": [
        "Download Repo and Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YbnT8hMBAE49",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Download Repo\n",
        "%cd /content\n",
        "!rm -rf learned-kv-compression\n",
        "!git clone -b colab https://henro25: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@github.com/henro25/learned-kv-compression\n",
        "%cd /content/learned-kv-compression/\n",
        "%ls\n",
        "\n",
        "# Install Requirements\n",
        "%pip install -r colab_requirements.txt\n",
        "%pip uninstall gcsfs -y\n",
        "%pip install --upgrade fsspec==2025.3.2\n",
        "%pip install gcsfs==2024.12.0\n",
        "%pip install --upgrade datasets\n",
        "\n",
        "!apt-get update -y\n",
        "!apt-get install -y jq\n",
        "\n",
        "# Enable permissions if needed\n",
        "!chmod +x run_experiments.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7vdgntcAana"
      },
      "source": [
        "## Training the Autoencoder\n",
        "\n",
        "This trains an autoencoder that compresses each KV vector to a 16-dimensional latent representation using 1000 texts from WikiText-103."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7idaakurATWT"
      },
      "outputs": [],
      "source": [
        "!python -m src.dictionary_learning.train \\\n",
        "    --name distilgpt2 \\\n",
        "    --latent_dim 16 \\\n",
        "    --num_epochs 10 \\\n",
        "    --batch_size 32 \\\n",
        "    --output_dir models/distilgpt2_16 \\\n",
        "    --num_train_texts 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3sF0VkRbUZX"
      },
      "source": [
        "# Benchmarking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6oJq5qtZTwj"
      },
      "source": [
        "Run a quick test for KV Cache compression with minimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j_99ro74ZTZw"
      },
      "outputs": [],
      "source": [
        "!./quick_test.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WF2aid4bRLa"
      },
      "source": [
        "Run experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MNdpwUKhbQv8"
      },
      "outputs": [],
      "source": [
        "!./run_experiments.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ5GZpidFIp1"
      },
      "source": [
        "Eval with Perplexity and Longbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2koR1iA0a-Az"
      },
      "outputs": [],
      "source": [
        "!./src/evaluation/run_evaluation.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGbD_IPvy1uU"
      },
      "outputs": [],
      "source": [
        "!./run_experiments.sh configs/distilgpt2_experiment.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hi-kL2grLiK"
      },
      "source": [
        "## Qwen2.5-0.5B Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMANLyemhaTB",
        "outputId": "afae98ce-1496-4dda-c316-f652677d2e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== KV Cache Compression Experiment ====\n",
            "Config file: configs/qwen25_0.5b_test.json\n",
            "========================================\n",
            "Starting experiment at Tue Apr 22 11:15:49 PM UTC 2025\n",
            "Gradient accumulation steps: 2\n",
            "Buffer size: 256\n",
            "Buffer multiplier: 2\n",
            "Max sequence length: 8192\n",
            "Running experiments with the following configuration:\n",
            "Models: ['Qwen/Qwen2.5-0.5B']\n",
            "Latent dimensions: [8]\n",
            "Learning rates: [0.0001]\n",
            "Cache sizes: [1, 10, 100, 1000]\n",
            "Quantization bits: [2, 4, 8, 16]\n",
            "Epochs: [1]\n",
            "Training texts: [100]\n",
            "Batch sizes: [64]\n",
            "Number of runs: [5]\n",
            "Output directory: test_results_Qwen\n",
            "MODEL DIR:  test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with model=Qwen/Qwen2.5-0.5B, latent_dim=8, lr=0.0001, epochs=1, train_texts=100\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001/train_config.json\n",
            "2025-04-22 23:15:53.214702: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-22 23:15:53.232012: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363753.253571    5480 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363753.260256    5480 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:15:53.282320: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 16,\n",
            " 'buffer_mult': 2,\n",
            " 'buffer_size': 256,\n",
            " 'device': 'cuda',\n",
            " 'dtype': 'bf16',\n",
            " 'eval_interval': 100,\n",
            " 'gradient_accumulation_steps': 2,\n",
            " 'hidden_size': 896,\n",
            " 'latent_dim': 8,\n",
            " 'latent_dims': [8],\n",
            " 'learning_rates': [0.0001],\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 8192,\n",
            " 'models': ['Qwen/Qwen2.5-0.5B'],\n",
            " 'name': 'Qwen/Qwen2.5-0.5B',\n",
            " 'num_attention_heads': 14,\n",
            " 'num_epochs': 1,\n",
            " 'num_eval_texts': 10,\n",
            " 'num_hidden_layers': 24,\n",
            " 'num_train_texts': 100,\n",
            " 'output_dir': 'test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001',\n",
            " 'quantization_bits': [2, 4, 8, 16],\n",
            " 'seed': 42,\n",
            " 'skip_training': False,\n",
            " 'vocab_size': 151936}\n",
            "Using dtype: torch.bfloat16\n",
            "tokenizer_config.json: 100% 7.23k/7.23k [00:00<00:00, 34.1MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 12.7MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 2.58MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:01<00:00, 5.22MB/s]\n",
            "config.json: 100% 681/681 [00:00<00:00, 5.57MB/s]\n",
            "model.safetensors: 100% 988M/988M [00:02<00:00, 470MB/s]\n",
            "generation_config.json: 100% 138/138 [00:00<00:00, 978kB/s]\n",
            "README.md: 100% 10.5k/10.5k [00:00<00:00, 44.7MB/s]\n",
            "test-00000-of-00001.parquet: 100% 733k/733k [00:00<00:00, 70.7MB/s]\n",
            "train-00000-of-00002.parquet: 100% 157M/157M [00:00<00:00, 338MB/s]\n",
            "train-00001-of-00002.parquet: 100% 157M/157M [00:00<00:00, 319MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 657k/657k [00:00<00:00, 398MB/s]\n",
            "Generating test split: 100% 4358/4358 [00:00<00:00, 109995.83 examples/s]\n",
            "Generating train split: 100% 1801350/1801350 [00:02<00:00, 884097.58 examples/s]\n",
            "Generating validation split: 100% 3760/3760 [00:00<00:00, 609514.69 examples/s]\n",
            "README.md: 100% 16.0k/16.0k [00:00<00:00, 57.6MB/s]\n",
            "LongBench.py: 100% 3.98k/3.98k [00:00<00:00, 31.7MB/s]\n",
            "0000.parquet: 100% 1.31M/1.31M [00:00<00:00, 242MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 3423.47 examples/s]\n",
            "0000.parquet: 100% 6.62M/6.62M [00:00<00:00, 29.1MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 3064.05 examples/s]\n",
            "0000.parquet: 100% 3.59M/3.59M [00:00<00:00, 16.9MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 5703.01 examples/s]\n",
            "0000.parquet: 100% 8.12M/8.12M [00:00<00:00, 241MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 2557.98 examples/s]\n",
            "0000.parquet: 100% 5.17M/5.17M [00:00<00:00, 158MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 6050.73 examples/s]\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 8192, buffer_size: 256)\n",
            "Allocating buffers with shape (16, 24, 14, 256, 64), total elements: 88080384\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 8192, buffer_size: 256)\n",
            "Allocating buffers with shape (16, 24, 14, 256, 64), total elements: 88080384\n",
            "Epoch 1/1: 100% 6/6 [00:04<00:00,  1.37it/s]\n",
            "Epoch 1, Loss: 0.0604\n",
            "Saved test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001/autoencoders_epoch_1.pth\n",
            "Training complete!\n",
            "Training finished for Qwen_Qwen2.5-0.5B_latent8_lr0.0001_epochs1_texts100. Checking for model file...\n",
            "Running check: ls -l test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001/autoencoders_final.pth\n",
            "-rw-r--r-- 1 root root 87490 Apr 22 23:16 test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001/autoencoders_final.pth\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_quant2_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:17:02.607038: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-22 23:17:02.626580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363822.648514    5872 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363822.655076    5872 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:17:02.677114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:54<00:00,  5.49s/it]\n",
            "Eval compressed: 100% 10/10 [01:04<00:00,  6.44s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 24.90it/s]\n",
            "Eval compressed: 100% 10/10 [00:04<00:00,  2.02it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 27.56it/s]\n",
            "Eval compressed: 100% 10/10 [00:10<00:00,  1.04s/it]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 27.98it/s]\n",
            "Eval compressed: 100% 10/10 [00:06<00:00,  1.46it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 27.51it/s]\n",
            "Eval compressed: 100% 10/10 [00:08<00:00,  1.20it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 28.28it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.71it/s]\n",
            "Saved benchmark results to test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_quant2_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 14259.73\n",
            "Compressed perplexity: 13381.52\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=132.08, compressed=28519.35\n",
            "  hotpotqa: baseline=43.47, compressed=21089.48\n",
            "  2wikimqa: baseline=98.47, compressed=29969.45\n",
            "  musique: baseline=74.65, compressed=15244.36\n",
            "  dureader: baseline=373.24, compressed=43569.75\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_quant4_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:20:07.674444: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-22 23:20:07.694430: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745364007.716821    6708 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745364007.723501    6708 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:20:07.745901: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:55<00:00,  5.57s/it]\n",
            "Eval compressed: 100% 10/10 [01:04<00:00,  6.50s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 24.93it/s]\n",
            "Eval compressed: 100% 10/10 [00:04<00:00,  2.01it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 28.04it/s]\n",
            "Eval compressed: 100% 10/10 [00:10<00:00,  1.04s/it]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 28.28it/s]\n",
            "Eval compressed: 100% 10/10 [00:06<00:00,  1.44it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 27.26it/s]\n",
            "Eval compressed: 100% 10/10 [00:08<00:00,  1.20it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 28.38it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.61it/s]\n",
            "Saved benchmark results to test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_quant4_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 2136.77\n",
            "Compressed perplexity: 14529.92\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=132.08, compressed=25745.75\n",
            "  hotpotqa: baseline=43.47, compressed=22407.46\n",
            "  2wikimqa: baseline=98.47, compressed=18770.92\n",
            "  musique: baseline=74.65, compressed=12939.45\n",
            "  dureader: baseline=373.24, compressed=40944.20\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_quant8_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:23:13.676849: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-22 23:23:13.696790: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745364193.719080    7537 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745364193.725613    7537 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:23:13.747131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:55<00:00,  5.50s/it]\n",
            "Eval compressed: 100% 10/10 [01:03<00:00,  6.39s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 25.06it/s]\n",
            "Eval compressed: 100% 10/10 [00:04<00:00,  2.03it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 28.08it/s]\n",
            "Eval compressed: 100% 10/10 [00:10<00:00,  1.02s/it]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 28.43it/s]\n",
            "Eval compressed: 100% 10/10 [00:06<00:00,  1.49it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 28.10it/s]\n",
            "Eval compressed: 100% 10/10 [00:08<00:00,  1.20it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 28.89it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.80it/s]\n",
            "Saved benchmark results to test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_quant8_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 24.91\n",
            "Compressed perplexity: 14151.33\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=132.08, compressed=27496.95\n",
            "  hotpotqa: baseline=43.47, compressed=22411.09\n",
            "  2wikimqa: baseline=98.47, compressed=18081.62\n",
            "  musique: baseline=74.65, compressed=13921.09\n",
            "  dureader: baseline=373.24, compressed=46444.20\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_quant16_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:26:18.155696: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-22 23:26:18.175220: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745364378.197331    8363 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745364378.203897    8363 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:26:18.225692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline:  50% 5/10 [00:25<00:25,  5.10s/it]\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/learned-kv-compression/src/inference/benchmark.py\", line 244, in <module>\n",
            "    run_benchmark(\n",
            "  File \"/content/learned-kv-compression/src/inference/benchmark.py\", line 209, in run_benchmark\n",
            "    base_ppl = evaluate_with_kv_quantization(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/learned-kv-compression/src/inference/benchmark.py\", line 129, in evaluate_with_kv_quantization\n",
            "    out = model(\n",
            "          ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\", line 172, in wrapped_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 823, in forward\n",
            "    outputs: BaseModelOutputWithPast = self.model(\n",
            "                                       ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 965, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 549, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "                    ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 277, in forward\n",
            "    hidden_states = self.post_attention_layernorm(hidden_states)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 222, in forward\n",
            "    hidden_states = hidden_states.to(torch.float32)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/learned-kv-compression/run_experiments.py\", line 301, in <module>\n",
            "    main() \n",
            "    ^^^^^^\n",
            "  File \"/content/learned-kv-compression/run_experiments.py\", line 227, in main\n",
            "    result_dir = run_benchmark(\n",
            "                 ^^^^^^^^^^^^^^\n",
            "  File \"/content/learned-kv-compression/run_experiments.py\", line 113, in run_benchmark\n",
            "    subprocess.run(cmd)\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 550, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1201, in communicate\n",
            "    self.wait()\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1264, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2053, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2011, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!./run_experiments.sh configs/qwen25_0.5b_test.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
