{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6KkToOAF1M"
      },
      "source": [
        "# Notebook to run learned kv compression scripts on colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prGnb-vKAnzs"
      },
      "source": [
        "Download Repo and Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YbnT8hMBAE49",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Download Repo\n",
        "%cd /content\n",
        "!rm -rf learned-kv-compression\n",
        "!git clone -b colab https://henro25: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@github.com/henro25/learned-kv-compression\n",
        "%cd /content/learned-kv-compression/\n",
        "%ls\n",
        "\n",
        "# Install Requirements\n",
        "%pip install -r colab_requirements.txt\n",
        "%pip uninstall gcsfs -y\n",
        "%pip install --upgrade fsspec==2025.3.2\n",
        "%pip install gcsfs==2024.12.0\n",
        "%pip install --upgrade datasets\n",
        "\n",
        "!apt-get update -y\n",
        "!apt-get install -y jq\n",
        "\n",
        "# Enable permissions if needed\n",
        "!chmod +x run_experiments.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7vdgntcAana"
      },
      "source": [
        "## Training the Autoencoder\n",
        "\n",
        "This trains an autoencoder that compresses each KV vector to a 16-dimensional latent representation using 1000 texts from WikiText-103."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7idaakurATWT"
      },
      "outputs": [],
      "source": [
        "!python -m src.dictionary_learning.train \\\n",
        "    --name distilgpt2 \\\n",
        "    --latent_dim 16 \\\n",
        "    --num_epochs 10 \\\n",
        "    --batch_size 32 \\\n",
        "    --output_dir models/distilgpt2_16 \\\n",
        "    --num_train_texts 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3sF0VkRbUZX"
      },
      "source": [
        "# Benchmarking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6oJq5qtZTwj"
      },
      "source": [
        "Run a quick test for KV Cache compression with minimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "j_99ro74ZTZw"
      },
      "outputs": [],
      "source": [
        "!./quick_test.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WF2aid4bRLa"
      },
      "source": [
        "Run experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MNdpwUKhbQv8"
      },
      "outputs": [],
      "source": [
        "!./run_experiments.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ5GZpidFIp1"
      },
      "source": [
        "Eval with Perplexity and Longbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2koR1iA0a-Az"
      },
      "outputs": [],
      "source": [
        "!./src/evaluation/run_evaluation.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTzz1NyT_gl9",
        "outputId": "06587e1e-ffe8-47f3-e739-2ffb95879e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== KV Cache Compression Experiment ====\n",
            "Config file: configs/distilgpt2_experiment.json\n",
            "========================================\n",
            "Starting experiment at Tue Apr 22 11:05:05 PM UTC 2025\n",
            "Gradient accumulation steps: 2\n",
            "Buffer size: 256\n",
            "Buffer multiplier: 2\n",
            "Max sequence length: 1024\n",
            "Running experiments with the following configuration:\n",
            "Models: ['distilgpt2']\n",
            "Latent dimensions: [8, 16, 32]\n",
            "Learning rates: [0.0005]\n",
            "Cache sizes: [1, 10, 100, 1000]\n",
            "Quantization bits: [2, 4, 8, 16]\n",
            "Epochs: [2]\n",
            "Training texts: [100]\n",
            "Batch sizes: [64]\n",
            "Number of runs: [5]\n",
            "Output directory: experiment_results_distilgpt2\n",
            "MODEL DIR:  experiment_results_distilgpt2/distilgpt2/distilgpt2_latent8_lr0.0005\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with model=distilgpt2, latent_dim=8, lr=0.0005, epochs=2, train_texts=100\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config experiment_results_distilgpt2/distilgpt2/distilgpt2_latent8_lr0.0005/train_config.json\n",
            "2025-04-22 23:05:08.838098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363108.871969    7149 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363108.882093    7149 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:05:08.913366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 64,\n",
            " 'buffer_mult': 2,\n",
            " 'buffer_size': 256,\n",
            " 'cache_sizes': [1, 10, 100, 1000],\n",
            " 'device': 'cuda',\n",
            " 'dtype': 'bf16',\n",
            " 'eval_interval': 1000,\n",
            " 'gradient_accumulation_steps': 2,\n",
            " 'hidden_size': 768,\n",
            " 'latent_dim': 8,\n",
            " 'latent_dims': [8, 16, 32],\n",
            " 'learning_rates': [0.0005],\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0005,\n",
            " 'max_seq_len': 1024,\n",
            " 'models': ['distilgpt2'],\n",
            " 'name': 'distilgpt2',\n",
            " 'num_attention_heads': 12,\n",
            " 'num_epochs': 2,\n",
            " 'num_eval_texts': 10,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_train_texts': 100,\n",
            " 'output_dir': 'experiment_results_distilgpt2/distilgpt2/distilgpt2_latent8_lr0.0005',\n",
            " 'quantization_bits': [2, 4, 8, 16],\n",
            " 'seed': 42,\n",
            " 'skip_training': False,\n",
            " 'vocab_size': 50257}\n",
            "Using dtype: torch.bfloat16\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Epoch 1/2: 100% 1/1 [00:01<00:00,  1.37s/it]\n",
            "Epoch 1, Loss: 0.1406\n",
            "Saved experiment_results_distilgpt2/distilgpt2/distilgpt2_latent8_lr0.0005/autoencoders_epoch_1.pth\n",
            "Epoch 2/2: 100% 1/1 [00:01<00:00,  1.42s/it]\n",
            "Epoch 2, Loss: 0.1680\n",
            "Saved experiment_results_distilgpt2/distilgpt2/distilgpt2_latent8_lr0.0005/autoencoders_epoch_2.pth\n",
            "Training complete!\n",
            "Training finished for distilgpt2_latent8_lr0.0005_epochs2_texts100. Checking for model file...\n",
            "Running check: ls -l experiment_results_distilgpt2/distilgpt2/distilgpt2_latent8_lr0.0005/autoencoders_final.pth\n",
            "-rw-r--r-- 1 root root 22082 Apr 22 23:05 experiment_results_distilgpt2/distilgpt2/distilgpt2_latent8_lr0.0005/autoencoders_final.pth\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant2_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:05:45.272381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363145.306513    7341 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363145.317120    7341 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:05:45.346542: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.06s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.22s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 74.63it/s]\n",
            "Eval compressed: 100% 10/10 [00:00<00:00, 10.30it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 67.66it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.57it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 76.27it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.40it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 72.42it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.32it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 72.07it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.19it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant2_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 1321.81\n",
            "Compressed perplexity: 5165.08\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=5538.18\n",
            "  hotpotqa: baseline=202.80, compressed=3432.37\n",
            "  2wikimqa: baseline=378.33, compressed=4607.41\n",
            "  musique: baseline=229.11, compressed=2487.47\n",
            "  dureader: baseline=86.57, compressed=4046.93\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant4_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:06:36.462979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363196.482692    7586 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363196.488640    7586 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:06:36.510057: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.05s/it]\n",
            "Eval compressed: 100% 10/10 [00:11<00:00,  1.19s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 76.91it/s]\n",
            "Eval compressed: 100% 10/10 [00:00<00:00, 10.45it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 88.28it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  3.87it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 76.43it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.41it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 70.27it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.21it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 70.79it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.26it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant4_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 104.58\n",
            "Compressed perplexity: 5142.36\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=5639.87\n",
            "  hotpotqa: baseline=202.80, compressed=3499.46\n",
            "  2wikimqa: baseline=378.33, compressed=4930.56\n",
            "  musique: baseline=229.11, compressed=2678.22\n",
            "  dureader: baseline=86.57, compressed=4007.60\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant8_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:07:25.311473: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363245.330839    7821 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363245.336734    7821 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:07:25.356676: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.07s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.21s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 76.03it/s]\n",
            "Eval compressed: 100% 10/10 [00:00<00:00, 10.38it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 65.14it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.22it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 75.91it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.45it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 70.37it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.32it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 71.62it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.05it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant8_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 76.71\n",
            "Compressed perplexity: 4850.02\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=5648.74\n",
            "  hotpotqa: baseline=202.80, compressed=3401.87\n",
            "  2wikimqa: baseline=378.33, compressed=4962.31\n",
            "  musique: baseline=229.11, compressed=2671.02\n",
            "  dureader: baseline=86.57, compressed=4067.91\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant16_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:08:15.118516: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363295.138200    8060 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363295.144078    8060 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:08:15.164231: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.04s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.20s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 70.95it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.77it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 67.03it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.08it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 76.92it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.55it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 70.05it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.19it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 70.37it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.73it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant16_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 76.35\n",
            "Compressed perplexity: 4870.22\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=5632.55\n",
            "  hotpotqa: baseline=202.80, compressed=3430.05\n",
            "  2wikimqa: baseline=378.33, compressed=4965.97\n",
            "  musique: baseline=229.11, compressed=2673.76\n",
            "  dureader: baseline=86.57, compressed=4030.75\n",
            "MODEL DIR:  experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with model=distilgpt2, latent_dim=16, lr=0.0005, epochs=2, train_texts=100\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/train_config.json\n",
            "2025-04-22 23:09:01.368364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363341.387816    8297 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363341.393848    8297 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:09:01.413706: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 64,\n",
            " 'buffer_mult': 2,\n",
            " 'buffer_size': 256,\n",
            " 'cache_sizes': [1, 10, 100, 1000],\n",
            " 'device': 'cuda',\n",
            " 'dtype': 'bf16',\n",
            " 'eval_interval': 1000,\n",
            " 'gradient_accumulation_steps': 2,\n",
            " 'hidden_size': 768,\n",
            " 'latent_dim': 16,\n",
            " 'latent_dims': [8, 16, 32],\n",
            " 'learning_rates': [0.0005],\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0005,\n",
            " 'max_seq_len': 1024,\n",
            " 'models': ['distilgpt2'],\n",
            " 'name': 'distilgpt2',\n",
            " 'num_attention_heads': 12,\n",
            " 'num_epochs': 2,\n",
            " 'num_eval_texts': 10,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_train_texts': 100,\n",
            " 'output_dir': 'experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005',\n",
            " 'quantization_bits': [2, 4, 8, 16],\n",
            " 'seed': 42,\n",
            " 'skip_training': False,\n",
            " 'vocab_size': 50257}\n",
            "Using dtype: torch.bfloat16\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Epoch 1/2: 100% 1/1 [00:01<00:00,  1.51s/it]\n",
            "Epoch 1, Loss: 0.1172\n",
            "Saved experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_epoch_1.pth\n",
            "Epoch 2/2: 100% 1/1 [00:01<00:00,  1.55s/it]\n",
            "Epoch 2, Loss: 0.1445\n",
            "Saved experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_epoch_2.pth\n",
            "Training complete!\n",
            "Training finished for distilgpt2_latent16_lr0.0005_epochs2_texts100. Checking for model file...\n",
            "Running check: ls -l experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_final.pth\n",
            "-rw-r--r-- 1 root root 34370 Apr 22 23:09 experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_final.pth\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=16, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:09:35.782359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363375.801646    8462 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363375.808094    8462 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:09:35.827768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.05s/it]\n",
            "Eval compressed: 100% 10/10 [00:11<00:00,  1.20s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 76.47it/s]\n",
            "Eval compressed: 100% 10/10 [00:00<00:00, 10.18it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 69.85it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  3.88it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 76.19it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.63it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 69.85it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.42it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 71.94it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.31it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 1321.81\n",
            "Compressed perplexity: 2845.07\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=3130.72\n",
            "  hotpotqa: baseline=202.80, compressed=1570.19\n",
            "  2wikimqa: baseline=378.33, compressed=2298.43\n",
            "  musique: baseline=229.11, compressed=1256.41\n",
            "  dureader: baseline=86.57, compressed=6302.04\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=16, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:10:25.824385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363425.845645    8699 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363425.851632    8699 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:10:25.872196: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.07s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.21s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 75.76it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  9.29it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 80.10it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.37it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 76.21it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.30it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 67.80it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.29it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 72.10it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.10it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 104.58\n",
            "Compressed perplexity: 2643.93\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=2971.26\n",
            "  hotpotqa: baseline=202.80, compressed=1413.01\n",
            "  2wikimqa: baseline=378.33, compressed=2322.61\n",
            "  musique: baseline=229.11, compressed=1282.97\n",
            "  dureader: baseline=86.57, compressed=5126.50\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=16, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:11:15.723942: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363475.743139    8938 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363475.749152    8938 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:11:15.768542: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.00s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.21s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 59.65it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  9.99it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 84.81it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.05it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 80.61it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.54it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 70.52it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.37it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 72.68it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.52it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 76.71\n",
            "Compressed perplexity: 2649.30\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=3002.11\n",
            "  hotpotqa: baseline=202.80, compressed=1401.27\n",
            "  2wikimqa: baseline=378.33, compressed=2311.44\n",
            "  musique: baseline=229.11, compressed=1260.55\n",
            "  dureader: baseline=86.57, compressed=5411.92\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=16, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:12:04.653954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363524.673995    9172 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363524.680177    9172 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:12:04.700803: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.06s/it]\n",
            "Eval compressed: 100% 10/10 [00:11<00:00,  1.20s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 73.10it/s]\n",
            "Eval compressed: 100% 10/10 [00:00<00:00, 10.53it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 85.65it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.03it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 75.67it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.44it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 71.32it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.31it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 65.78it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.51it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 76.35\n",
            "Compressed perplexity: 2654.36\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=3003.91\n",
            "  hotpotqa: baseline=202.80, compressed=1392.83\n",
            "  2wikimqa: baseline=378.33, compressed=2312.27\n",
            "  musique: baseline=229.11, compressed=1275.96\n",
            "  dureader: baseline=86.57, compressed=5501.79\n",
            "MODEL DIR:  experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with model=distilgpt2, latent_dim=32, lr=0.0005, epochs=2, train_texts=100\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/train_config.json\n",
            "2025-04-22 23:12:50.050953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363570.070862    9409 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363570.076991    9409 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:12:50.096953: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 64,\n",
            " 'buffer_mult': 2,\n",
            " 'buffer_size': 256,\n",
            " 'cache_sizes': [1, 10, 100, 1000],\n",
            " 'device': 'cuda',\n",
            " 'dtype': 'bf16',\n",
            " 'eval_interval': 1000,\n",
            " 'gradient_accumulation_steps': 2,\n",
            " 'hidden_size': 768,\n",
            " 'latent_dim': 32,\n",
            " 'latent_dims': [8, 16, 32],\n",
            " 'learning_rates': [0.0005],\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0005,\n",
            " 'max_seq_len': 1024,\n",
            " 'models': ['distilgpt2'],\n",
            " 'name': 'distilgpt2',\n",
            " 'num_attention_heads': 12,\n",
            " 'num_epochs': 2,\n",
            " 'num_eval_texts': 10,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_train_texts': 100,\n",
            " 'output_dir': 'experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005',\n",
            " 'quantization_bits': [2, 4, 8, 16],\n",
            " 'seed': 42,\n",
            " 'skip_training': False,\n",
            " 'vocab_size': 50257}\n",
            "Using dtype: torch.bfloat16\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Epoch 1/2: 100% 1/1 [00:01<00:00,  1.49s/it]\n",
            "Epoch 1, Loss: 0.1064\n",
            "Saved experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_epoch_1.pth\n",
            "Epoch 2/2: 100% 1/1 [00:01<00:00,  1.42s/it]\n",
            "Epoch 2, Loss: 0.1328\n",
            "Saved experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_epoch_2.pth\n",
            "Training complete!\n",
            "Training finished for distilgpt2_latent32_lr0.0005_epochs2_texts100. Checking for model file...\n",
            "Running check: ls -l experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_final.pth\n",
            "-rw-r--r-- 1 root root 59330 Apr 22 23:13 experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_final.pth\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:13:24.012385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363604.031662    9570 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363604.037595    9570 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:13:24.057444: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.06s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.21s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 78.90it/s]\n",
            "Eval compressed: 100% 10/10 [00:00<00:00, 10.03it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 83.65it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.39it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 75.84it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.49it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 70.30it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.32it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 72.05it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.98it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 1321.81\n",
            "Compressed perplexity: 2211.33\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=3304.49\n",
            "  hotpotqa: baseline=202.80, compressed=1233.34\n",
            "  2wikimqa: baseline=378.33, compressed=2568.70\n",
            "  musique: baseline=229.11, compressed=1319.47\n",
            "  dureader: baseline=86.57, compressed=913.54\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:14:14.025221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363654.044731    9809 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363654.051392    9809 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:14:14.071375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.04s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.23s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 73.29it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.86it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 69.34it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.97it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 77.17it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.35it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 69.65it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.27it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 72.81it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.70it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 104.58\n",
            "Compressed perplexity: 2292.37\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=3251.55\n",
            "  hotpotqa: baseline=202.80, compressed=1265.29\n",
            "  2wikimqa: baseline=378.33, compressed=2570.72\n",
            "  musique: baseline=229.11, compressed=1252.39\n",
            "  dureader: baseline=86.57, compressed=998.02\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:15:03.414125: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363703.434108   10042 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363703.440420   10042 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:15:03.461079: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.05s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.21s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 76.07it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  9.92it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 76.24it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.07it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 75.40it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.38it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 68.05it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.31it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 69.32it/s]\n",
            "Eval compressed: 100% 10/10 [00:02<00:00,  4.85it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 76.71\n",
            "Compressed perplexity: 2229.15\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=3276.19\n",
            "  hotpotqa: baseline=202.80, compressed=1232.55\n",
            "  2wikimqa: baseline=378.33, compressed=2547.88\n",
            "  musique: baseline=229.11, compressed=1248.11\n",
            "  dureader: baseline=86.57, compressed=1002.21\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5/benchmark_config.json\n",
            "2025-04-22 23:15:52.782752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745363752.802004   10281 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745363752.807898   10281 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 23:15:52.827265: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/10 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 10/10 [00:10<00:00,  1.07s/it]\n",
            "Eval compressed: 100% 10/10 [00:12<00:00,  1.20s/it]\n",
            "Calculating perplexity (narrativeqa): 100% 10/10 [00:00<00:00, 78.45it/s]\n",
            "Eval compressed: 100% 10/10 [00:00<00:00, 10.21it/s]\n",
            "Calculating perplexity (hotpotqa): 100% 10/10 [00:00<00:00, 89.10it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.05it/s]\n",
            "Calculating perplexity (2wikimqa): 100% 10/10 [00:00<00:00, 82.07it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  7.39it/s]\n",
            "Calculating perplexity (musique): 100% 10/10 [00:00<00:00, 70.22it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  5.49it/s]\n",
            "Calculating perplexity (dureader): 100% 10/10 [00:00<00:00, 70.40it/s]\n",
            "Eval compressed: 100% 10/10 [00:01<00:00,  6.15it/s]\n",
            "Saved benchmark results to experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5/benchmark_results.json\n",
            "Baseline perplexity: 76.35\n",
            "Compressed perplexity: 2232.21\n",
            "LongBench results:\n",
            "  narrativeqa: baseline=406.35, compressed=3240.09\n",
            "  hotpotqa: baseline=202.80, compressed=1231.97\n",
            "  2wikimqa: baseline=378.33, compressed=2524.15\n",
            "  musique: baseline=229.11, compressed=1239.44\n",
            "  dureader: baseline=86.57, compressed=1003.04\n",
            "\n",
            "================================================================================\n",
            "Experiment Summary\n",
            "================================================================================\n",
            "Models tested: ['distilgpt2']\n",
            "Latent dimensions tested: [8, 16, 32]\n",
            "Learning rates tested: [0.0005]\n",
            "Quantization bits tested: [2, 4, 8, 16]\n",
            "Epochs tested: [2]\n",
            "Training texts tested: [100]\n",
            "Batch sizes tested: [64]\n",
            "Number of runs tested: [5]\n",
            "KV cache sizes tested: [1, 10, 100, 1000] MB\n",
            "Total runtime: 0h 11m 30.45s\n",
            "Results saved to: experiment_results_distilgpt2\n",
            "Total experiments: 12\n",
            "================================================================================\n",
            "Experiment Results:\n",
            "- Model: distilgpt2, Latent dim: 8, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 2, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant2_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 8, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 4, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant4_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 8, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 8, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant8_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 8, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 16, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent8_lr0.0005_quant16_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 16, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 2, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 16, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 4, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 16, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 8, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 16, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 16, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 32, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 2, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 32, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 4, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 32, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 8, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 32, LR: 0.0005, Epochs: 2, Texts: 100, Quantization bits: 16, Batch: 64, Runs: 5\n",
            "  Result dir: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5\n",
            "================================================================================\n",
            "Experiment completed at Tue Apr 22 11:16:36 PM UTC 2025\n",
            "Generating comparison report...\n",
            "./run_experiments.sh: line 30: jq: command not found\n",
            "Experiment summary not found, comparison report may be incomplete\n",
            "Found result directories: \n",
            "No result directories found for comparison analysis!\n",
            "Experiment and analysis complete!\n"
          ]
        }
      ],
      "source": [
        "!./run_experiments.sh configs/distilgpt2_experiment.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hi-kL2grLiK"
      },
      "source": [
        "## Qwen2.5-0.5B Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PMANLyemhaTB"
      },
      "outputs": [],
      "source": [
        "!./run_experiments.sh configs/qwen25_0.5b_test.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
