{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6KkToOAF1M"
      },
      "source": [
        "# Notebook to run learned kv compression scripts on colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prGnb-vKAnzs"
      },
      "source": [
        "Download Repo and Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YbnT8hMBAE49",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Download Repo\n",
        "%cd /content\n",
        "!rm -rf learned-kv-compression\n",
        "!git clone -b colab https://henro25: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@github.com/henro25/learned-kv-compression\n",
        "%cd /content/learned-kv-compression/\n",
        "%ls\n",
        "\n",
        "# Install Requirements\n",
        "%pip install -r colab_requirements.txt\n",
        "%pip uninstall gcsfs -y\n",
        "%pip install --upgrade fsspec==2025.3.2\n",
        "%pip install gcsfs==2024.12.0\n",
        "%pip install --upgrade datasets\n",
        "\n",
        "!apt-get update -y\n",
        "!apt-get install -y jq\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Enable permissions if needed\n",
        "!chmod +x run_experiments.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distilgpt2 Test"
      ],
      "metadata": {
        "id": "53smQpKCQjDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !./run_experiments.sh configs/distilgpt2_test.json"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U7pL1VbTNWy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r test_results_distilgpt2.zip test_results_distilgpt2\n",
        "# files.download('test_results_distilgpt2.zip')"
      ],
      "metadata": {
        "id": "Ns4EzLboQaUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distilgpt2 Experiment"
      ],
      "metadata": {
        "id": "vr3uFia_QmSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./run_experiments.sh configs/distilgpt2_experiment.json"
      ],
      "metadata": {
        "id": "3AAGcBw1QGXY",
        "outputId": "f01995b2-fe5b-4134-fa47-a7db05933b30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== KV Cache Compression Experiment ====\n",
            "Config file: configs/distilgpt2_experiment.json\n",
            "Working directory: /content/learned-kv-compression\n",
            "========================================\n",
            "Experiment results directory: /content/learned-kv-compression/experiment_results_distilgpt2\n",
            "Starting experiment at Tue Apr 29 11:48:22 PM UTC 2025\n",
            "Gradient accumulation steps: 2\n",
            "Buffer size: 256\n",
            "Buffer multiplier: 2\n",
            "Max sequence length: 1024\n",
            "Running experiments with the following configuration:\n",
            "Models: ['distilgpt2']\n",
            "Latent dimensions: [16, 32]\n",
            "Learning rates: [0.0005]\n",
            "Cache sizes: [1, 10, 100, 1000]\n",
            "Quantization bits: [2, 4, 8, 16]\n",
            "Epochs: [10]\n",
            "Training texts: [50000]\n",
            "Batch sizes: [64]\n",
            "Number of runs: [5]\n",
            "Output directory: /content/learned-kv-compression/experiment_results_distilgpt2\n",
            "MODEL DIR:  /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with model=distilgpt2, latent_dim=16, lr=0.0005, epochs=10, train_texts=50000\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/train_config.json\n",
            "2025-04-29 23:48:25.517332: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-29 23:48:25.535290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745970505.556825    1982 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745970505.563365    1982 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-29 23:48:25.585504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 64,\n",
            " 'buffer_mult': 2,\n",
            " 'buffer_size': 256,\n",
            " 'cache_sizes': [1, 10, 100, 1000],\n",
            " 'device': 'cuda',\n",
            " 'dtype': 'bf16',\n",
            " 'eval_interval': 10000,\n",
            " 'gradient_accumulation_steps': 2,\n",
            " 'hidden_size': 768,\n",
            " 'latent_dim': 16,\n",
            " 'latent_dims': [16, 32],\n",
            " 'learning_rates': [0.0005],\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0005,\n",
            " 'max_seq_len': 1024,\n",
            " 'models': ['distilgpt2'],\n",
            " 'name': 'distilgpt2',\n",
            " 'num_attention_heads': 12,\n",
            " 'num_epochs': 10,\n",
            " 'num_eval_texts': 50,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_train_texts': 50000,\n",
            " 'output_dir': '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005',\n",
            " 'quantization_bits': [2, 4, 8, 16],\n",
            " 'seed': 42,\n",
            " 'skip_training': False,\n",
            " 'vocab_size': 50257}\n",
            "Using dtype: torch.bfloat16\n",
            "tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 210kB/s]\n",
            "config.json: 100% 762/762 [00:00<00:00, 6.10MB/s]\n",
            "vocab.json: 100% 1.04M/1.04M [00:00<00:00, 3.14MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 47.0MB/s]\n",
            "tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 6.08MB/s]\n",
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "model.safetensors: 100% 353M/353M [00:01<00:00, 340MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 1.11MB/s]\n",
            "README.md: 100% 10.5k/10.5k [00:00<00:00, 51.9MB/s]\n",
            "test-00000-of-00001.parquet: 100% 733k/733k [00:00<00:00, 107MB/s]\n",
            "train-00000-of-00002.parquet: 100% 157M/157M [00:00<00:00, 315MB/s]\n",
            "train-00001-of-00002.parquet: 100% 157M/157M [00:00<00:00, 164MB/s]\n",
            "validation-00000-of-00001.parquet: 100% 657k/657k [00:00<00:00, 63.1MB/s]\n",
            "Generating test split: 100% 4358/4358 [00:00<00:00, 116608.78 examples/s]\n",
            "Generating train split: 100% 1801350/1801350 [00:02<00:00, 879566.00 examples/s]\n",
            "Generating validation split: 100% 3760/3760 [00:00<00:00, 627510.07 examples/s]\n",
            "README.md: 100% 16.0k/16.0k [00:00<00:00, 82.5MB/s]\n",
            "LongBench.py: 100% 3.98k/3.98k [00:00<00:00, 37.0MB/s]\n",
            "0000.parquet: 100% 1.31M/1.31M [00:00<00:00, 98.7MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 3105.35 examples/s]\n",
            "0000.parquet: 100% 6.62M/6.62M [00:00<00:00, 250MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 3160.27 examples/s]\n",
            "0000.parquet: 100% 3.59M/3.59M [00:00<00:00, 25.3MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 5691.05 examples/s]\n",
            "0000.parquet: 100% 8.12M/8.12M [00:00<00:00, 20.5MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 3020.16 examples/s]\n",
            "0000.parquet: 100% 5.17M/5.17M [00:00<00:00, 71.5MB/s]\n",
            "Generating test split: 100% 200/200 [00:00<00:00, 4479.20 examples/s]\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Epoch 1/10: 100% 562/562 [06:00<00:00,  1.56it/s]\n",
            "Epoch 1, Loss: 0.2441\n",
            "Epoch 2/10: 100% 562/562 [05:59<00:00,  1.56it/s]\n",
            "Epoch 2, Loss: 0.1954\n",
            "Epoch 3/10: 100% 562/562 [05:59<00:00,  1.56it/s]\n",
            "Epoch 3, Loss: 0.1795\n",
            "Epoch 4/10: 100% 562/562 [06:00<00:00,  1.56it/s]\n",
            "Epoch 4, Loss: 0.1750\n",
            "Epoch 5/10: 100% 562/562 [06:00<00:00,  1.56it/s]\n",
            "Epoch 5, Loss: 0.1738\n",
            "Saved /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_epoch_5.pth\n",
            "Epoch 6/10: 100% 562/562 [06:01<00:00,  1.56it/s]\n",
            "Epoch 6, Loss: 0.1733\n",
            "Epoch 7/10: 100% 562/562 [06:00<00:00,  1.56it/s]\n",
            "Epoch 7, Loss: 0.1731\n",
            "Epoch 8/10: 100% 562/562 [06:00<00:00,  1.56it/s]\n",
            "Epoch 8, Loss: 0.1734\n",
            "Epoch 9/10: 100% 562/562 [06:00<00:00,  1.56it/s]\n",
            "Epoch 9, Loss: 0.1735\n",
            "Epoch 10/10: 100% 562/562 [06:00<00:00,  1.56it/s]\n",
            "Epoch 10, Loss: 0.1731\n",
            "Saved /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_epoch_10.pth\n",
            "Saved final autoencoder checkpoint to: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_final.pth\n",
            "Training complete!\n",
            "Training finished for distilgpt2_latent16_lr0.0005_epochs10_texts50000.\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=16, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5/benchmark_config.json\n",
            "2025-04-30 00:49:25.825722: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 00:49:25.844273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745974165.865746   17255 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745974165.872293   17255 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 00:49:25.894469: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 50/50 [00:32<00:00,  1.52it/s]\n",
            "Eval compressed: 100% 50/50 [00:38<00:00,  1.29it/s]\n",
            "PPL narrativeqa: 100% 50/50 [00:00<00:00, 128.02it/s]\n",
            "Eval compressed: 100% 50/50 [00:04<00:00, 10.35it/s]\n",
            "PPL hotpotqa: 100% 50/50 [00:00<00:00, 144.28it/s]\n",
            "Eval compressed: 100% 50/50 [00:09<00:00,  5.46it/s]\n",
            "PPL 2wikimqa: 100% 50/50 [00:00<00:00, 143.73it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.68it/s]\n",
            "PPL musique: 100% 50/50 [00:00<00:00, 143.68it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.09it/s]\n",
            "PPL dureader: 100% 50/50 [00:00<00:00, 145.44it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.30it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 1324.95, Compressed PPL: nan\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=16, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5/benchmark_config.json\n",
            "2025-04-30 00:51:46.657932: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 00:51:46.675941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745974306.697209   17903 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745974306.703729   17903 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 00:51:46.725413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 50/50 [00:33<00:00,  1.49it/s]\n",
            "Eval compressed: 100% 50/50 [00:39<00:00,  1.26it/s]\n",
            "PPL narrativeqa: 100% 50/50 [00:00<00:00, 127.27it/s]\n",
            "Eval compressed: 100% 50/50 [00:04<00:00, 10.05it/s]\n",
            "PPL hotpotqa: 100% 50/50 [00:00<00:00, 139.13it/s]\n",
            "Eval compressed: 100% 50/50 [00:09<00:00,  5.34it/s]\n",
            "PPL 2wikimqa: 100% 50/50 [00:00<00:00, 143.46it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.50it/s]\n",
            "PPL musique: 100% 50/50 [00:00<00:00, 143.81it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  5.95it/s]\n",
            "PPL dureader: 100% 50/50 [00:00<00:00, 145.27it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.15it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 112.51, Compressed PPL: 4303161.00\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=16, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5/benchmark_config.json\n",
            "2025-04-30 00:54:10.153333: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 00:54:10.171492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745974450.192826   18565 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745974450.199411   18565 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 00:54:10.220874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 50/50 [00:33<00:00,  1.50it/s]\n",
            "Eval compressed: 100% 50/50 [00:39<00:00,  1.28it/s]\n",
            "PPL narrativeqa: 100% 50/50 [00:00<00:00, 128.43it/s]\n",
            "Eval compressed: 100% 50/50 [00:04<00:00, 10.30it/s]\n",
            "PPL hotpotqa: 100% 50/50 [00:00<00:00, 141.85it/s]\n",
            "Eval compressed: 100% 50/50 [00:09<00:00,  5.39it/s]\n",
            "PPL 2wikimqa: 100% 50/50 [00:00<00:00, 143.11it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.58it/s]\n",
            "PPL musique: 100% 50/50 [00:00<00:00, 143.85it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.01it/s]\n",
            "PPL dureader: 100% 50/50 [00:00<00:00, 145.14it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.21it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 82.81, Compressed PPL: 2417148.50\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=16, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5/benchmark_config.json\n",
            "2025-04-30 00:56:32.691852: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 00:56:32.709782: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745974592.731163   19223 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745974592.737773   19223 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 00:56:32.759682: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 50/50 [00:33<00:00,  1.50it/s]\n",
            "Eval compressed: 100% 50/50 [00:39<00:00,  1.26it/s]\n",
            "PPL narrativeqa: 100% 50/50 [00:00<00:00, 127.75it/s]\n",
            "Eval compressed: 100% 50/50 [00:04<00:00, 10.16it/s]\n",
            "PPL hotpotqa: 100% 50/50 [00:00<00:00, 139.65it/s]\n",
            "Eval compressed: 100% 50/50 [00:09<00:00,  5.35it/s]\n",
            "PPL 2wikimqa: 100% 50/50 [00:00<00:00, 143.22it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.55it/s]\n",
            "PPL musique: 100% 50/50 [00:00<00:00, 144.07it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  5.95it/s]\n",
            "PPL dureader: 100% 50/50 [00:00<00:00, 145.20it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.13it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 82.51, Compressed PPL: 2147319.75\n",
            "MODEL DIR:  /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with model=distilgpt2, latent_dim=32, lr=0.0005, epochs=10, train_texts=50000\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/train_config.json\n",
            "2025-04-30 00:58:52.435984: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 00:58:52.453792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745974732.475032   19887 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745974732.481634   19887 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 00:58:52.503647: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 64,\n",
            " 'buffer_mult': 2,\n",
            " 'buffer_size': 256,\n",
            " 'cache_sizes': [1, 10, 100, 1000],\n",
            " 'device': 'cuda',\n",
            " 'dtype': 'bf16',\n",
            " 'eval_interval': 10000,\n",
            " 'gradient_accumulation_steps': 2,\n",
            " 'hidden_size': 768,\n",
            " 'latent_dim': 32,\n",
            " 'latent_dims': [16, 32],\n",
            " 'learning_rates': [0.0005],\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0005,\n",
            " 'max_seq_len': 1024,\n",
            " 'models': ['distilgpt2'],\n",
            " 'name': 'distilgpt2',\n",
            " 'num_attention_heads': 12,\n",
            " 'num_epochs': 10,\n",
            " 'num_eval_texts': 50,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_train_texts': 50000,\n",
            " 'output_dir': '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005',\n",
            " 'quantization_bits': [2, 4, 8, 16],\n",
            " 'seed': 42,\n",
            " 'skip_training': False,\n",
            " 'vocab_size': 50257}\n",
            "Using dtype: torch.bfloat16\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 1024, buffer_size: 256)\n",
            "Allocating buffers with shape (64, 6, 12, 256, 64), total elements: 75497472\n",
            "Epoch 1/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 1, Loss: 0.2248\n",
            "Epoch 2/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 2, Loss: 0.1714\n",
            "Epoch 3/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 3, Loss: 0.1519\n",
            "Epoch 4/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 4, Loss: 0.1432\n",
            "Epoch 5/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 5, Loss: 0.1404\n",
            "Saved /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_epoch_5.pth\n",
            "Epoch 6/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 6, Loss: 0.1396\n",
            "Epoch 7/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 7, Loss: 0.1394\n",
            "Epoch 8/10: 100% 562/562 [06:03<00:00,  1.54it/s]\n",
            "Epoch 8, Loss: 0.1396\n",
            "Epoch 9/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 9, Loss: 0.1397\n",
            "Epoch 10/10: 100% 562/562 [06:04<00:00,  1.54it/s]\n",
            "Epoch 10, Loss: 0.1394\n",
            "Saved /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_epoch_10.pth\n",
            "Saved final autoencoder checkpoint to: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_final.pth\n",
            "Training complete!\n",
            "Training finished for distilgpt2_latent32_lr0.0005_epochs10_texts50000.\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5/benchmark_config.json\n",
            "2025-04-30 02:00:10.815046: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 02:00:10.833253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745978410.854564   34858 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745978410.861114   34858 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 02:00:10.882903: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 50/50 [00:33<00:00,  1.51it/s]\n",
            "Eval compressed: 100% 50/50 [00:39<00:00,  1.27it/s]\n",
            "PPL narrativeqa: 100% 50/50 [00:00<00:00, 126.13it/s]\n",
            "Eval compressed: 100% 50/50 [00:04<00:00, 10.17it/s]\n",
            "PPL hotpotqa: 100% 50/50 [00:00<00:00, 139.90it/s]\n",
            "Eval compressed: 100% 50/50 [00:09<00:00,  5.38it/s]\n",
            "PPL 2wikimqa: 100% 50/50 [00:00<00:00, 141.69it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.55it/s]\n",
            "PPL musique: 100% 50/50 [00:00<00:00, 141.96it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  5.97it/s]\n",
            "PPL dureader: 100% 50/50 [00:00<00:00, 143.19it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.17it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 1324.95, Compressed PPL: 1611985.75\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5/benchmark_config.json\n",
            "2025-04-30 02:02:34.498252: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 02:02:34.516670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745978554.538296   35524 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745978554.544936   35524 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 02:02:34.567114: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 50/50 [00:33<00:00,  1.50it/s]\n",
            "Eval compressed: 100% 50/50 [00:39<00:00,  1.28it/s]\n",
            "PPL narrativeqa: 100% 50/50 [00:00<00:00, 126.76it/s]\n",
            "Eval compressed: 100% 50/50 [00:04<00:00, 10.25it/s]\n",
            "PPL hotpotqa: 100% 50/50 [00:00<00:00, 140.94it/s]\n",
            "Eval compressed: 100% 50/50 [00:09<00:00,  5.40it/s]\n",
            "PPL 2wikimqa: 100% 50/50 [00:00<00:00, 143.61it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.56it/s]\n",
            "PPL musique: 100% 50/50 [00:00<00:00, 142.30it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.00it/s]\n",
            "PPL dureader: 100% 50/50 [00:00<00:00, 144.26it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.16it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 112.51, Compressed PPL: 42774.00\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5/benchmark_config.json\n",
            "2025-04-30 02:04:56.293040: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 02:04:56.311021: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745978696.332288   36176 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745978696.338763   36176 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 02:04:56.360377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 50/50 [00:32<00:00,  1.52it/s]\n",
            "Eval compressed: 100% 50/50 [00:38<00:00,  1.29it/s]\n",
            "PPL narrativeqa: 100% 50/50 [00:00<00:00, 129.91it/s]\n",
            "Eval compressed: 100% 50/50 [00:04<00:00, 10.40it/s]\n",
            "PPL hotpotqa: 100% 50/50 [00:00<00:00, 144.22it/s]\n",
            "Eval compressed: 100% 50/50 [00:09<00:00,  5.48it/s]\n",
            "PPL 2wikimqa: 100% 50/50 [00:00<00:00, 146.05it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.62it/s]\n",
            "PPL musique: 100% 50/50 [00:00<00:00, 144.53it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.07it/s]\n",
            "PPL dureader: 100% 50/50 [00:00<00:00, 146.78it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.16it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 82.81, Compressed PPL: 422284.28\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=distilgpt2, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5/benchmark_config.json\n",
            "2025-04-30 02:07:20.186760: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-30 02:07:20.204944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745978840.226209   36838 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745978840.232776   36838 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-30 02:07:20.255157: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Eval quantized KV baseline:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 50/50 [00:33<00:00,  1.50it/s]\n",
            "Eval compressed: 100% 50/50 [00:39<00:00,  1.27it/s]\n",
            "PPL narrativeqa: 100% 50/50 [00:00<00:00, 126.05it/s]\n",
            "Eval compressed: 100% 50/50 [00:04<00:00, 10.21it/s]\n",
            "PPL hotpotqa: 100% 50/50 [00:00<00:00, 140.47it/s]\n",
            "Eval compressed: 100% 50/50 [00:09<00:00,  5.38it/s]\n",
            "PPL 2wikimqa: 100% 50/50 [00:00<00:00, 144.75it/s]\n",
            "Eval compressed: 100% 50/50 [00:07<00:00,  6.58it/s]\n",
            "PPL musique: 100% 50/50 [00:00<00:00, 141.05it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  5.96it/s]\n",
            "PPL dureader: 100% 50/50 [00:00<00:00, 144.16it/s]\n",
            "Eval compressed: 100% 50/50 [00:08<00:00,  6.14it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 82.51, Compressed PPL: 463065.94\n",
            "\n",
            "================================================================================\n",
            "Experiment Summary\n",
            "================================================================================\n",
            "Models tested: ['distilgpt2']\n",
            "Latent dimensions tested: [16, 32]\n",
            "Learning rates tested: [0.0005]\n",
            "Quantization bits tested: [2, 4, 8, 16]\n",
            "Epochs tested: [10]\n",
            "Training texts tested: [50000]\n",
            "Batch sizes tested: [64]\n",
            "Number of runs tested: [5]\n",
            "KV cache sizes tested: [1, 10, 100, 1000] MB\n",
            "Total runtime: 2h 21m 18.34s\n",
            "Results saved to: /content/learned-kv-compression/experiment_results_distilgpt2\n",
            "Total experiments: 8\n",
            "================================================================================\n",
            "Experiment Results:\n",
            "- Model: distilgpt2, Latent dim: 16, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 10, Texts: 50000, Quantization bits: 2, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 16, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 10, Texts: 50000, Quantization bits: 4, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 16, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 10, Texts: 50000, Quantization bits: 8, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 16, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 10, Texts: 50000, Quantization bits: 16, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 32, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 10, Texts: 50000, Quantization bits: 2, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 32, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 10, Texts: 50000, Quantization bits: 4, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 32, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 10, Texts: 50000, Quantization bits: 8, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5\n",
            "- Model: distilgpt2, Latent dim: 32, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 10, Texts: 50000, Quantization bits: 16, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5\n",
            "================================================================================\n",
            "Experiment completed at Wed Apr 30 02:09:40 AM UTC 2025\n",
            "Generating comparison report...\n",
            "Looking for summary file at: /content/learned-kv-compression/experiment_results_distilgpt2/experiment_summary.json\n",
            "Using experiment summary to find result directories\n",
            "Found result directories: /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5 /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5 /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5 /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5 /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5 /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5 /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5 /content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5\n",
            "Loading results from: ['/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5', '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5', '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5', '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5', '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5', '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5', '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5', '/content/learned-kv-compression/experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5']\n",
            "Generating comparison plots...\n",
            "Generating summary report...\n",
            "Report generated at /content/learned-kv-compression/experiment_results_distilgpt2/comparison/comparison_report.md\n",
            "Comparison complete. Results saved to /content/learned-kv-compression/experiment_results_distilgpt2/comparison\n",
            "Comparison analysis complete!\n",
            "Experiment and analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r experiment_results_distilgpt2.zip experiment_results_distilgpt2\n",
        "files.download('experiment_results_distilgpt2.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ccky5bMNGUna",
        "outputId": "4978f9d6-3bb1-4be3-f80e-c3f4f0632cb2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: experiment_results_distilgpt2/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/experiment_summary.json (deflated 89%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_final.pth (deflated 34%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_epoch_10.pth (deflated 34%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/attention_viz/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/train_config.json (deflated 53%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005/autoencoders_epoch_5.pth (deflated 33%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant4_batch64_runs5/benchmark_results.json (deflated 69%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_final.pth (deflated 38%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_epoch_10.pth (deflated 39%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/attention_viz/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/train_config.json (deflated 53%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005/autoencoders_epoch_5.pth (deflated 38%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant2_batch64_runs5/benchmark_results.json (deflated 69%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant8_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant8_batch64_runs5/benchmark_results.json (deflated 69%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent16_lr0.0005_epochs10_texts50000/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent16_lr0.0005_quant16_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant4_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant16_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/benchmark_distilgpt2_latent32_lr0.0005_quant2_batch64_runs5/benchmark_results.json (deflated 69%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2/distilgpt2_latent32_lr0.0005_epochs10_texts50000/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/experiment_summary.txt (deflated 42%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch65_runs1/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch65_runs1/benchmark_config.json (deflated 49%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/compression_ratio.png (deflated 42%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/evaluation_results.json (deflated 55%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/evaluation_results.csv (deflated 40%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/plots/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/plots/perplexity_ratio_heatmap.png (deflated 18%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/plots/performance_degradation.png (deflated 20%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/plots/perplexity_comparison.png (deflated 20%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/time_comparison.png (deflated 22%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/.DS_Store (deflated 96%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/speedup.png (deflated 23%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_config.json (deflated 50%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_results.json (deflated 95%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent16/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent16/train_config.json (deflated 47%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent16/autoencoder_config.json (deflated 52%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2400_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_5_batch_900_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1200_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3300_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_100_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2200_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1100_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3200_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1900_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_0_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2100_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1800_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_300_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2800_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3000_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3700_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2300_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2600_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1400_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1700_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1300_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2500_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1600_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1500_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2900_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3800_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2000_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3100_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3400_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3600_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_200_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_1000_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3900_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_2700_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/attention_viz/epoch_1_batch_3500_layer_0_head_0.png (deflated 1%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/train_config.json (deflated 47%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_config.json (deflated 52%)\n",
            "  adding: experiment_results_distilgpt2/.DS_Store (deflated 96%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/benchmark_distilgpt2_latent1/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/benchmark_distilgpt2_latent1/evaluation_results.json (deflated 54%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/benchmark_distilgpt2_latent1/evaluation_results.csv (deflated 39%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/benchmark_distilgpt2_latent1/plots/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/benchmark_distilgpt2_latent1/plots/perplexity_ratio_heatmap.png (deflated 17%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/benchmark_distilgpt2_latent1/plots/performance_degradation.png (deflated 21%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/benchmark_distilgpt2_latent1/plots/perplexity_comparison.png (deflated 21%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs1/benchmark_config.json (deflated 49%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/compression_ratio.png (deflated 40%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/time_comparison.png (deflated 23%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/evaluation_results.json (deflated 55%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/evaluation_results.csv (deflated 41%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/plots/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/plots/perplexity_ratio_heatmap.png (deflated 17%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/plots/performance_degradation.png (deflated 20%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/plots/perplexity_comparison.png (deflated 20%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/speedup.png (deflated 23%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_config.json (deflated 50%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_results.json (deflated 95%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs3/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs3/benchmark_config.json (deflated 49%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent32/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent32/train_config.json (deflated 47%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent32/autoencoder_config.json (deflated 52%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent1/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent1/attention_viz/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent1/attention_viz/epoch_1_batch_0_layer_0_head_0.png (deflated 6%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent1/train_config.json (deflated 48%)\n",
            "  adding: experiment_results_distilgpt2/distilgpt2_latent1/autoencoder_config.json (deflated 53%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/benchmark_distilgpt2_latent1/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/benchmark_distilgpt2_latent1/evaluation_results.json (deflated 54%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/benchmark_distilgpt2_latent1/evaluation_results.csv (deflated 39%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/benchmark_distilgpt2_latent1/plots/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/benchmark_distilgpt2_latent1/plots/perplexity_ratio_heatmap.png (deflated 17%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/benchmark_distilgpt2_latent1/plots/performance_degradation.png (deflated 21%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/benchmark_distilgpt2_latent1/plots/perplexity_comparison.png (deflated 21%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent1_batch64_runs2/benchmark_config.json (deflated 49%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/compression_ratio.png (deflated 40%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/time_comparison.png (deflated 22%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/speedup.png (deflated 27%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_config.json (deflated 50%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/evaluation_results.json (deflated 55%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/evaluation_results.csv (deflated 41%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/plots/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/plots/perplexity_ratio_heatmap.png (deflated 17%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/plots/performance_degradation.png (deflated 21%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/plots/perplexity_comparison.png (deflated 20%)\n",
            "  adding: experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_results.json (deflated 95%)\n",
            "  adding: experiment_results_distilgpt2/comparison/ (stored 0%)\n",
            "  adding: experiment_results_distilgpt2/comparison/tradeoff.png (deflated 23%)\n",
            "  adding: experiment_results_distilgpt2/comparison/compression_ratio.png (deflated 22%)\n",
            "  adding: experiment_results_distilgpt2/comparison/time_comparison.png (deflated 26%)\n",
            "  adding: experiment_results_distilgpt2/comparison/bar_compression_ratio.png (deflated 31%)\n",
            "  adding: experiment_results_distilgpt2/comparison/speedup_multiple_latent_dim.png (deflated 22%)\n",
            "  adding: experiment_results_distilgpt2/comparison/speedup_heatmap.png (deflated 29%)\n",
            "  adding: experiment_results_distilgpt2/comparison/speedup.png (deflated 24%)\n",
            "  adding: experiment_results_distilgpt2/comparison/time_comparison_multiple_latent_dim.png (deflated 21%)\n",
            "  adding: experiment_results_distilgpt2/comparison/comparison_report.md (deflated 61%)\n",
            "  adding: experiment_results_distilgpt2/comparison/bar_comparison_largest.png (deflated 26%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f4316696-e5ee-4ba4-806a-fb8a0fe64e8f\", \"experiment_results_distilgpt2.zip\", 21386260)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qwen2.5 0.5b Test"
      ],
      "metadata": {
        "id": "lQvonSLlQpEp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMANLyemhaTB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# !./run_experiments.sh configs/qwen25_0.5b_test.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !zip -r test_results_Qwen.zip test_results_Qwen\n",
        "# files.download('test_results_Qwen.zip')"
      ],
      "metadata": {
        "id": "3NOPBHVlQWqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qwen2.5 0.5b Experiment"
      ],
      "metadata": {
        "id": "cbiAd6nqQs-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./run_experiments.sh configs/qwen25_0.5b_experiment.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie6SsRANQKFU",
        "outputId": "0031ca2e-421e-4fc6-b1fd-eda04cf13668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== KV Cache Compression Experiment ====\n",
            "Config file: configs/qwen25_0.5b_experiment.json\n",
            "Working directory: /content/learned-kv-compression\n",
            "========================================\n",
            "Experiment results directory: /content/learned-kv-compression/experiment_results_Qwen\n",
            "Starting experiment at Sun Apr 27 05:35:54 PM UTC 2025\n",
            "Gradient accumulation steps: 2\n",
            "Buffer size: 256\n",
            "Buffer multiplier: 2\n",
            "Max sequence length: 8192\n",
            "Running experiments with the following configuration:\n",
            "Models: ['Qwen/Qwen2.5-0.5B']\n",
            "Latent dimensions: [32]\n",
            "Learning rates: [0.0005]\n",
            "Cache sizes: [1, 10, 100, 1000]\n",
            "Quantization bits: [2, 4, 8, 16]\n",
            "Epochs: [5]\n",
            "Training texts: [10000]\n",
            "Batch sizes: [64]\n",
            "Number of runs: [5]\n",
            "Output directory: /content/learned-kv-compression/experiment_results_Qwen\n",
            "MODEL DIR:  /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with model=Qwen/Qwen2.5-0.5B, latent_dim=32, lr=0.0005, epochs=5, train_texts=10000\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005/train_config.json\n",
            "2025-04-27 17:35:56.885981: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-27 17:35:56.904094: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745775356.925314   34651 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745775356.931904   34651 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-27 17:35:56.953392: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 16,\n",
            " 'buffer_mult': 2,\n",
            " 'buffer_size': 256,\n",
            " 'device': 'cuda',\n",
            " 'dtype': 'bf16',\n",
            " 'eval_interval': 10000,\n",
            " 'gradient_accumulation_steps': 2,\n",
            " 'hidden_size': 896,\n",
            " 'latent_dim': 32,\n",
            " 'latent_dims': [32],\n",
            " 'learning_rates': [0.0005],\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0005,\n",
            " 'max_seq_len': 8192,\n",
            " 'models': ['Qwen/Qwen2.5-0.5B'],\n",
            " 'name': 'Qwen/Qwen2.5-0.5B',\n",
            " 'num_attention_heads': 14,\n",
            " 'num_epochs': 5,\n",
            " 'num_eval_texts': 25,\n",
            " 'num_hidden_layers': 24,\n",
            " 'num_train_texts': 10000,\n",
            " 'output_dir': '/content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005',\n",
            " 'quantization_bits': [2, 4, 8, 16],\n",
            " 'seed': 42,\n",
            " 'skip_training': False,\n",
            " 'vocab_size': 151936}\n",
            "Using dtype: torch.bfloat16\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 8192, buffer_size: 256)\n",
            "Allocating buffers with shape (16, 24, 14, 256, 64), total elements: 88080384\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 8192, buffer_size: 256)\n",
            "Allocating buffers with shape (16, 24, 14, 256, 64), total elements: 88080384\n",
            "Epoch 1/5: 100% 500/500 [06:59<00:00,  1.19it/s]\n",
            "Epoch 1, Loss: 0.0258\n",
            "Epoch 2/5: 100% 500/500 [06:55<00:00,  1.20it/s]\n",
            "Epoch 2, Loss: 0.0129\n",
            "Epoch 3/5: 100% 500/500 [06:54<00:00,  1.21it/s]\n",
            "Epoch 3, Loss: 0.0123\n",
            "Epoch 4/5: 100% 500/500 [06:52<00:00,  1.21it/s]\n",
            "Epoch 4, Loss: 0.0122\n",
            "Epoch 5/5: 100% 500/500 [06:51<00:00,  1.21it/s]\n",
            "Epoch 5, Loss: 0.0121\n",
            "Saved /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005/autoencoders_epoch_5.pth\n",
            "Saved final autoencoder checkpoint to: /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005/autoencoders_final.pth\n",
            "Training complete!\n",
            "Training finished for Qwen_Qwen2.5-0.5B_latent32_lr0.0005_epochs5_texts10000.\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant2_batch64_runs5/benchmark_config.json\n",
            "2025-04-27 18:11:07.706295: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-27 18:11:07.725933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745777467.747572   43276 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745777467.754333   43276 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-27 18:11:07.776363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Eval quantized KV baseline:   0% 0/25 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 25/25 [01:32<00:00,  3.70s/it]\n",
            "Eval compressed: 100% 25/25 [01:47<00:00,  4.30s/it]\n",
            "PPL narrativeqa: 100% 25/25 [00:00<00:00, 26.80it/s]\n",
            "Eval compressed: 100% 25/25 [00:14<00:00,  1.73it/s]\n",
            "PPL hotpotqa: 100% 25/25 [00:00<00:00, 27.31it/s]\n",
            "Eval compressed: 100% 25/25 [00:23<00:00,  1.06it/s]\n",
            "PPL 2wikimqa: 100% 25/25 [00:00<00:00, 27.88it/s]\n",
            "Eval compressed: 100% 25/25 [00:18<00:00,  1.38it/s]\n",
            "PPL musique: 100% 25/25 [00:00<00:00, 27.80it/s]\n",
            "Eval compressed: 100% 25/25 [00:20<00:00,  1.22it/s]\n",
            "PPL dureader: 100% 25/25 [00:00<00:00, 27.92it/s]\n",
            "Eval compressed: 100% 25/25 [00:05<00:00,  4.70it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant2_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 10913.99, Compressed PPL: nan\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant4_batch64_runs5/benchmark_config.json\n",
            "2025-04-27 18:16:27.467667: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-27 18:16:27.487190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745777787.509189   44647 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745777787.515971   44647 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-27 18:16:27.537305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Eval quantized KV baseline:   0% 0/25 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 25/25 [01:32<00:00,  3.70s/it]\n",
            "Eval compressed: 100% 25/25 [01:47<00:00,  4.32s/it]\n",
            "PPL narrativeqa: 100% 25/25 [00:00<00:00, 27.74it/s]\n",
            "Eval compressed: 100% 25/25 [00:14<00:00,  1.73it/s]\n",
            "PPL hotpotqa: 100% 25/25 [00:00<00:00, 26.25it/s]\n",
            "Eval compressed: 100% 25/25 [00:23<00:00,  1.06it/s]\n",
            "PPL 2wikimqa: 100% 25/25 [00:00<00:00, 27.65it/s]\n",
            "Eval compressed: 100% 25/25 [00:18<00:00,  1.38it/s]\n",
            "PPL musique: 100% 25/25 [00:00<00:00, 27.68it/s]\n",
            "Eval compressed: 100% 25/25 [00:20<00:00,  1.21it/s]\n",
            "PPL dureader: 100% 25/25 [00:00<00:00, 27.15it/s]\n",
            "Eval compressed: 100% 25/25 [00:05<00:00,  4.61it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant4_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 2125.33, Compressed PPL: 112921.81\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant8_batch64_runs5/benchmark_config.json\n",
            "2025-04-27 18:21:48.227348: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-27 18:21:48.247000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745778108.269097   46020 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745778108.275765   46020 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-27 18:21:48.297834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Eval quantized KV baseline:   0% 0/25 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 25/25 [01:31<00:00,  3.66s/it]\n",
            "Eval compressed: 100% 25/25 [01:47<00:00,  4.29s/it]\n",
            "PPL narrativeqa: 100% 25/25 [00:00<00:00, 26.27it/s]\n",
            "Eval compressed: 100% 25/25 [00:14<00:00,  1.73it/s]\n",
            "PPL hotpotqa: 100% 25/25 [00:00<00:00, 27.63it/s]\n",
            "Eval compressed: 100% 25/25 [00:23<00:00,  1.05it/s]\n",
            "PPL 2wikimqa: 100% 25/25 [00:00<00:00, 27.78it/s]\n",
            "Eval compressed: 100% 25/25 [00:17<00:00,  1.39it/s]\n",
            "PPL musique: 100% 25/25 [00:00<00:00, 26.95it/s]\n",
            "Eval compressed: 100% 25/25 [00:20<00:00,  1.22it/s]\n",
            "PPL dureader: 100% 25/25 [00:00<00:00, 27.99it/s]\n",
            "Eval compressed: 100% 25/25 [00:05<00:00,  4.75it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant8_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 26.28, Compressed PPL: 64010.35\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=32, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant16_batch64_runs5/benchmark_config.json\n",
            "2025-04-27 18:27:08.640382: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-27 18:27:08.660658: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745778428.682038   47397 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745778428.688494   47397 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-27 18:27:08.710209: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Eval quantized KV baseline:   0% 0/25 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Eval quantized KV baseline: 100% 25/25 [01:30<00:00,  3.61s/it]\n",
            "Eval compressed: 100% 25/25 [01:45<00:00,  4.22s/it]\n",
            "PPL narrativeqa: 100% 25/25 [00:00<00:00, 28.19it/s]\n",
            "Eval compressed: 100% 25/25 [00:14<00:00,  1.77it/s]\n",
            "PPL hotpotqa: 100% 25/25 [00:00<00:00, 27.11it/s]\n",
            "Eval compressed: 100% 25/25 [00:23<00:00,  1.06it/s]\n",
            "PPL 2wikimqa: 100% 25/25 [00:00<00:00, 28.05it/s]\n",
            "Eval compressed: 100% 25/25 [00:17<00:00,  1.41it/s]\n",
            "PPL musique: 100% 25/25 [00:00<00:00, 28.85it/s]\n",
            "Eval compressed: 100% 25/25 [00:20<00:00,  1.23it/s]\n",
            "PPL dureader: 100% 25/25 [00:00<00:00, 28.96it/s]\n",
            "Eval compressed: 100% 25/25 [00:05<00:00,  4.78it/s]\n",
            "Saved benchmark results to /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant16_batch64_runs5/benchmark_results.json\n",
            "Baseline PPL: 24.67, Compressed PPL: 68616.98\n",
            "\n",
            "================================================================================\n",
            "Experiment Summary\n",
            "================================================================================\n",
            "Models tested: ['Qwen/Qwen2.5-0.5B']\n",
            "Latent dimensions tested: [32]\n",
            "Learning rates tested: [0.0005]\n",
            "Quantization bits tested: [2, 4, 8, 16]\n",
            "Epochs tested: [5]\n",
            "Training texts tested: [10000]\n",
            "Batch sizes tested: [64]\n",
            "Number of runs tested: [5]\n",
            "KV cache sizes tested: [1, 10, 100, 1000] MB\n",
            "Total runtime: 0h 56m 23.20s\n",
            "Results saved to: /content/learned-kv-compression/experiment_results_Qwen\n",
            "Total experiments: 4\n",
            "================================================================================\n",
            "Experiment Results:\n",
            "- Model: Qwen/Qwen2.5-0.5B, Latent dim: 32, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 5, Texts: 10000, Quantization bits: 2, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant2_batch64_runs5\n",
            "- Model: Qwen/Qwen2.5-0.5B, Latent dim: 32, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 5, Texts: 10000, Quantization bits: 4, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant4_batch64_runs5\n",
            "- Model: Qwen/Qwen2.5-0.5B, Latent dim: 32, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 5, Texts: 10000, Quantization bits: 8, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant8_batch64_runs5\n",
            "- Model: Qwen/Qwen2.5-0.5B, Latent dim: 32, Cache sizes: [1, 10, 100, 1000] MB, LR: 0.0005, Epochs: 5, Texts: 10000, Quantization bits: 16, Batch: 64, Runs: 5\n",
            "  Result dir: /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant16_batch64_runs5\n",
            "================================================================================\n",
            "Experiment completed at Sun Apr 27 06:32:18 PM UTC 2025\n",
            "Generating comparison report...\n",
            "Looking for summary file at: /content/learned-kv-compression/experiment_results_Qwen/experiment_summary.json\n",
            "Using experiment summary to find result directories\n",
            "Found result directories: /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant2_batch64_runs5 /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant4_batch64_runs5 /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant8_batch64_runs5 /content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant16_batch64_runs5\n",
            "Loading results from: ['/content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant2_batch64_runs5', '/content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant4_batch64_runs5', '/content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant8_batch64_runs5', '/content/learned-kv-compression/experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant16_batch64_runs5']\n",
            "Generating comparison plots...\n",
            "Generating summary report...\n",
            "Report generated at /content/learned-kv-compression/experiment_results_Qwen/comparison/comparison_report.md\n",
            "Comparison complete. Results saved to /content/learned-kv-compression/experiment_results_Qwen/comparison\n",
            "Comparison analysis complete!\n",
            "Experiment and analysis complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r experiment_results_Qwen.zip experiment_results_Qwen\n",
        "files.download('experiment_results_Qwen.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BxFW0qqSvCIo",
        "outputId": "aa972bf8-cdef-4c1a-d0a2-3924ba9be1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: experiment_results_Qwen/ (stored 0%)\n",
            "updating: experiment_results_Qwen/comparison/ (stored 0%)\n",
            "updating: experiment_results_Qwen/comparison/bar_compression_ratio.png (deflated 32%)\n",
            "updating: experiment_results_Qwen/comparison/time_comparison.png (deflated 30%)\n",
            "updating: experiment_results_Qwen/comparison/speedup_heatmap.png (deflated 32%)\n",
            "updating: experiment_results_Qwen/comparison/bar_comparison_largest.png (deflated 31%)\n",
            "updating: experiment_results_Qwen/comparison/speedup.png (deflated 27%)\n",
            "updating: experiment_results_Qwen/comparison/comparison_report.md (deflated 56%)\n",
            "updating: experiment_results_Qwen/comparison/compression_ratio.png (deflated 28%)\n",
            "updating: experiment_results_Qwen/comparison/tradeoff.png (deflated 25%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/Qwen_Qwen2.5-1.5B_latent8_lr0.0001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/Qwen_Qwen2.5-1.5B_latent8_lr0.0001/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/Qwen_Qwen2.5-1.5B_latent8_lr0.001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/Qwen_Qwen2.5-1.5B_latent8_lr0.001/attention_viz/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/Qwen_Qwen2.5-1.5B_latent8_lr0.001/attention_viz/epoch_1_batch_0_layer_0_head_0.png (deflated 18%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/Qwen_Qwen2.5-1.5B_latent8_lr0.001/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/benchmark_Qwen_Qwen2.5-1.5B_latent8_lr0.0005_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/benchmark_Qwen_Qwen2.5-1.5B_latent8_lr0.0005_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/benchmark_Qwen_Qwen2.5-1.5B_latent8_lr0.0001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/benchmark_Qwen_Qwen2.5-1.5B_latent8_lr0.0001_batch64_runs5/benchmark_config.json (deflated 59%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/benchmark_Qwen_Qwen2.5-1.5B_latent8_lr0.001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/benchmark_Qwen_Qwen2.5-1.5B_latent8_lr0.001_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/Qwen_Qwen2.5-1.5B_latent8_lr0.0005/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-1.5B/Qwen_Qwen2.5-1.5B_latent8_lr0.0005/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0005/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0005/attention_viz/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0005/autoencoders_final.pth (deflated 45%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0005/train_config.json (deflated 52%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0005/autoencoders_epoch_5.pth (deflated 45%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0005_epochs5_texts10000/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant16_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant16_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant16_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant4_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant4_batch64_runs5/benchmark_results.json (deflated 69%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant4_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant2_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant2_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant2_batch64_runs5/benchmark_config.json (deflated 59%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.001/attention_viz/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.001/attention_viz/epoch_1_batch_100_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.001/attention_viz/epoch_1_batch_200_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.001/attention_viz/epoch_1_batch_0_layer_0_head_0.png (deflated 19%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.001/train_config.json (deflated 52%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant8_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant8_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0005_quant8_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent16_lr0.0005/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent16_lr0.0005/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent8_lr0.0001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent8_lr0.0001/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent8_lr0.001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent8_lr0.001/attention_viz/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent8_lr0.001/attention_viz/epoch_1_batch_0_layer_0_head_0.png (deflated 17%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent8_lr0.001/train_config.json (deflated 52%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent16_lr0.001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent16_lr0.001/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent16_lr0.0001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent16_lr0.0001_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent32_lr0.001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent32_lr0.001/train_config.json (deflated 52%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent16_lr0.0005_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent16_lr0.0005_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent16_lr0.001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent16_lr0.001_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent32_lr0.0001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent32_lr0.0001_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent32_lr0.0001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent32_lr0.0001/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent16_lr0.0001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent16_lr0.0001/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent8_lr0.0005/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent8_lr0.0005/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent32_lr0.0005/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/Qwen_Qwen2.5-3B_latent32_lr0.0005/train_config.json (deflated 53%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent8_lr0.0005_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent8_lr0.0005_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent8_lr0.001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent8_lr0.001_batch64_runs5/benchmark_config.json (deflated 57%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent8_lr0.0001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent8_lr0.0001_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent32_lr0.0005_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent32_lr0.0005_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent32_lr0.001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-3B/benchmark_Qwen_Qwen2.5-3B_latent32_lr0.001_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8/attention_viz/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8/attention_viz/epoch_1_batch_300_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8/attention_viz/epoch_1_batch_100_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8/attention_viz/epoch_1_batch_200_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8/attention_viz/epoch_1_batch_0_layer_0_head_0.png (deflated 4%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8/train_config.json (deflated 52%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/benchmark_Qwen_Qwen2.5-7B_latent8_lr0.001_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/benchmark_Qwen_Qwen2.5-7B_latent8_lr0.001_batch64_runs5/benchmark_config.json (deflated 57%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_800_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_300_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_100_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_2_batch_0_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_400_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_200_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_900_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_700_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_500_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_600_layer_0_head_0.png (deflated 1%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/attention_viz/epoch_1_batch_0_layer_0_head_0.png (deflated 19%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.001/train_config.json (deflated 52%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.0005/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-7B/Qwen_Qwen2.5-7B_latent8_lr0.0005/train_config.json (deflated 52%)\n",
            "updating: experiment_results_Qwen/experiment_summary.json (deflated 83%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant8_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant8_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant8_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant16_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant16_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant16_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent16_lr0.0005_epochs5_texts10000/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent16_lr0.0005/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent16_lr0.0005/attention_viz/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent16_lr0.0005/autoencoders_final.pth (deflated 37%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent16_lr0.0005/train_config.json (deflated 52%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent16_lr0.0005/autoencoders_epoch_5.pth (deflated 37%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant2_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant2_batch64_runs5/benchmark_results.json (deflated 69%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant2_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant4_batch64_runs5/ (stored 0%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant4_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "updating: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent16_lr0.0005_quant4_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant16_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant16_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant16_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant8_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant8_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant8_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant2_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant2_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant2_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant4_batch64_runs5/ (stored 0%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant4_batch64_runs5/benchmark_results.json (deflated 68%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent32_lr0.0005_quant4_batch64_runs5/benchmark_config.json (deflated 58%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005_epochs5_texts10000/ (stored 0%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005/ (stored 0%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005/attention_viz/ (stored 0%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005/autoencoders_final.pth (deflated 32%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005/train_config.json (deflated 52%)\n",
            "  adding: experiment_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent32_lr0.0005/autoencoders_epoch_5.pth (deflated 32%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68d46370-e43f-4ac9-9021-ef972a29905f\", \"experiment_results_Qwen.zip\", 10196581)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}