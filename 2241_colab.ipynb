{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta6KkToOAF1M"
      },
      "source": [
        "# Notebook to run learned kv compression scripts on colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prGnb-vKAnzs"
      },
      "source": [
        "Download Repo and Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YbnT8hMBAE49",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Download Repo\n",
        "%cd /content\n",
        "!rm -rf learned-kv-compression\n",
        "!git clone -b colab https://henro25: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@github.com/henro25/learned-kv-compression\n",
        "%cd /content/learned-kv-compression/\n",
        "%ls\n",
        "\n",
        "# Install Requirements\n",
        "%pip install -r colab_requirements.txt\n",
        "%pip uninstall gcsfs -y\n",
        "%pip install --upgrade fsspec==2025.3.2\n",
        "%pip install gcsfs==2024.12.0\n",
        "%pip install --upgrade datasets\n",
        "\n",
        "# Enable permissions if needed\n",
        "!chmod +x run_experiments.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7vdgntcAana"
      },
      "source": [
        "## Training the Autoencoder\n",
        "\n",
        "This trains an autoencoder that compresses each KV vector to a 16-dimensional latent representation using 1000 texts from WikiText-103."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7idaakurATWT",
        "outputId": "563c4136-2eb2-411e-a292-45478d1cf366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-08 05:49:17.395306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744091357.417357   24026 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744091357.424720   24026 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 05:49:17.447735: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 32,\n",
            " 'buffer_mult': 2,\n",
            " 'config': 'src/configs/default_config.json',\n",
            " 'device': 'cuda',\n",
            " 'eval_interval': 100,\n",
            " 'head_dim': 64,\n",
            " 'input_dim': 64,\n",
            " 'latent_dim': 16,\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 256,\n",
            " 'name': 'distilgpt2',\n",
            " 'num_epochs': 10,\n",
            " 'num_eval_texts': 100,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_key_value_heads': 12,\n",
            " 'num_train_texts': 1000,\n",
            " 'output_dir': 'models/distilgpt2_16',\n",
            " 'seed': 42}\n",
            "Using 1000 texts for training and 100 texts for evaluation\n",
            "Epoch 1/10: 100% 31/31 [00:13<00:00,  2.38it/s]\n",
            "Epoch 1/10, Average Loss: 0.2170\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_1.pth\n",
            "Epoch 2/10: 100% 31/31 [00:13<00:00,  2.29it/s]\n",
            "Epoch 2/10, Average Loss: 0.2473\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_2.pth\n",
            "Epoch 3/10: 100% 31/31 [00:13<00:00,  2.37it/s]\n",
            "Epoch 3/10, Average Loss: 0.2460\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_3.pth\n",
            "Epoch 4/10: 100% 31/31 [00:13<00:00,  2.28it/s]\n",
            "Epoch 4/10, Average Loss: 0.2429\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_4.pth\n",
            "Epoch 5/10: 100% 31/31 [00:13<00:00,  2.34it/s]\n",
            "Epoch 5/10, Average Loss: 0.2419\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_5.pth\n",
            "Epoch 6/10: 100% 31/31 [00:13<00:00,  2.25it/s]\n",
            "Epoch 6/10, Average Loss: 0.2405\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_6.pth\n",
            "Epoch 7/10: 100% 31/31 [00:13<00:00,  2.28it/s]\n",
            "Epoch 7/10, Average Loss: 0.2403\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_7.pth\n",
            "Epoch 8/10: 100% 31/31 [00:14<00:00,  2.17it/s]\n",
            "Epoch 8/10, Average Loss: 0.2389\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_8.pth\n",
            "Epoch 9/10: 100% 31/31 [00:13<00:00,  2.34it/s]\n",
            "Epoch 9/10, Average Loss: 0.2396\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_9.pth\n",
            "Epoch 10/10: 100% 31/31 [00:13<00:00,  2.25it/s]\n",
            "Epoch 10/10, Average Loss: 0.2394\n",
            "Checkpoint saved at models/distilgpt2_16/autoencoder_epoch_10.pth\n",
            "Final model saved at models/distilgpt2_16/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in models/distilgpt2_16\n"
          ]
        }
      ],
      "source": [
        "!python -m src.dictionary_learning.train \\\n",
        "    --name distilgpt2 \\\n",
        "    --latent_dim 16 \\\n",
        "    --num_epochs 10 \\\n",
        "    --batch_size 32 \\\n",
        "    --output_dir models/distilgpt2_16 \\\n",
        "    --num_train_texts 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3sF0VkRbUZX"
      },
      "source": [
        "# Benchmarking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6oJq5qtZTwj"
      },
      "source": [
        "Run a quick test for KV Cache compression with minimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j_99ro74ZTZw",
        "outputId": "45a5b5eb-10ad-4e21-b04b-87e75d76b085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== Quick Test: KV Cache Compression ====\n",
            "Model: distilgpt2\n",
            "Latent dimension: 16\n",
            "Number of epochs: 1\n",
            "Number of training texts: 10\n",
            "Cache size: 1 MB\n",
            "Batch size: 512\n",
            "Number of runs: 3\n",
            "========================================\n",
            "Step 1: Training autoencoder...\n",
            "2025-04-08 05:51:56.068042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744091516.089612   24733 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744091516.096595   24733 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 05:51:56.119553: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 2,\n",
            " 'buffer_mult': 2,\n",
            " 'config': 'src/configs/default_config.json',\n",
            " 'device': 'cuda',\n",
            " 'eval_interval': 100,\n",
            " 'head_dim': 64,\n",
            " 'input_dim': 64,\n",
            " 'latent_dim': 16,\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 256,\n",
            " 'name': 'distilgpt2',\n",
            " 'num_epochs': 1,\n",
            " 'num_eval_texts': 100,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_key_value_heads': 12,\n",
            " 'num_train_texts': 10,\n",
            " 'output_dir': 'test_models/distilgpt2_latent16',\n",
            " 'seed': 42}\n",
            "Using 10 texts for training and 100 texts for evaluation\n",
            "Epoch 1/1: 100% 5/5 [00:01<00:00,  3.63it/s]\n",
            "Epoch 1/1, Average Loss: 0.1833\n",
            "Checkpoint saved at test_models/distilgpt2_latent16/autoencoder_epoch_1.pth\n",
            "Final model saved at test_models/distilgpt2_latent16/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in test_models/distilgpt2_latent16\n",
            "Step 2: Running benchmark test...\n",
            "2025-04-08 05:52:22.415621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744091542.435197   24848 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744091542.440773   24848 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 05:52:22.463350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loaded autoencoder from test_models/distilgpt2_latent16/autoencoder_final.pth\n",
            "Benchmarking different cache sizes:   0% 0/1 [00:00<?, ?it/s]\n",
            "Running benchmark for 1.0MB KV cache\n",
            "\n",
            "Generating 1.0MB KV cache:   0% 0/56 [00:00<?, ?it/s]\u001b[A\n",
            "Generating 1.0MB KV cache:  18% 10/56 [00:00<00:01, 39.67it/s]\u001b[A\n",
            "Generating 1.0MB KV cache:  71% 40/56 [00:00<00:00, 89.40it/s]\n",
            "Generated KV cache of size: 0.98MB (56 tokens)\n",
            "Measuring baseline (no compression)...\n",
            "Compressing KV cache...\n",
            "Measuring with compression...\n",
            "Compression ratio: 4.00x\n",
            "Speedup factor: 0.85x\n",
            "Time to first token (baseline): 0.0063s ± 0.0002s\n",
            "Time to first token (compressed): 0.0073s ± 0.0001s\n",
            "Benchmarking different cache sizes: 100% 1/1 [00:00<00:00,  1.94it/s]\n",
            "Results saved to test_results/benchmark_results.json\n",
            "Quick test completed!\n",
            "Check test_results for results and visualizations.\n"
          ]
        }
      ],
      "source": [
        "!./quick_test.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WF2aid4bRLa"
      },
      "source": [
        "Run experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MNdpwUKhbQv8",
        "outputId": "af5e7758-e5f6-4c32-a7d4-2d4ba06974af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'learned-kv-compression'...\n",
            "remote: Enumerating objects: 349, done.\u001b[K\n",
            "remote: Counting objects: 100% (349/349), done.\u001b[K\n",
            "remote: Compressing objects: 100% (242/242), done.\u001b[K\n",
            "remote: Total 349 (delta 154), reused 260 (delta 80), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (349/349), 12.44 MiB | 40.96 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n",
            "/content/learned-kv-compression\n",
            "==== KV Cache Compression Experiment ====\n",
            "Model: distilgpt2\n",
            "Latent dimensions: 8 16 32\n",
            "Cache sizes (MB): 1 10 100 1000\n",
            "Number of epochs: 5\n",
            "Number of training texts: 10000\n",
            "Batch size: 64\n",
            "Number of runs for timing: 5\n",
            "Output directory: experiment_results_distilgpt2\n",
            "========================================\n",
            "./run_experiments.sh: line 40: venv/bin/activate: No such file or directory\n",
            "Starting experiment at Tue Apr  8 08:30:27 PM UTC 2025\n",
            "MODEL DIR:  experiment_results_distilgpt2/distilgpt2_latent8\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with latent_dim=8\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config experiment_results_distilgpt2/distilgpt2_latent8/train_config.json --latent_dim 8 --num_epochs 5 --num_train_texts 10000 --output_dir experiment_results_distilgpt2/distilgpt2_latent8\n",
            "2025-04-08 20:30:29.003445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744144229.023864   19963 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744144229.030219   19963 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 20:30:29.051770: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 2,\n",
            " 'buffer_mult': 2,\n",
            " 'config': 'experiment_results_distilgpt2/distilgpt2_latent8/train_config.json',\n",
            " 'device': 'cuda',\n",
            " 'eval_interval': 100,\n",
            " 'head_dim': 64,\n",
            " 'input_dim': 64,\n",
            " 'latent_dim': 8,\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 256,\n",
            " 'name': 'distilgpt2',\n",
            " 'num_epochs': 5,\n",
            " 'num_eval_texts': 100,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_key_value_heads': 12,\n",
            " 'num_train_texts': 10000,\n",
            " 'output_dir': 'experiment_results_distilgpt2/distilgpt2_latent8',\n",
            " 'seed': 42}\n",
            "Using 8000 texts for training and 100 texts for evaluation\n",
            "Training data includes -2000 LongBench texts\n",
            "Evaluation data includes 0 LongBench texts\n",
            "Epoch 1/5: 100% 4000/4000 [02:08<00:00, 31.06it/s]\n",
            "Epoch 1/5, Average Loss: 0.2186\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_epoch_1.pth\n",
            "Epoch 2/5: 100% 4000/4000 [02:11<00:00, 30.33it/s]\n",
            "Epoch 2/5, Average Loss: 0.1947\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_epoch_2.pth\n",
            "Epoch 3/5: 100% 4000/4000 [02:11<00:00, 30.30it/s]\n",
            "Epoch 3/5, Average Loss: 0.1900\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_epoch_3.pth\n",
            "Epoch 4/5: 100% 4000/4000 [02:12<00:00, 30.25it/s]\n",
            "Epoch 4/5, Average Loss: 0.1892\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_epoch_4.pth\n",
            "Epoch 5/5: 100% 4000/4000 [02:12<00:00, 30.20it/s]\n",
            "Epoch 5/5, Average Loss: 0.1891\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_epoch_5.pth\n",
            "Final model saved at experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in experiment_results_distilgpt2/distilgpt2_latent8\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with latent_dim=8\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --model distilgpt2 --autoencoder experiment_results_distilgpt2/distilgpt2_latent8/autoencoder_final.pth --latent_dim 8 --cache_sizes 1.0 10.0 100.0 1000.0 --batch_size 64 --num_runs 5 --output experiment_results_distilgpt2/benchmark_distilgpt2_latent8 --config experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_config.json\n",
            "2025-04-08 20:41:54.534887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744144914.553021   22806 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744144914.557864   22806 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 20:41:54.579203: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "\n",
            "Calculating WikiText perplexity...\n",
            "Calculating perplexity:   0% 0/200 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Calculating perplexity: 100% 200/200 [00:02<00:00, 75.37it/s]\n",
            "Baseline perplexity: 78.16\n",
            "Evaluating with compressed cache: 100% 200/200 [02:40<00:00,  1.25it/s]\n",
            "Compressed cache perplexity: 132631.91\n",
            "\n",
            "Evaluating on LongBench...\n",
            "\n",
            "Evaluating on narrativeqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 136.73it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:14<00:00, 13.59it/s]\n",
            "narrativeqa - Baseline PPL: 405.45, Compressed PPL: 2289.62\n",
            "\n",
            "Evaluating on hotpotqa...\n",
            "Calculating perplexity: 100% 200/200 [00:02<00:00, 99.38it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:27<00:00,  7.40it/s]\n",
            "hotpotqa - Baseline PPL: 269.38, Compressed PPL: 1848.73\n",
            "\n",
            "Evaluating on 2wikimqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 137.42it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:24<00:00,  8.09it/s]\n",
            "2wikimqa - Baseline PPL: 420.95, Compressed PPL: 2179.03\n",
            "\n",
            "Evaluating on musique...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 139.50it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:25<00:00,  7.75it/s]\n",
            "musique - Baseline PPL: 211.17, Compressed PPL: 1040.83\n",
            "\n",
            "Evaluating on dureader...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 118.15it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:27<00:00,  7.33it/s]\n",
            "dureader - Baseline PPL: 68.06, Compressed PPL: 2426.22\n",
            "\n",
            "Benchmarking complete!\n",
            "Results saved in experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8\n",
            "- Evaluation results (JSON): experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/evaluation_results.json\n",
            "- Evaluation results (CSV): experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/evaluation_results.csv\n",
            "- Plots saved in experiment_results_distilgpt2/benchmark_distilgpt2_latent8/benchmark_distilgpt2_latent8/plots\n",
            "MODEL DIR:  experiment_results_distilgpt2/distilgpt2_latent16\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with latent_dim=16\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config experiment_results_distilgpt2/distilgpt2_latent16/train_config.json --latent_dim 16 --num_epochs 5 --num_train_texts 10000 --output_dir experiment_results_distilgpt2/distilgpt2_latent16\n",
            "2025-04-08 20:47:01.720506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744145221.740800   24136 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744145221.747080   24136 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 20:47:01.768226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 2,\n",
            " 'buffer_mult': 2,\n",
            " 'config': 'experiment_results_distilgpt2/distilgpt2_latent16/train_config.json',\n",
            " 'device': 'cuda',\n",
            " 'eval_interval': 100,\n",
            " 'head_dim': 64,\n",
            " 'input_dim': 64,\n",
            " 'latent_dim': 16,\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 256,\n",
            " 'name': 'distilgpt2',\n",
            " 'num_epochs': 5,\n",
            " 'num_eval_texts': 100,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_key_value_heads': 12,\n",
            " 'num_train_texts': 10000,\n",
            " 'output_dir': 'experiment_results_distilgpt2/distilgpt2_latent16',\n",
            " 'seed': 42}\n",
            "Using 8000 texts for training and 100 texts for evaluation\n",
            "Training data includes -2000 LongBench texts\n",
            "Evaluation data includes 0 LongBench texts\n",
            "Epoch 1/5: 100% 4000/4000 [02:10<00:00, 30.63it/s]\n",
            "Epoch 1/5, Average Loss: 0.2000\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent16/autoencoder_epoch_1.pth\n",
            "Epoch 2/5: 100% 4000/4000 [02:12<00:00, 30.12it/s]\n",
            "Epoch 2/5, Average Loss: 0.1768\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent16/autoencoder_epoch_2.pth\n",
            "Epoch 3/5: 100% 4000/4000 [02:13<00:00, 30.05it/s]\n",
            "Epoch 3/5, Average Loss: 0.1717\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent16/autoencoder_epoch_3.pth\n",
            "Epoch 4/5: 100% 4000/4000 [02:12<00:00, 30.18it/s]\n",
            "Epoch 4/5, Average Loss: 0.1702\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent16/autoencoder_epoch_4.pth\n",
            "Epoch 5/5: 100% 4000/4000 [02:14<00:00, 29.76it/s]\n",
            "Epoch 5/5, Average Loss: 0.1698\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent16/autoencoder_epoch_5.pth\n",
            "Final model saved at experiment_results_distilgpt2/distilgpt2_latent16/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in experiment_results_distilgpt2/distilgpt2_latent16\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with latent_dim=16\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --model distilgpt2 --autoencoder experiment_results_distilgpt2/distilgpt2_latent16/autoencoder_final.pth --latent_dim 16 --cache_sizes 1.0 10.0 100.0 1000.0 --batch_size 64 --num_runs 5 --output experiment_results_distilgpt2/benchmark_distilgpt2_latent16 --config experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_config.json\n",
            "2025-04-08 20:58:33.049951: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744145913.067398   27001 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744145913.072198   27001 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 20:58:33.092918: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "\n",
            "Calculating WikiText perplexity...\n",
            "Calculating perplexity:   0% 0/200 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Calculating perplexity: 100% 200/200 [00:02<00:00, 88.95it/s]\n",
            "Baseline perplexity: 78.16\n",
            "Evaluating with compressed cache: 100% 200/200 [02:42<00:00,  1.23it/s]\n",
            "Compressed cache perplexity: 199864.08\n",
            "\n",
            "Evaluating on LongBench...\n",
            "\n",
            "Evaluating on narrativeqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 116.64it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:15<00:00, 12.91it/s]\n",
            "narrativeqa - Baseline PPL: 405.45, Compressed PPL: 1683.19\n",
            "\n",
            "Evaluating on hotpotqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 134.25it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:27<00:00,  7.26it/s]\n",
            "hotpotqa - Baseline PPL: 269.38, Compressed PPL: 1297.14\n",
            "\n",
            "Evaluating on 2wikimqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 137.17it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:24<00:00,  8.14it/s]\n",
            "2wikimqa - Baseline PPL: 420.95, Compressed PPL: 1295.81\n",
            "\n",
            "Evaluating on musique...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 111.99it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:26<00:00,  7.54it/s]\n",
            "musique - Baseline PPL: 211.17, Compressed PPL: 749.33\n",
            "\n",
            "Evaluating on dureader...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 135.41it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:27<00:00,  7.17it/s]\n",
            "dureader - Baseline PPL: 68.06, Compressed PPL: 1272.39\n",
            "\n",
            "Benchmarking complete!\n",
            "Results saved in experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16\n",
            "- Evaluation results (JSON): experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/evaluation_results.json\n",
            "- Evaluation results (CSV): experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/evaluation_results.csv\n",
            "- Plots saved in experiment_results_distilgpt2/benchmark_distilgpt2_latent16/benchmark_distilgpt2_latent16/plots\n",
            "MODEL DIR:  experiment_results_distilgpt2/distilgpt2_latent32\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with latent_dim=32\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config experiment_results_distilgpt2/distilgpt2_latent32/train_config.json --latent_dim 32 --num_epochs 5 --num_train_texts 10000 --output_dir experiment_results_distilgpt2/distilgpt2_latent32\n",
            "2025-04-08 21:03:43.574671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744146223.595160   28366 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744146223.601431   28366 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 21:03:43.624834: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "{'batch_size': 2,\n",
            " 'buffer_mult': 2,\n",
            " 'config': 'experiment_results_distilgpt2/distilgpt2_latent32/train_config.json',\n",
            " 'device': 'cuda',\n",
            " 'eval_interval': 100,\n",
            " 'head_dim': 64,\n",
            " 'input_dim': 64,\n",
            " 'latent_dim': 32,\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 256,\n",
            " 'name': 'distilgpt2',\n",
            " 'num_epochs': 5,\n",
            " 'num_eval_texts': 100,\n",
            " 'num_hidden_layers': 6,\n",
            " 'num_key_value_heads': 12,\n",
            " 'num_train_texts': 10000,\n",
            " 'output_dir': 'experiment_results_distilgpt2/distilgpt2_latent32',\n",
            " 'seed': 42}\n",
            "Using 8000 texts for training and 100 texts for evaluation\n",
            "Training data includes -2000 LongBench texts\n",
            "Evaluation data includes 0 LongBench texts\n",
            "Epoch 1/5: 100% 4000/4000 [02:12<00:00, 30.13it/s]\n",
            "Epoch 1/5, Average Loss: 0.1789\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent32/autoencoder_epoch_1.pth\n",
            "Epoch 2/5: 100% 4000/4000 [02:13<00:00, 29.88it/s]\n",
            "Epoch 2/5, Average Loss: 0.1439\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent32/autoencoder_epoch_2.pth\n",
            "Epoch 3/5: 100% 4000/4000 [02:15<00:00, 29.57it/s]\n",
            "Epoch 3/5, Average Loss: 0.1355\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent32/autoencoder_epoch_3.pth\n",
            "Epoch 4/5: 100% 4000/4000 [02:14<00:00, 29.67it/s]\n",
            "Epoch 4/5, Average Loss: 0.1319\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent32/autoencoder_epoch_4.pth\n",
            "Epoch 5/5: 100% 4000/4000 [02:15<00:00, 29.59it/s]\n",
            "Epoch 5/5, Average Loss: 0.1308\n",
            "Checkpoint saved at experiment_results_distilgpt2/distilgpt2_latent32/autoencoder_epoch_5.pth\n",
            "Final model saved at experiment_results_distilgpt2/distilgpt2_latent32/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in experiment_results_distilgpt2/distilgpt2_latent32\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with latent_dim=32\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --model distilgpt2 --autoencoder experiment_results_distilgpt2/distilgpt2_latent32/autoencoder_final.pth --latent_dim 32 --cache_sizes 1.0 10.0 100.0 1000.0 --batch_size 64 --num_runs 5 --output experiment_results_distilgpt2/benchmark_distilgpt2_latent32 --config experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_config.json\n",
            "2025-04-08 21:15:23.526653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744146923.556266   31251 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744146923.564997   31251 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 21:15:23.597713: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:817: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "\n",
            "Calculating WikiText perplexity...\n",
            "Calculating perplexity:   0% 0/200 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Calculating perplexity: 100% 200/200 [00:02<00:00, 89.27it/s]\n",
            "Baseline perplexity: 78.16\n",
            "Evaluating with compressed cache: 100% 200/200 [02:39<00:00,  1.25it/s]\n",
            "Compressed cache perplexity: 5367.88\n",
            "\n",
            "Evaluating on LongBench...\n",
            "\n",
            "Evaluating on narrativeqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 139.31it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:14<00:00, 13.60it/s]\n",
            "narrativeqa - Baseline PPL: 405.45, Compressed PPL: 1546.69\n",
            "\n",
            "Evaluating on hotpotqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 136.91it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:27<00:00,  7.29it/s]\n",
            "hotpotqa - Baseline PPL: 269.38, Compressed PPL: 1113.29\n",
            "\n",
            "Evaluating on 2wikimqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 138.00it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:24<00:00,  8.32it/s]\n",
            "2wikimqa - Baseline PPL: 420.95, Compressed PPL: 1364.17\n",
            "\n",
            "Evaluating on musique...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 138.06it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:26<00:00,  7.45it/s]\n",
            "musique - Baseline PPL: 211.17, Compressed PPL: 664.79\n",
            "\n",
            "Evaluating on dureader...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 140.12it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:27<00:00,  7.22it/s]\n",
            "dureader - Baseline PPL: 68.06, Compressed PPL: 361.14\n",
            "\n",
            "Benchmarking complete!\n",
            "Results saved in experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32\n",
            "- Evaluation results (JSON): experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/evaluation_results.json\n",
            "- Evaluation results (CSV): experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/evaluation_results.csv\n",
            "- Plots saved in experiment_results_distilgpt2/benchmark_distilgpt2_latent32/benchmark_distilgpt2_latent32/plots\n",
            "\n",
            "================================================================================\n",
            "Experiment Summary\n",
            "================================================================================\n",
            "Model: distilgpt2\n",
            "Latent dimensions tested: [8, 16, 32]\n",
            "KV cache sizes tested: [1.0, 10.0, 100.0, 1000.0] MB\n",
            "Batch size: 64\n",
            "Number of runs for timing: 5\n",
            "Total runtime: 0h 50m 1.04s\n",
            "Results saved to: experiment_results_distilgpt2\n",
            "- Latent dim 8: experiment_results_distilgpt2/benchmark_distilgpt2_latent8\n",
            "- Latent dim 16: experiment_results_distilgpt2/benchmark_distilgpt2_latent16\n",
            "- Latent dim 32: experiment_results_distilgpt2/benchmark_distilgpt2_latent32\n",
            "================================================================================\n",
            "Experiment completed at Tue Apr  8 09:20:28 PM UTC 2025\n",
            "Generating comparison report...\n",
            "Loading results from: ['experiment_results_distilgpt2/benchmark_distilgpt2_latent8', 'experiment_results_distilgpt2/benchmark_distilgpt2_latent16', 'experiment_results_distilgpt2/benchmark_distilgpt2_latent32']\n",
            "Generating comparison plots...\n",
            "Generating summary report...\n",
            "Report generated at experiment_results_distilgpt2/comparison/comparison_report.md\n",
            "Comparison complete. Results saved to experiment_results_distilgpt2/comparison\n",
            "Experiment and analysis complete!\n"
          ]
        }
      ],
      "source": [
        "!./run_experiments.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ5GZpidFIp1"
      },
      "source": [
        "Eval with Perplexity and Longbench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2koR1iA0a-Az",
        "outputId": "f3478a05-6662-4fd3-bcf6-239cdc86bbe3",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting evaluation...\n",
            "2025-04-08 06:00:03.677330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744092003.697675   26909 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744092003.703449   26909 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-08 06:00:03.728452: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Successfully loaded autoencoder from models/distilgpt2_16/autoencoder_final.pth\n",
            "Loaded 100 evaluation texts\n",
            "\n",
            "Calculating baseline perplexity...\n",
            "Calculating perplexity: 100% 100/100 [00:01<00:00, 63.12it/s]\n",
            "Baseline perplexity: 86.11\n",
            "\n",
            "Calculating perplexity with compressed cache...\n",
            "Evaluating with compressed cache: 100% 100/100 [00:01<00:00, 51.55it/s]\n",
            "Compressed cache perplexity: 2.29\n",
            "\n",
            "Evaluating on LongBench...\n",
            "\n",
            "Evaluating on narrativeqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 125.47it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 63.69it/s]\n",
            "narrativeqa - Baseline PPL: 414.46, Compressed PPL: 1834.44\n",
            "\n",
            "Evaluating on hotpotqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 125.37it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 51.95it/s]\n",
            "hotpotqa - Baseline PPL: 272.10, Compressed PPL: 1788.02\n",
            "\n",
            "Evaluating on 2wikimqa...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 127.85it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 63.89it/s]\n",
            "2wikimqa - Baseline PPL: 421.86, Compressed PPL: 2102.78\n",
            "\n",
            "Evaluating on musique...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 126.00it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 52.75it/s]\n",
            "musique - Baseline PPL: 212.93, Compressed PPL: 1385.13\n",
            "\n",
            "Evaluating on dureader...\n",
            "Calculating perplexity: 100% 200/200 [00:01<00:00, 126.77it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:03<00:00, 63.97it/s]\n",
            "dureader - Baseline PPL: 68.44, Compressed PPL: 5792.14\n",
            "Saved perplexity comparison plot to models/perplexity_comparison.png\n",
            "\n",
            "Evaluation complete!\n",
            "Results saved in models\n",
            "- Evaluation results: models/evaluation_results.json\n",
            "- Perplexity comparison plot: models/perplexity_comparison.png\n",
            "Evaluation complete! Results are saved in models/distilgpt2_16\n"
          ]
        }
      ],
      "source": [
        "!./src/evaluation/run_evaluation.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hi-kL2grLiK"
      },
      "source": [
        "## Qwen2.5-0.5B Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMANLyemhaTB",
        "outputId": "f0251263-0649-4777-a478-2433e1459a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== KV Cache Compression Experiment ====\n",
            "Config file: configs/qwen25_0.5b_test.json\n",
            "========================================\n",
            "Starting experiment at Mon Apr 21 11:19:53 PM UTC 2025\n",
            "Gradient accumulation steps: 2\n",
            "Buffer size: 256\n",
            "Buffer multiplier: 2\n",
            "Max sequence length: 8192\n",
            "Running experiments with the following configuration:\n",
            "Models: ['Qwen/Qwen2.5-0.5B']\n",
            "Latent dimensions: [8]\n",
            "Learning rates: [0.0001]\n",
            "Cache sizes: [1, 10, 100, 1000]\n",
            "Epochs: [1]\n",
            "Training texts: [100]\n",
            "Batch sizes: [64]\n",
            "Number of runs: [5]\n",
            "Output directory: test_results_Qwen\n",
            "MODEL DIR:  test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001\n",
            "\n",
            "================================================================================\n",
            "Training autoencoder with model=Qwen/Qwen2.5-0.5B, latent_dim=8, lr=0.0001, epochs=1, train_texts=100\n",
            "================================================================================\n",
            "python -m src.dictionary_learning.train --config test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001/train_config.json\n",
            "2025-04-21 23:19:55.464385: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-21 23:19:55.481916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745277595.503002    5322 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745277595.509556    5322 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-21 23:19:55.530998: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Training with configuration:\n",
            "{'batch_size': 16,\n",
            " 'buffer_mult': 2,\n",
            " 'buffer_size': 256,\n",
            " 'device': 'cuda',\n",
            " 'dtype': 'bf16',\n",
            " 'eval_interval': 100,\n",
            " 'gradient_accumulation_steps': 2,\n",
            " 'hidden_size': 896,\n",
            " 'latent_dim': 8,\n",
            " 'latent_dims': [8],\n",
            " 'learning_rates': [0.0001],\n",
            " 'lm_batch_size': 1,\n",
            " 'lr': 0.0001,\n",
            " 'max_seq_len': 8192,\n",
            " 'models': ['Qwen/Qwen2.5-0.5B'],\n",
            " 'name': 'Qwen/Qwen2.5-0.5B',\n",
            " 'num_attention_heads': 14,\n",
            " 'num_epochs': 1,\n",
            " 'num_eval_texts': 50,\n",
            " 'num_hidden_layers': 24,\n",
            " 'num_train_texts': 100,\n",
            " 'output_dir': 'test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001',\n",
            " 'seed': 42,\n",
            " 'skip_training': False,\n",
            " 'vocab_size': 151936}\n",
            "Using dtype: torch.bfloat16\n",
            "Using 100 texts for training and 100 texts for evaluation\n",
            "Training data includes 0 LongBench texts\n",
            "Evaluation data includes 50 LongBench texts\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 8192, buffer_size: 256)\n",
            "Allocating buffers with shape (16, 24, 14, 256, 64), total elements: 88080384\n",
            "Buffer using data type: torch.bfloat16\n",
            "Using buffer sequence length of 256 tokens (max_seq_len: 8192, buffer_size: 256)\n",
            "Allocating buffers with shape (16, 24, 14, 256, 64), total elements: 88080384\n",
            "Epoch 1/1: 100% 6/6 [00:04<00:00,  1.31it/s]\n",
            "Epoch 1/1, Average Loss: 0.0597\n",
            "Checkpoint saved at test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001/autoencoder_epoch_1.pth\n",
            "Final model saved at test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001/autoencoder_final.pth\n",
            "Training complete!\n",
            "Best evaluation loss: inf\n",
            "Models saved in test_results_Qwen/Qwen_Qwen2.5-0.5B/Qwen_Qwen2.5-0.5B_latent8_lr0.0001\n",
            "\n",
            "================================================================================\n",
            "Running benchmark with model=Qwen/Qwen2.5-0.5B, latent_dim=8, batch_size=64, num_runs=5\n",
            "================================================================================\n",
            "python -m src.inference.benchmark --config test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_batch64_runs5/benchmark_config.json\n",
            "Using dtype: torch.bfloat16, buffer size: 256\n",
            "2025-04-21 23:20:37.222605: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-21 23:20:37.241146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745277637.262432    5554 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745277637.268816    5554 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-21 23:20:37.289388: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "\n",
            "Calculating WikiText perplexity...\n",
            "Calculating perplexity:   0% 0/50 [00:00<?, ?it/s]`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Calculating perplexity: 100% 50/50 [00:02<00:00, 22.23it/s]\n",
            "Baseline perplexity: 26.17\n",
            "Evaluating with compressed cache: 100% 50/50 [02:49<00:00,  3.38s/it]\n",
            "Compressed cache perplexity: 11872.12\n",
            "\n",
            "Evaluating on LongBench...\n",
            "\n",
            "Evaluating on narrativeqa...\n",
            "Calculating perplexity: 100% 200/200 [00:06<00:00, 29.35it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [01:18<00:00,  2.55it/s]\n",
            "narrativeqa - Baseline PPL: 142.70, Compressed PPL: 20316.71\n",
            "\n",
            "Evaluating on hotpotqa...\n",
            "Calculating perplexity: 100% 200/200 [00:06<00:00, 28.72it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [02:30<00:00,  1.33it/s]\n",
            "hotpotqa - Baseline PPL: 63.27, Compressed PPL: 12489.91\n",
            "\n",
            "Evaluating on 2wikimqa...\n",
            "Calculating perplexity: 100% 200/200 [00:06<00:00, 29.00it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [02:05<00:00,  1.60it/s]\n",
            "2wikimqa - Baseline PPL: 96.67, Compressed PPL: 22803.84\n",
            "\n",
            "Evaluating on musique...\n",
            "Calculating perplexity: 100% 200/200 [00:06<00:00, 28.90it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [02:20<00:00,  1.43it/s]\n",
            "musique - Baseline PPL: 63.63, Compressed PPL: 8629.03\n",
            "\n",
            "Evaluating on dureader...\n",
            "Calculating perplexity: 100% 200/200 [00:06<00:00, 29.33it/s]\n",
            "Evaluating with compressed cache: 100% 200/200 [00:39<00:00,  5.11it/s]\n",
            "dureader - Baseline PPL: 269.00, Compressed PPL: 37545.94\n",
            "\n",
            "Benchmarking complete!\n",
            "Results saved in test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_batch64_runs5/benchmark_Qwen_Qwen2.5-0.5B_latent8\n",
            "- Evaluation results (JSON): test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_batch64_runs5/benchmark_Qwen_Qwen2.5-0.5B_latent8/evaluation_results.json\n",
            "- Evaluation results (CSV): test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_batch64_runs5/benchmark_Qwen_Qwen2.5-0.5B_latent8/evaluation_results.csv\n",
            "- Plots saved in test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_batch64_runs5/benchmark_Qwen_Qwen2.5-0.5B_latent8/plots\n",
            "\n",
            "================================================================================\n",
            "Experiment Summary\n",
            "================================================================================\n",
            "Models tested: ['Qwen/Qwen2.5-0.5B']\n",
            "Latent dimensions tested: [8]\n",
            "Learning rates tested: [0.0001]\n",
            "Epochs tested: [1]\n",
            "Training texts tested: [100]\n",
            "Batch sizes tested: [64]\n",
            "Number of runs tested: [5]\n",
            "KV cache sizes tested: [1, 10, 100, 1000] MB\n",
            "Total runtime: 0h 13m 26.89s\n",
            "Results saved to: test_results_Qwen\n",
            "Total experiments: 1\n",
            "================================================================================\n",
            "Experiment Results:\n",
            "- Model: Qwen/Qwen2.5-0.5B, Latent dim: 8, LR: 0.0001, Epochs: 1, Texts: 100, Batch: 64, Runs: 5\n",
            "  Result dir: test_results_Qwen/Qwen_Qwen2.5-0.5B/benchmark_Qwen_Qwen2.5-0.5B_latent8_lr0.0001_batch64_runs5\n",
            "================================================================================\n",
            "Experiment completed at Mon Apr 21 11:33:20 PM UTC 2025\n",
            "Generating comparison report...\n",
            "./run_experiments.sh: line 30: jq: command not found\n",
            "Experiment summary not found, comparison report may be incomplete\n",
            "Found result directories: \n",
            "No result directories found for comparison analysis!\n",
            "Experiment and analysis complete!\n"
          ]
        }
      ],
      "source": [
        "!./run_experiments.sh configs/qwen25_0.5b_test.json"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}