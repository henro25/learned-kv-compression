{
  "name": "distilgpt2",
  "input_dim": 64,
  "latent_dim": 32,
  "batch_size": 64,
  "num_epochs": 10,
  "hidden_size": 768,
  "encoder_layer_sizes": [32, 32],
  "decoder_layer_sizes": [32],
  "activation": "GELU",
  "gradient_accumulation_steps": 2,
  "lr": 0.001,
  "seed": 42,
  "head_dim": 64,
  "buffer_mult": 2,
  "num_hidden_layers": 6,
  "lm_batch_size": 1,
  "num_attention_heads": 12,
  "val_split": 0.1,
  "lr_reduce_factor": 0.5,
  "lr_patience": 1,
  "eval_interval": 1,
  "config": "configs/distilgpt2_experiment_32.json",
  "num_train_texts": 20000,
  "num_eval_texts": 10,
  "device": "cuda",
  
  "max_seq_len": 1024,

  "autoencoder_path": "experiment_results_distilgpt2/asym_32/autoencoders_final_distilgpt2_latent_32_ae_3_2.pth",
  "output_dir": "experiment_results_distilgpt2/asym_32",
  "vocab_size": 50257,
  "cache_sizes": [1, 10, 100, 1000],
  "quantization_bits": [2, 4, 8, 16],
  "model_name": "distilgpt2",
  "dtype": "fp16"
}
