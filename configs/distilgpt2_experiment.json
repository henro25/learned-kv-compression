{
    "models": ["distilgpt2"],
    "latent_dims": [8, 16, 32],
    "learning_rates": [1e-3, 5e-4],
    "num_epochs": [2],
    "num_train_texts": [100],
    "seed": 42,
    "skip_training": false,
    "output_dir": "experiment_results_distilgpt2",
  
    "eval_interval": 1000,
    "num_eval_texts": 200,
  
    "lm_batch_size": 1,
    "buffer_size": 256,
    "batch_size": 64,
    "dtype": "bf16",
    "buffer_mult": 2,
    "gradient_accumulation_steps": 2,
  
    "device": "cuda",
  
    "name": "distilgpt2",
    "num_hidden_layers": 6,
    "num_attention_heads": 12,
    "max_seq_len": 1024,
    "hidden_size": 768,
    "vocab_size": 50257
  } 